# Q-Learning 简介

![缩略图](https://huggingface.co/blog/assets/70_deep_rl_q_part1/thumbnail.gif  =800x200)

------

在 [本课程的第一章中](https://huggingface.co/blog/deep-rl-intro)，我们学习了强化学习 (RL)、RL 过程以及解决 RL 问题的不同方法。今天，我们将 **深入研究其中一种强化学习方法：基于价值的方法** ，并研究我们的第一个 RL 算法： **Q-Learning。**

我们还将 **从头开始实现我们的第一个 RL 智能体**：Q-Learning 智能体，并将在两种环境中对其进行训练：

1. Frozen-Lake-v1（防滑版本）：我们的智能体需要通过仅在冻结的瓷砖 (F) 上行走并避开洞 (H) 来 **从起始状态 (S) 进入目标状态 (G** )。
2. 自动驾驶出租车需要 **学会在** 城市中导航，才能 **将乘客从 A 点运送到 B 点。**

![环境](https://huggingface.co/blog/assets/70_deep_rl_q_part1/envs.gif)本单元分为 2 个部分：

![两部分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two_parts.jpg)

在第一部分中，我们将 **学习基于值的方法以及蒙特卡洛和时间差分学习之间的区别。**

在第二部分， **我们将研究我们的第一个 RL 算法：Q-Learning，并实现我们的第一个 RL 智能体。**

**如果你想学习深度 Q 学习**（第 3 单元），这个单元是基础 ：第一个能够玩 Atari 游戏并 **在其中一些**游戏 （突破、太空入侵者……）上击败人类水平的深度 RL 算法.

## **什么是强化学习？简短回顾**

在 RL 中，我们构建了一个可以 **做出明智决策**的智能体。例如，一个 **学习玩电子游戏的智能体。** 或者是一个交易智能体， 通过**对购买哪些股票和何时出售** 做出明智的决策 来**学习最大化其利益。**

![强化学习过程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/rl-process.jpg)

但是，为了做出明智的决策，我们的智能体将 **通过反复试验与环境交互** 并接收奖励（正面或负面） **作为反馈，从而从环境中学习。**

它的目标 **是最大化其期望累积奖励** （由于奖励假设）。

**智能体的决策过程称为策略 π：** 给定 一个状态，策略将输出一个动作或动作的概率分布。也就是说，给定对环境的观察，策略将提供智能体应采取的动作（或每个动作的多个概率）。

![策略](https://huggingface.co/blog/assets/70_deep_rl_q_part1/policy.jpg)

**我们的目标是找到一个最优策略 π** *，也就是一个导致最佳期望累积奖励的策略。

为了找到这个最优策略（从而解决 RL 问题），有 **两种主要类型的 RL 方法**：

- *基于策略的方法*： **直接训练策略函数** 以学习在给定状态下要采取的动作。
- *基于价值的方法*： **训练一个价值函数** 来学习 **哪个状态更有价值** ，并使用这个价值函数**来采取导致它的动作。**

![两种强化学习方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-approaches.jpg)

在本章中， **我们将深入探讨基于价值的方法。**

## **两种基于值的方法**

在基于值的方法中， 我们学习了一个 **将状态映射到处于该状态的期望值** 的价值函数。

![基于价值的方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/vbm-1.jpg)

状态的值是 智能体**从该状态开始然后按照我们的策略行动时** 可以获得的**期望折扣回报。**

如果您忘记了折扣是什么，您[可以阅读本节](https://huggingface.co/blog/deep-rl-intro#rewards-and-the-discounting)。

> 但是，按照我们的策略动作意味着什么？毕竟，我们没有基于价值的方法的策略，因为我们训练的是价值函数而不是策略。

请记住， **RL 智能体的目标是拥有最优策略 π。**

为了找到它，我们学习到有两种不同的方法：

- *基于策略的方法：* **直接训练策略** 以选择在给定状态下要采取的动作（或在该状态下的动作的概率分布）。在这种情况下，我们 **没有价值函数。**

![两种强化学习方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-approaches-2.jpg)

该策略将状态作为输入并输出在该状态下要采取的动作（确定性策略）。

因此， **我们不会手动定义策略的行为；它是定义它的培训。**

- *基于值的方法：* **间接地，通过训练** 输出状态值或状态-动作对的值函数。鉴于这个价值函数，我们的策略 **将采取动作。**

但是，因为我们没有训练我们的策略， **我们需要指定它的行为。** 例如，如果我们想要一个策略，在给定价值函数的情况下，将采取总是导致最大回报的动作， **我们将创建一个贪婪策略。**

![两种强化学习方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-approaches-3.jpg)给定一个状态，我们的动作值函数（我们训练的）输出该状态下每个动作的值，然后我们的贪婪策略（我们定义的）选择具有最大状态-动作对值的动作。

因此，无论您使用什么方法来解决您的问题， **您都会有一个 policy**，但在您不训练它的基于值的方法的情况下，您的 policy **只是您指定的一个简单函数** （例如贪婪策略）和该策略**使用价值函数给出的值来选择其操作。**

所以区别在于：

- 在基于策略的情况下， **通过直接训练策略来找到最优策略。**
- 在基于价值的情况下， **找到最佳价值函数会间接获得最佳策略。**

![价值与策略之间的联系](https://huggingface.co/blog/assets/70_deep_rl_q_part1/link-value-policy.jpg)

事实上，大多数时候，在基于价值的方法中，您将使用 **Epsilon-Greedy 策略** 来处理探索/利用权衡；我们在本单元的第二部分讨论 Q-Learning 时，我们再讨论它。

因此，我们有两种类型的基于值的函数：

### **状态值函数**

我们在策略 π 下编写状态值函数，如下所示：

![状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/state-value-function-1.jpg)

对于每个状态，如果智能体 **从该状态开始，状态值函数会输出期望回报，** 然后永远遵循该策略（如果您愿意，可以用于所有未来的时间步长）。

![状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/state-value-function-2.jpg)如果我们取值为 -7 的状态：它是从该状态开始并根据我们的策略（贪婪策略）采取动作的期望回报，所以正确，正确，正确，向下，向下，正确，正确。

### **动作价值函数**

在动作值函数中，对于每个状态和动作对，动作值函数 **输出如果智能体在该状态下开始并采取动作的期望回报** ，然后永远遵循策略。

在策略 π 下，在状态 s 中采取动作的价值是：

![动作状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/action-state-value-function-1.jpg)

![动作状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/action-state-value-function-2.jpg)

我们看到区别在于：

- 在状态值函数中，我们计算 **状态的值   $ S_t $ **
- 在动作-价值函数中，我们计算 **状态-动作对的值 $(S_t, A_t)$ , 即在该状态下采取该动作的价值。**

![两种类型的价值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-types.jpg)注意：对于动作值函数的例子，我们没有填写所有的状态动作对

在任何一种情况下，无论我们选择什么价值函数（状态-价值函数 或 动作-价值函数）， **价值都是期望回报。**意味着 **要计算状态或状态-动作对的每个值，我们需要将智能体从该状态开始可以获得的所有奖励相加。**

这可能是一个乏味的过程，而这 **正是贝尔曼方程可以帮助我们的地方。**

## **贝尔曼方程：简化价值估计**

贝尔曼方程 **简化了状态值或状态-动作价值的计算。**

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman.jpg)

根据我们目前学到的知识，我们知道，如果我们计算 $V(S_t)$（状态的值），我们需要从该状态开始计算回报，然后永远遵循该策略。 **（我们在以下示例中定义的策略是贪婪策略，为简化起见，我们不对奖励打折扣）。**

所以要计算$V(S_t) $，我们需要做期望奖励的总和。因此：

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman2.jpg)计算状态 1 的值：如果智能体从该状态开始，然后在所有时间步中遵循贪婪策略（采取导致最佳状态值的动作)，则奖励的总和。

然后，计算 $V(S_{t+1})$，我们需要计算从  $S_{t+1}$ 状态开始的回报.

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman3.jpg)计算状态 2 的值：奖励的总和 **如果智能体在该状态下开始，然后遵循所有时间步的**策略。

所以你看，如果你需要为每个状态值或状态动作值做这件事，那将是一个相当乏味的过程。

**我们可以使用贝尔曼方程**，而不是计算每个状态或每个状态-动作对的期望回报 。

贝尔曼方程是一个递归方程，它的工作原理如下：我们可以将任何状态的值视为：

**即时奖励 $R_{t+1}$ + 紧随其后的状态的贴现值  ( gamma  $V(S_{t+1})$). **

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman4.jpg)为了简单起见，我们不打折扣，所以 gamma = 1。

我们回到示例中，状态 1 的值 = 如果我们从该状态开始，期望的累积回报。

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman2.jpg)

计算状态 1 的值： **如果智能体从状态 1 开始，** 然后在 **所有时间步都遵循策略，则奖励的总和。**

这相当于 $V(S_{t})$ = 立即奖励 $R_{t+1}$ + 下一个状态的折扣值 gamma * $V(S_{t+1})$

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman6.jpg)

为简单起见，这里我们不打折扣，所以 gamma = 1。

- 的价值 $V(S_{t+1})$= 立即奖励 $R_{t+2}$ + 下一个状态的折扣值  gamma * $V(S_{t+2})$。
- 等等。

回顾一下，贝尔曼方程的想法是，不是将每个值计算为期望收益的总和， **这是一个漫长的过程。** 这相当于 **立即奖励的总和 + 随后状态的折现值。**

## **蒙特卡洛与时间差分学习**

在深入 Q-Learning 之前，我们需要谈论的最后一件事是两种学习方式。

记住，RL 智能体 **通过与其环境交互来学习。** 这个想法是， **使用获得的经验**，给定它获得的奖励，将 **更新其价值或策略。**

蒙特卡洛和时间差异学习是 **关于如何训练我们的价值函数或策略函数的两种不同策略。** 他们都 **使用经验来解决 RL 问题。**

一方面，蒙特卡洛 **在学习之前使用了一整段经验。** 另一方面，Temporal Difference **仅使用一个步骤 $(S_t, A_t, R_{t+1}, S_{t+1})$ 学习。**

**我们将使用基于值的方法示例来**解释它们 。

### **蒙特卡洛：在 Episode 结束时学习**

蒙特卡洛等到这 Episode 结束，计算 $G_t$(return) 并将其用作 **更新的目标 $V(S_t)$.**

因此， **在更新我们的价值函数之前，它需要一个完整的交互片段。**

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/monte-carlo-approach.jpg)

如果我们举个例子：

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-2.jpg)

- 我们总是从 **同一个起点开始Episode。**
- **智能体使用策略采取动作**。例如，使用 Epsilon Greedy 策略，一种在探索（随机动作）和利用之间交替的策略。
- 我们得到 **奖励和下一个状态。**
- 如果猫吃掉了老鼠或者老鼠移动了 > 10 步，我们就会终止这一集。
- 在这一集的结尾， **我们有一个状态、动作、奖励和下一个状态的列表**
- **智能体将汇总 总奖励 $G_t$ （看看效果如何）。
- 然后会 基于公式 更新 $V(s_t)$

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-3.jpg)

- 然后 **用这些新知识开始新游戏**

通过运行越来越多的 ， **智能体将学会玩得越来越好。**

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-3p.jpg)

例如，如果我们使用 Monte Carlo 训练状态值函数：

- 我们刚刚开始训练我们的 Value 函数， **所以它为每个状态返回 0 值**
- 我们的学习率 (lr) 为 0.1，折扣率为 1（= 无折扣）
- 我们的鼠标 **探索环境并采取随机动作**

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-4.jpg)

- 老鼠走了十多步，这一集就结束了。

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-4p.jpg)

- 我们有一个state、action、rewards、next_state的列表， **我们需要计算 return $G{t}$**
- $G_t = R_{t+1} + R_{t+2} + R_{t+3} ...$
- $G_t = R_{t+1} + R_{t+2} + R_{t+3}…$（为简单起见，我们不打折奖励）。
- $G_t = 1 + 0 + 0 + 0+ 0 + 0 + 1 + 1 + 0 + 0$
- $G_t= 3$
- 我们现在可以更新$V(S_0)$：

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-5.jpg)

- 新的 $V(S_0) = V(S_0) + lr * [G_t — V(S_0)]$
- 新的 $V(S_0) = 0 + 0.1 * [3 – 0]*Ⅴ* ( *S*0)=0+0 . 1*[ 3 - 0 ]$
- 新的 $V(S_0) = 0.3$

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-5p.jpg)

### **时间差异学习：在每一步学习**

- 另一方面，时间差异只等待一个交互（一步）$S_{t+1}$
- 形成一个TD目标并更新 $V(S_t)$ 使用 $R_{t+1}$ 和 gamma * $V(S_{t+1}) $.

TD的想法 **是更新 $ V(S_t)$ 在每一步。**

但是因为我们没有在整个Episode中播放，所以我们没有G_t（期望收益）。相反，**我们估计G_t 通过添加 $R_{t+1}$以及下一个状态的贴现值。**

这称为引导。之所以这样称呼，是因为 TD 的更新部分基于现有的估计 $V(S_{t+1})$ 而不是一个完整的样本 $G_t$

![时间差异](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-1.jpg)

此方法称为 TD(0) 或 **单步 TD（在任何单个步骤后更新值函数）。**

![时间差异](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-1p.jpg)

如果我们举同样的例子，

![时间差异](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-2.jpg)

- 我们刚刚开始训练我们的 Value 函数，所以它为每个状态返回 0 值。
- 我们的学习率 (lr) 是 0.1，我们的折扣率是 1（没有折扣）。
- 我们的鼠标探索环境并采取随机动作： **向左移动**
- 它得到了奖励 $R_{t+1} = 1 $因为 **它吃一块奶酪**

![时间差异](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-2p.jpg)

![时间差异](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-3.jpg)

我们现在可以更新  $V(S_0)$：

新的  $V(S_0) = V(S_0) + lr * [R_1 + gamma * V(S_1) - V(S_0)]$

新的  $V(S_0) = 0 + 0.1 * [1 + 1 * 0–0]$

新的   $V(S_0) = 0.1$

所以我们刚刚更新了状态 0 的价值函数。

现在我们 **继续使用我们更新的价值函数与这个环境进行交互。**

![时间差异](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-3p.jpg)

如果我们总结一下：

- 使用蒙特卡洛，我们从一个完整的情节更新价值函数，因此我们 **使用这一情节的实际准确贴现回报。**
- 通过 TD learning，我们从一步更新价值函数，所以我们替换G_t*G**吨*我们没有 **估计回报，称为 TD 目标。**

![概括](https://huggingface.co/blog/assets/70_deep_rl_q_part1/Summary.jpg)

所以现在，在深入 Q-Learning 之前，让我们总结一下我们刚刚学到的内容：

我们有两种基于值的函数：

- 状态值函数：如果 **智能体从给定状态开始并在之后永远按照策略动作，则输出期望回报。**
- Action-Value 函数：如果智能体在给定状态下开始，则输出期望回报， **在该状态下采取给定动作，** 然后永远按照策略动作。
- 在基于价值的方法中， **我们手动定义策略，** 因为我们不训练它，我们训练一个价值函数。这个想法是，如果我们有一个最优的价值函数，我们 **就会有一个最优的策略。**

有两种方法可以学习价值函数的策略：

- 使用 *蒙特卡洛方法*，我们从一个完整的情节更新价值函数，因此我们 **使用这一情节的实际准确贴现回报。**
- 使用 *TD 学习方法，* 我们从一个步骤更新价值函数，因此我们将我们没有的 Gt 替换 **为称为 TD 目标的估计回报。**

![概括](https://huggingface.co/blog/assets/70_deep_rl_q_part1/summary-learning-mtds.jpg)
