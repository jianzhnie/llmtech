# Q-Learning 简介

在 [本课程的第一章中](https://huggingface.co/blog/deep-rl-intro)，学习了强化学习 (RL)、RL 过程以及解决 RL 问题的不同方法。今天，将 **深入研究其中一种强化学习方法：基于价值的方法** ，并研究的第一个 RL 算法： **Q-Learning。**

本单元分为 2 个部分：

![两部分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two_parts.jpg)

在第一部分，将 **学习基于值的方法以及蒙特卡洛和时间差分学习之间的区别。**

在第二部分， **将研究第一个 RL 算法：Q-Learning，并实现第一个 RL 智能体。**

**如果你想学习深度 Q 学习**（第 3 单元）：第一个能够玩 Atari 游戏并 **在其中一些**游戏 （突破、太空入侵者……）上击败人类水平的深度 RL 算法. 学好本单元很有必要。

## **什么是强化学习？简短回顾**

在 RL 中，构建了一个可以 **做出明智决策**的智能体。例如，一个 **学习玩电子游戏的智能体。** 或者是一个交易智能体， 通过**对购买哪些股票和何时出售** 做出明智的决策 来**学习最大化其利益。**

![强化学习过程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/rl-process.jpg)

但是，为了做出明智的决策，智能体将 **通过反复试验与环境交互** 并接收奖励（正面或负面） **作为反馈，从而从环境中学习。**

它的目标 **是最大化其期望累积奖励** （由于奖励假设）。

**智能体的决策过程称为策略 π：** 给定 一个状态，策略将输出一个动作或动作的概率分布。也就是说，给定对环境的观察，策略将提供智能体应采取的动作（或每个动作的多个概率）。

![策略](https://huggingface.co/blog/assets/70_deep_rl_q_part1/policy.jpg)

我们的目标是找到最优策略 π，也就是导致最佳期望累积奖励的策略。

为了找到这个最优策略（从而解决 RL 问题），有 **两种主要的 RL 方法**：

- *基于策略的方法*： **直接训练策略函数** 以学习在给定状态下要采取的动作。
- *基于价值的方法*： **训练一个价值函数**来学习 **哪个状态更有价值** ，并使用这个价值函数**来采取导致它的动作。**

![两种强化学习方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-approaches.jpg)

在本章中， **将深入探讨基于价值的方法。**

## **两种基于值的方法**

在基于值的方法中， 学习了一个**将状态映射到处于该状态的期望值** 的价值函数。

![基于价值的方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/vbm-1.jpg)

状态的值是智能体**从该状态开始然后按照策略行动时** 可以获得的**期望折扣回报。**

> 但是，按照策略动作意味着什么？毕竟，没有基于价值的方法的策略，因为训练的是价值函数而不是策略。

请记住， **RL 智能体的目标是拥有最优策略 π。**

为了找到它，有两种不同的方法：

- *基于策略的方法：* **直接训练策略** 以选择在给定状态下要采取的动作（或在该状态下的动作的概率分布）。在这种情况下， **没有价值函数。**

![两种强化学习方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-approaches-2.jpg)

该策略将状态作为输入并输出在该状态下要采取的动作（确定性策略）。

- *基于值的方法：* **间接地，通过训练** 输出状态值或状态-动作对的值函数。根据这个价值函数，我们的策略**将采取导致值最大的动作。**

但是，因为我们没有训练好策略函数， **需要指定它的行为。** 例如，如果想要一个策略，在给定价值函数的情况下，将采取总是导致最大回报的动作， **这将创建一个贪婪策略。**

![两种强化学习方法](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-approaches-3.jpg)给定一个状态，动作值函数（训练的）输出该状态下每个动作的值，然后的贪婪策略（定义的)选择具有最大状态-动作对值的动作。

因此，无论您使用什么方法来解决问题， **您都会有一个 policy**，但在您不训练它的基于值的方法的情况下，您的 policy **只是您指定的一个简单函数** （例如贪婪策略）和该策略**使用价值函数给出的值来选择其操作。**

所以区别在于：

- 在基于策略的情况下， **通过直接训练策略来找到最优策略。**
- 在基于价值的情况下， **找到最佳价值函数会间接获得最佳策略。**

![价值与策略之间的联系](https://huggingface.co/blog/assets/70_deep_rl_q_part1/link-value-policy.jpg)

事实上，大多数时候，在基于价值的方法中，您将使用 **Epsilon-Greedy 策略** 来处理探索/利用权衡；在本单元的第二部分讨论 Q-Learning 时，再讨论它。

因此，有两种类型的基于值的函数：

### **状态值函数**

在策略 π 下编写状态值函数，如下所示：

![状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/state-value-function-1.jpg)

对于每个状态，如果智能体 **从该状态开始，状态值函数会输出期望回报，** 然后永远遵循该策略（如果您愿意，可以用于所有未来的时间步长）。

![状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/state-value-function-2.jpg)如果取值为 -7 的状态：它是从该状态开始并根据我们的策略（贪婪策略) 采取动作的期望回报。

### **动作价值函数**

在动作值函数中，对于每个状态和动作对，动作值函数 **输出如果智能体在该状态下开始并采取动作的期望回报** ，然后永远遵循该策略。

在策略 π 下，在状态 s 中采取动作的价值是：

![动作状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/action-state-value-function-1.jpg)

![动作状态值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/action-state-value-function-2.jpg)

区别在于：

- 在状态值函数中，计算 **状态的值   $ S_t $ **
- 在动作-价值函数中，计算 **状态-动作对的值 $(S_t, A_t)$ , 即在该状态下采取该动作的价值。**

![两种类型的价值函数](https://huggingface.co/blog/assets/70_deep_rl_q_part1/two-types.jpg)注意：对于动作值函数的例子，没有填写所有的状态动作对

在任何一种情况下，无论选择什么价值函数（状态-价值函数 或 动作-价值函数）， **价值都是期望回报。**意味着 **要计算状态或状态-动作对的每个值，需要将智能体从该状态开始可以获得的所有奖励相加。**

这可能是一个乏味的过程，而这**正是贝尔曼方程可以帮助的地方。**

## **贝尔曼方程：简化价值估计**

贝尔曼方程 **简化了状态值或状态-动作价值的计算。**

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman.jpg)

根据目前学到的知识，如果计算 $V(S_t)$（状态的值），需要从该状态开始计算回报，然后永远遵循该策略。 **（在以下示例中定义的策略是贪婪策略，为简化起见，不对奖励打折扣）。**

所以要计算$V(S_t) $，需要做期望奖励的总和。因此：

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman2.jpg)计算状态 1 的值：如果智能体从该状态开始，然后在所有时间步中遵循贪婪策略（采取导致最佳状态值的动作)，则奖励的总和。

然后，计算 $V(S_{t+1})$，需要计算从  $S_{t+1}$ 状态开始的回报.

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman3.jpg)计算状态 2 的值：奖励的总和 **如果智能体在该状态下开始，然后遵循所有时间步的**策略。

所以你看，如果你需要为每个状态值或状态动作值做这件事，那将是一个相当乏味的过程。

**可以使用贝尔曼方程**，而不是计算每个状态或每个状态-动作对的期望回报 。

贝尔曼方程是一个递归方程，它的工作原理如下：可以将任何状态的值视为：

**即时奖励 $R_{t+1}$ + 紧随其后的状态的贴现值  ( gamma  $V(S_{t+1})$). **

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman4.jpg)为了简单起见，不打折扣，所以 gamma = 1。

回到示例中，状态 1 的值 = 如果从该状态开始，期望的累积回报。

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman2.jpg)

计算状态 1 的值： **如果智能体从状态 1 开始，** 然后在 **所有时间步都遵循策略，则奖励的总和。**

这相当于 $V(S_{t})$ = 立即奖励 $R_{t+1}$ + 下一个状态的折扣值 gamma * $V(S_{t+1})$

![贝尔曼方程](https://huggingface.co/blog/assets/70_deep_rl_q_part1/bellman6.jpg)

为简单起见，这里不打折扣，所以 gamma = 1。

- 价值 $V(S_{t+1})$= 立即奖励 $R_{t+2}$ + 下一个状态的折扣值  gamma * $V(S_{t+2})$。

回顾一下，贝尔曼方程的想法是，不是将每个值计算为期望收益的总和， 相当于 **立即奖励的总和 + 随后状态的折现值。**

## **蒙特卡洛与时间差分学习**

在深入 Q-Learning 之前，需要谈论的最后一件事是两种学习方式。RL 智能体 **通过与其环境交互来学习。** 这个想法是， **使用获得的经验**，给定它获得的奖励，将 **更新其价值或策略。**

蒙特卡洛和时间差分学习是 **关于如何训练的价值函数或策略函数的两种不同策略。** 他们都 **使用经验来解决 RL 问题。**蒙特卡洛 **在学习之前使用了一整段经验。** Temporal Difference **仅使用一个步骤 $(S_t, A_t, R_{t+1}, S_{t+1})$ 学习。将使用基于值的方法示例来**解释它们 。

### **蒙特卡洛：在 Episode 结束时学习**

蒙特卡洛等到 Episode 结束，计算 $G_t$(return) 并将其用作 **更新的目标 $V(S_t)$.**

因此， **在更新的价值函数之前，它需要一个完整的交互片段。**

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/monte-carlo-approach.jpg)

如果举个例子：

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-2.jpg)

- 总是从 **同一个起点开始Episode。**
- **智能体使用策略采取动作**。例如，使用 Epsilon Greedy 策略，一种在探索（随机动作）和利用之间交替的策略。
- 得到 **奖励和下一个状态。**
- 如果猫吃掉了老鼠或者老鼠移动了 > 10 步，就会终止这一集。
- 在这一集的结尾， **有一个状态、动作、奖励和下一个状态的列表**
- 智能体将汇总 总奖励 $G_t$ （看看效果如何）。
- 然后会 基于公式 更新 $V(s_t)$

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-3.jpg)

- 然后 **用这些新知识开始新游戏**

通过运行越来越多的次数 ， **智能体将学会玩得越来越好。**

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-3p.jpg)

例如，如果使用 Monte Carlo 训练状态值函数：

- 刚刚开始训练 Value 函数， **每个状态初始化 0 值**
- 学习率 (lr) 为 0.1，折扣率为 1（= 无折扣）
- 老鼠 **探索环境并采取随机动作**

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-4.jpg)

- 老鼠走了十多步，这一集就结束了。

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-4p.jpg)

- 有一个state、action、rewards、next_state的列表， **需要计算 return $G{t}$**
- $G_t = R_{t+1} + R_{t+2} + R_{t+3} ...$
- $G_t = R_{t+1} + R_{t+2} + R_{t+3}…$（为简单起见，不打折奖励）。
- $G_t = 1 + 0 + 0 + 0+ 0 + 0 + 1 + 1 + 0 + 0$
- $G_t= 3$
- 现在可以更新$V(S_0)$：

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-5.jpg)

- 新的 $V(S_0) = V(S_0) + lr * [G_t — V(S_0)]$
- 新的 $V(S_0) = 0 + 0.1 * [3 – 0]*Ⅴ* ( *S*0)=0+0 . 1*[ 3 - 0 ]$
- 新的 $V(S_0) = 0.3$

![蒙特卡洛](https://huggingface.co/blog/assets/70_deep_rl_q_part1/MC-5p.jpg)

### **时间差分学习：在每一步学习**

- 另一方面，时间差分只等待一个交互（一步）$S_{t+1}$
- 形成一个TD目标并更新 $V(S_t)$ 使用 $R_{t+1}$ 和 gamma * $V(S_{t+1}) $.

TD的想法 **是更新 $ V(S_t)$ 在每一步。**

但是因为没有在整个Episode中播放，所以没有$G_t$（期望收益）。相反，**估计G_t 通过添加 $R_{t+1}$以及下一个状态的贴现值。**

这称为引导。之所以这样称呼，是因为 TD 的更新部分基于现有的估计 $V(S_{t+1})$ 而不是一个完整的样本 $G_t$

![时间差分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-1.jpg)

此方法称为 TD(0) 或 **单步 TD（在任何单个步骤后更新值函数）。**

![时间差分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-1p.jpg)

如果举同样的例子，

![时间差分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-2.jpg)

- 刚刚开始训练Value 函数，它为每个状态初始化 0 值。
- 学习率 (lr) 是 0.1，折扣率是 1（没有折扣）。
- 老鼠探索环境并采取随机动作： **向左移动**
- 它得到了奖励 $R_{t+1} = 1 $ 因为 **它吃一块奶酪**

![时间差分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-2p.jpg)

![时间差分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-3.jpg)

现在可以更新  $V(S_0)$：

新的  $V(S_0) = V(S_0) + lr * [R_1 + gamma * V(S_1) - V(S_0)]$

新的  $V(S_0) = 0 + 0.1 * [1 + 1 * 0–0]$

新的   $V(S_0) = 0.1$

所以刚刚更新了状态 0 的价值函数。

现在 **继续使用更新的价值函数与这个环境进行交互。**

![时间差分](https://huggingface.co/blog/assets/70_deep_rl_q_part1/TD-3p.jpg)

总结一下：

- 使用蒙特卡洛，从一个完整的 episode 更新价值函数，因此 **使用这一episode的实际准确折扣报。**
- 通过 TD learning，从一步更新价值函数，所以替换$G_t$没有 **估计回报，称为 TD 目标。**

![概括](https://huggingface.co/blog/assets/70_deep_rl_q_part1/Summary.jpg)

所以现在，在深入 Q-Learning 之前，总结一下刚刚学到的内容：

有两种基于值的函数：

- 状态值函数：如果 **智能体从给定状态开始并在之后永远按照策略动作，输出期望回报。**
- Action-Value 函数：如果智能体在给定状态下开始，则输出期望回报， **在该状态下采取给定动作，** 然后永远按照策略动作。
- 在基于价值的方法中， **手动定义策略，** 因为不训练它，训练一个价值函数。这个想法是，如果有一个最优的价值函数， **就会有一个最优的策略。**

有两种方法可以学习价值函数的策略：

- 使用 *蒙特卡洛方法*，从一个完整的episode更新价值函数，因此 **使用这一episode的实际准确折扣报。**
- 使用 *TD 学习方法，* 从一个步骤更新价值函数，因此将没有的 Gt 替换 **为称为 TD 目标的估计回报。**

![概括](https://huggingface.co/blog/assets/70_deep_rl_q_part1/summary-learning-mtds.jpg)
