- [入门教程](deep-rl/deep-rl-class/README.md)
  - [第一章：深度强化学习简介](deep-rl/deep-rl-class/ch1_introduction.md)
  - [第二章：Q-Learning ](deep-rl/deep-rl-class/ch2_q-learning.md)
  - [第三章：Deep Q-Learning ](deep-rl/deep-rl-class/ch3_dqn.md)
  - [第四章：Policy Gradient  ](deep-rl/deep-rl-class/ch4_pg.md)
  - [第五章：Actor-Critic](deep-rl/deep-rl-class/ch5_a2c.md)
  - [第六章：近端策略优化 (PPO)](deep-rl/deep-rl-class/ch6_ppo.md)
  - [第七章：Decision Transfomer](deep-rl/deep-rl-class/ch7_decision-transformer.md)
  - [第八章：Multi-Agent RL](deep-rl/deep-rl-class/ch8_marl.md)
  - [第九章：RLHF](deep-rl/papers/RLHF.md)

- [进阶教程](deep-rl/algorithms/README.md)
  - [Policy gradient theorem的证明](deep-rl/algorithms/ch1_supp_pg.md)
  - [为什么A2C中减去 baseline 函数可以减小方差](deep-rl/algorithms/ch1_supp_a2c.md)
  - [步步深入TRPO](deep-rl/algorithms/ch1_supp_trpo.md)
  - [混合动作空间表征学习方法介绍（HyAR）](deep-rl/algorithms/ch2_supp_hyar.md)
  - [为什么 PPO 需要重要性采样, 而 DDPG 这个 off-policy 算法不需要](deep-rl/algorithms/ch2_supp_ppovsddpg.md)
  - [重参数化与强化学习](deep-rl/algorithms/ch2_supp_reparameterization.md)

- 强化学习环境
- [Awesome RL Envs](deep-rl/rltools/awesomeRLtools.md)
- [OpenAI Gym](deep-rl/envs/gym.md)
- [SMAC](deep-rl/envs/smac.md)
- [MARL Envs](deep-rl/envs/marl_env.md)


- 强化学习工具篇
  - [EnvPool: 并行环境模拟器](deep-rl/rltools/envpool.md)
  - [多智能体强化学习代码汇总](deep-rl/rltools/marltool.md)

- MuZero 解读
  
  - [蒙特卡洛树搜索 (MCTS)](deep-rl/muzero/MCTS.md)
  - [MuZero算法介绍](deep-rl/papers/muzero_intro.md)
  - [MuZero伪代码](deep-rl/papers/muzero_pseudocode.md)
  
- 强化学习[论文/算法]
- 多智能体强化学习

  - [MARL](deep-rl/papers/Overview.md)
  - [DRQN](deep-rl/papers/DRQN.md)
  - [IQL](deep-rl/papers/IQL.md)
  - [COMA](deep-rl/papers/COMA.md)
  - [VDN](deep-rl/papers/VDN.md)
  - [QTRAN](deep-rl/papers/QTRAN.md)
  - [QMIX](deep-rl/papers/QMIX.md)
  - [MADDPG](deep-rl/papers/MADDPG.md)
  - [MAT](deep-rl/papers/MAT.md)
