
## 第 $\mathbf{1 0}$ 章 连续控制

本书前面章节的内容全部都是离散控制, 即动作空间是一个离散的集合, 比如超级 玛丽游戏中的动作空间 $\mathcal{A}=\{$ 左, 右, 上 $\}$ 就是个离散集合。本章的内容是连续控制, 即 动作空间是个连续集合, 比如汽车的转向 $\mathcal{A}=\left[-40^{\circ}, 40^{\circ}\right]$ 就是连续集合。如果把连续动 作空间做离散化, 那么离散控制的方法就能直接解决连续控制问题; 我们在第10.1节讨 论连续集合的离散化。然而更好的办法是直接用连续控制方法, 而非离散化之后借用离 散控制方法。本章介绍两种连续控制方法：第10.2节介绍确定策略网络, 第10.5节介绍随 机策略网络。

## $10.1$ 离散控制与连续控制的区别

考虑这样一个问题：我们需要控制一只机械手臂, 完成某些任务, 获取奖励。机械 手臂有两个关节, 分别可以在 $\left[0^{\circ}, 360^{\circ}\right]$ 与 $\left[0^{\circ}, 180^{\circ}\right]$ 的范围内转动。这个问题的自由度 是 $d=2$, 动作是二维向量, 动作空间是连续集合 $\mathcal{A}=[0,360] \times[0,180]$ 。

此前我们学过的强化学习方法全部都是针对离散动作空间, 不能直接解决上述连 续控制问题。想把此前学过的离散控制方法应用到连续控制上, 必须要对连续动作空 间做离散化 (网格化) 。比如把连续集合 $\mathcal{A}=[0,360] \times[0,180]$ 变成离散集合 $\mathcal{A}^{\prime}=$ $\{0,20,40, \cdots, 360\} \times\{0,20,40, \cdots, 180\}$ ，见图 $10.1$ 。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-157.jpg?height=325&width=625&top_left_y=1551&top_left_x=270)

连续动作空间

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-157.jpg?height=323&width=619&top_left_y=1552&top_left_x=1027)

离散化之后的动作空间

图 10.1: 对连续动作空间 $\mathcal{A}=[0,360] \times[0,180]$ 做离散化 (网格化)。

对动作空间做离散化之后, 就可以应用之前学过的方法训练 DQN 或者策略网络, 用 于控制机械手臂。可是用离散化解决连续控制问题有个缺点。把自由度记作 $d$ 。自由度 $d$ 越大, 网格上的点就越多, 而且数量随着 $d$ 指数增长, 会造成维度灾难。动作空间的 大小即网格上点的数量。如果动作空间太大, DQN 和策略网络的训练都变得很困难, 强 化学习的结果会不好。上述离散化方法只适用于自由度 $d$ 很小的情况; 如果 $d$ 不是很小, 就应该使用连续控制方法。后面两节介绍两种连续控制的方法。

## 2 确定策略梯度 (DPG)

确定策略梯度 (deterministic policy gradient, DPG) 是最常用的连续控制方法。DPG 是一种 actor-critic 方法, 它有一个策略网络（演员), 一个价值网络（评委)。策略网络 控制智能体做运动, 它基于状态 $s$ 做出动作 $a_{\circ}{ }^{1}$ 价值网络不控制智能体, 只是基于状态 $s$ 给动作 $\boldsymbol{a}$ 打分, 从而指导策略网络做出改进。图 $10.2$ 是两个神经网络的关系。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-158.jpg?height=334&width=1419&top_left_y=684&top_left_x=387)

图 10.2: 确定策略梯度 (DPG) 方法的示意图。策略网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 的输入是状态 $s$, 输出是动作 $\boldsymbol{a}(d$ 维向量)。价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 的输入是状态 $s$ 和动作 $\boldsymbol{a}$, 输出是价值（实数）。

### 1 策略网络和价值网络

本节的策略网络不同于前面章节的策略网络。在之前章节里, 策略网络 $\pi(a \mid s ; \boldsymbol{\theta})$ 是 一个概率质量函数, 它输出的是概率值。本节的确定策略网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 的输出是 $d$ 维的向 量 $\boldsymbol{a}$, 作为动作。两种策略网络一个是随机的, 一个是确定性的:

- 之前章节中的策略网络 $\pi(a \mid s ; \boldsymbol{\theta})$ 带有随机性：给定状态 $s$, 策略网络输出的是离散 动作空间 $\mathcal{A}$ 上的概率分布; $\mathcal{A}$ 中的每个元素（动作）都有一个概率值。智能体依 据概率分布, 随机从 $\mathcal{A}$ 中抽取一个动作, 并执行动作。

- 本节的确定策略网络没有随机性：对于确定的状态 $s$, 策略网络 $\boldsymbol{\mu}$ 输出的动作 $\boldsymbol{a}$ 是 确定的。动作 $\boldsymbol{a}$ 直接是 $\boldsymbol{\mu}$ 的输出, 而非随机抽样得到的。

确定策略网络 $\boldsymbol{\mu}$ 的结构如图 $10.3$ 所示。如果输入的状态 $s$ 是个矩阵或者张量（例如 图片、视频), 那么 $\boldsymbol{\mu}$ 就由若干卷积层、全连接层等组成。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-158.jpg?height=372&width=1283&top_left_y=2127&top_left_x=449)

图 10.3: 确定策略网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 的结构。输入是状态 $s$, 输出是动作 $\boldsymbol{a}$ 。

${ }^{1}$ 本节中, 动作 $\boldsymbol{a}$ 是一个 $d$ 维向量, 而不是离散集合中的一个元素。因此本节用粗体 $\boldsymbol{a}$ 表示动作的观测值。 确定策略可以看做是随机策略的一个特例。确定策略 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 的输出是 $d$ 维向量, 它 的第 $i$ 个元素记作 $\widehat{\mu}_{i}=[\boldsymbol{\mu}(s ; \boldsymbol{\theta})]_{i}$ 。定义下面这个随机策略：

$$
\pi(\boldsymbol{a} \mid s ; \boldsymbol{\theta}, \boldsymbol{\sigma})=\prod_{i=1}^{d} \frac{1}{\sqrt{6.28} \sigma_{i}} \cdot \exp \left(-\frac{\left[a_{i}-\widehat{\mu}_{i}\right]^{2}}{2 \sigma_{i}^{2}}\right) .
$$

这个随机策略是均值为 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 、协方差矩阵为 $\operatorname{diag}\left(\sigma_{1}, \cdots, \sigma_{d}\right)$ 的多元正态分布。本节的 确定策略可以看做是上述随机策略在 $\boldsymbol{\sigma}=\left[\sigma_{1}, \cdots, \sigma_{d}\right]$ 为全零向量时的特例。

本节的价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 是对动作价值函数 $Q_{\pi}(s, \boldsymbol{a})$ 的近似。价值网络的结构如 图 $10.4$ 所示。价值网络的输入是状态 $s$ 和动作 $\boldsymbol{a}$, 输出的价值 $\widehat{q}=q(s, \boldsymbol{a} ; \boldsymbol{w})$ 是个实数, 可以反映动作的好坏; 动作 $\boldsymbol{a}$ 越好, 则价值 $\widehat{q}$ 就越大。所以价值网络可以评价策略网络 的表现。在训练的过程中, 价值网络帮助训练策略网络; 在训练结束之后, 价值网络就 被丢弃, 由策略网络控制智能体。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-159.jpg?height=400&width=1442&top_left_y=1028&top_left_x=247)

价值网 络

图 10.4: 价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 的结构。输入是状态 $s$ 和动作 $\boldsymbol{a}$, 输出是实数。

### 2 算法推导

用行为策略收集经验 : 本节的确定策略网络属于异策略 (off-policy) 方法, 即行为 策略 (behavior policy) 可以不同于目标策略（target policy）。目标策略即确定策略网络 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}_{\text {now }}\right)$, 其中 $\boldsymbol{\theta}_{\text {now }}$ 是策略网络最新的参数。行为策略可以是任意的, 比如

$$
\boldsymbol{a}=\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}_{\text {old }}\right)+\boldsymbol{\epsilon} .
$$

公式的意思是行为策略可以用过时的策略网络参数, 而且可以往动作中加入噪声 $\epsilon \in \mathbb{R}^{d}$ 。 异策略的好处在于可以把收集经验与训练神经网络分割开; 把收集到的经验存入经验回 放数组 (replay buffer), 在做训练的时候重复利用收集到的经验。见图 $10.5$ 。

用行为策略控制智能体与环境交互, 把智能体的轨迹 (trajectory) 整理成 $\left(s_{t}, \boldsymbol{a}_{t}, r_{t}\right.$, $\left.s_{t+1}\right)$ 这样的四元组, 存入经验回放数组。在训练的时候, 随机从数组中抽取一个四元组, 记作 $\left(s_{j}, \boldsymbol{a}_{j}, r_{j}, s_{j+1}\right)$ 。在训练策略网络 $\mu(s ; \boldsymbol{\theta})$ 的时候, 只用到状态 $s_{j}$ 。在训练价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 的时候, 要用到四元组中全部四个元素: $s_{j}, \boldsymbol{a}_{j}, r_{j}, s_{j+1}$ 。

训练策略网络： 首先通俗解释训练策略网络的原理。如图 $10.6$ 所示, 给定状态 $s$, 策略网络输出一个动作 $\boldsymbol{a}=\boldsymbol{\mu}(s ; \boldsymbol{\theta})$, 然后价值网络会给 $\boldsymbol{a}$ 打一个分数： $\widehat{q}=q(s, \boldsymbol{a} ; \boldsymbol{w})$ 。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-160.jpg?height=340&width=1402&top_left_y=270&top_left_x=384)

图 10.5: DPG 属于异策略, 收集经验与更新策略分开做。

参数 $\boldsymbol{\theta}$ 影响 $\boldsymbol{a}$, 从而影响 $\widehat{q}$ 。分数 $\widehat{q}$ 可以反映出 $\boldsymbol{\theta}$ 的好坏程度。训练策略网络的目标就 是改进参数 $\boldsymbol{\theta}$, 使 $\widehat{q}$ 变得更大。把策略网络看做演员, 价值网络看做评委。训练演员（策 略网络）的目的就是让他迎合迎合评委（价值网络）的喜好, 改变自己的表演技巧（即 参数 $\boldsymbol{\theta})$, 使得评委打分 $\widehat{q}$ 的均值更高。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-160.jpg?height=338&width=1419&top_left_y=1073&top_left_x=387)

图 10.6: 给定状态 $s$, 策略网络的参数 $\boldsymbol{\theta}$ 会影响 $\boldsymbol{a}$, 从而影响 $\widehat{q}=q(s, \boldsymbol{a} ; \boldsymbol{w})$ 。

根据以上解释, 我们来推导目标函数。如果当前状态是 $s$, 那么价值网络的打分就 是：

$$
q(s, \boldsymbol{\mu}(s ; \boldsymbol{\theta}) ; \boldsymbol{w}) .
$$

我们希望打分的期望尽量高, 所以把目标函数定义为打分的期望：

$$
J(\boldsymbol{\theta})=\mathbb{E}_{S}[q(S, \boldsymbol{\mu}(S ; \boldsymbol{\theta}) ; \boldsymbol{w})] .
$$

关于状态 $S$ 求期望消除掉了 $S$ 的影响; 不管面对什么样的状态 $S$, 策略网络（演员）都 应该做出很好的动作, 使得平均分 $J(\theta)$ 尽量高。策略网络的学习可以建模成这样一个最 大化问题:

$$
\max _{\boldsymbol{\theta}} J(\boldsymbol{\theta}) .
$$

注意, 这里我们只训练策略网络, 所以最大化问题中的优化变量是策略网络的参数 $\theta$, 而 价值网络的参数 $\boldsymbol{w}$ 被固定住。

可以用梯度上升来增大 $J(\boldsymbol{\theta})$ 。每次用随机变量 $S$ 的一个观测值（记作 $s_{j}$ ）来计算梯 度：

$$
\boldsymbol{g}_{j} \triangleq \nabla_{\boldsymbol{\theta}} q\left(s_{j}, \boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}\right) ; \boldsymbol{w}\right) .
$$

它是 $\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta})$ 的无偏估计。 $\boldsymbol{g}_{j}$ 叫做确定策略梯度 (deterministic policy gradient), 缩写 DPG 。 可以用链式法则求出梯度 $\boldsymbol{g}_{j}$ 。复习一下链式法则。如果有这样的函数关系: $\theta \rightarrow a \rightarrow$ $q$, 那么 $q$ 关于 $\theta$ 的导数可以写成

$$
\frac{\partial q}{\partial \theta}=\frac{\partial a}{\partial \theta} \cdot \frac{\partial q}{\partial a}
$$

价值网络的输出与 $\boldsymbol{\theta}$ 的函数关系如图 10.6 所示。应用链式法则, 我们得到下面的定理。

## 定理 10.1. 确定策略梯度

$$
\nabla_{\boldsymbol{\theta}} q\left(s_{j}, \boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}\right) ; \boldsymbol{w}\right)=\nabla_{\boldsymbol{\theta}} \boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}\right) \cdot \nabla_{\boldsymbol{a}} q\left(s_{j}, \widehat{\boldsymbol{a}}_{j} ; \boldsymbol{w}\right), \quad \text { 其中 } \widehat{\boldsymbol{a}}_{j}=\boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}\right) .
$$

由此我们得到更新 $\boldsymbol{\theta}$ 的算法。每次从经验回放数组里随机抽取一个状态, 记作 $s_{j}$ 。 计算 $\widehat{\boldsymbol{a}}_{j}=\boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}\right)$ 。用梯度上升更新一次 $\boldsymbol{\theta}$ :

$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\beta \cdot \nabla_{\boldsymbol{\theta}} \boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}\right) \cdot \nabla_{\boldsymbol{a}} q\left(s_{j}, \widehat{\boldsymbol{a}}_{j} ; \boldsymbol{w}\right)
$$

此处的 $\beta$ 是学习率, 需要手动调。这样做梯度上升, 可以逐渐让目标函数 $J(\boldsymbol{\theta})$ 增大, 也 就是让评委给演员的平均打分更高。

训练价值网络：首先通俗解释训练价值网络的原理。训练价值网络的目标是让价值 网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 的预测越来越接近真实价值函数 $Q_{\pi}(s, \boldsymbol{a})$ 。如果把价值网络看做评委, 那 么训练评委的目标就是让他的打分越来越准确。每一轮训练都要用到一个实际观测的奖 励 $r$, 可以把 $r$ 看做 “真理”, 用它来校准评委的打分。

训练价值网络要用 TD 算法。这里的 TD 算法与之前学过的标准 actor-critic 类似, 都 是让价值网络去拟合 $\mathrm{TD}$ 目标。每次从经验回放数组中取出一个四元组 $\left(s_{j}, \boldsymbol{a}_{j}, r_{j}, s_{j+1}\right)$, 用它更新一次参数 $\boldsymbol{w}$ 。首先让价值网络做预测:

$$
\widehat{q}_{j}=q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}\right) \quad \text { 和 } \quad \widehat{q}_{j+1}=q\left(s_{j+1}, \boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}\right) ; \boldsymbol{w}\right) .
$$

计算 $\mathrm{TD}$ 目标 $\widehat{y}_{j}=r_{j}+\gamma \cdot \widehat{q}_{j+1}$ 。定义损失函数

$$
L(\boldsymbol{w})=\frac{1}{2}\left[q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}\right)-\widehat{y}_{j}\right]^{2},
$$

计算梯度

$$
\nabla_{\boldsymbol{w}} L(\boldsymbol{w})=\underbrace{\left(\widehat{q}_{j}-\widehat{y}_{j}\right)}_{\text {TD 误差 } \delta_{j}} \cdot \nabla_{\boldsymbol{w}} q\left(s_{j}, a_{j} ; \boldsymbol{w}\right),
$$

做一轮梯度下降更新参数 $\boldsymbol{w}$ :

$$
\boldsymbol{w} \leftarrow \boldsymbol{w}-\alpha \cdot \nabla_{\boldsymbol{w}} L(\boldsymbol{w})
$$

这样可以让损失函数 $L(\boldsymbol{w})$ 减小, 也就是让价值网络的预测 $\widehat{q}_{j}=q(s, \boldsymbol{a} ; \boldsymbol{w})$ 更接近 TD 目 标 $\widehat{y}_{j}$ 。公式中的 $\alpha$ 是学习率, 需要手动调。

训练流程 : 做训练的时候, 可以同时对价值网络和策略网络做训练。每次从经验回 放数组中抽取一个四元组, 记作 $\left(s_{j}, \boldsymbol{a}_{j}, r_{j}, s_{j+1}\right)$ 。把神经网络当前参数记作 $\boldsymbol{w}_{\text {now }}$ 和 $\boldsymbol{\theta}_{\text {now }}$ 。 执行以下步骤更新策略网络和价值网络：

1. 让策略网络做预测:

$$
\widehat{\boldsymbol{a}}_{j}=\boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}_{\text {now }}\right) \quad \text { 和 } \quad \widehat{\boldsymbol{a}}_{j+1}=\boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}_{\text {now }}\right) .
$$

注计算动作 $\widehat{\boldsymbol{a}}_{j}$ 用的是当前的策略网络 $\boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}_{\text {now }}\right)$, 用 $\widehat{\boldsymbol{a}}_{j}$ 来更新 $\boldsymbol{\theta}_{\text {now }}$; 而从经验回 放数组中抽取的 $\boldsymbol{a}_{j}$ 则是用过时的策略网络 $\boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}_{\text {old }}\right)$ 算出的, 用 $\boldsymbol{a}_{j}$ 来更新 $\boldsymbol{w}_{\text {now }}$ 。 请注意 $\widehat{\boldsymbol{a}}_{j}$ 与 $\boldsymbol{a}_{j}$ 的区别。

2. 让价值网络做预测:

$$
\widehat{q}_{j}=q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}_{\text {now }}\right) \quad \text { 和 } \quad \widehat{q}_{j+1}=q\left(s_{j+1}, \widehat{\boldsymbol{a}}_{j+1} ; \boldsymbol{w}_{\text {now }}\right) .
$$

3. 计算 TD 目标和 TD 误差：

$$
\widehat{y}_{j}=r_{j}+\gamma \cdot \widehat{q}_{j+1} \quad \text { 和 } \quad \delta_{j}=\widehat{q}_{j}-\widehat{y}_{j} .
$$

4. 更新价值网络：

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\alpha \cdot \delta_{j} \cdot \nabla_{\boldsymbol{w}} q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}_{\text {now }}\right)
$$

5. 更新策略网络:

$$
\boldsymbol{\theta}_{\text {new }} \leftarrow \boldsymbol{\theta}_{\text {now }}+\beta \cdot \nabla_{\boldsymbol{\theta}} \boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}_{\text {now }}\right) \cdot \nabla_{\boldsymbol{a}} q\left(s_{j}, \widehat{\boldsymbol{a}}_{j} ; \boldsymbol{w}_{\text {now }}\right) .
$$

在实践中, 上述算法的表现并不好; 读者应当采用第 $10.4$ 节介绍的技巧训练策略网络和 价值网络。

## $10.3$ 深入分析 DPG

上一节介绍的 DPG 是一种“四不像”的方法。DPG 乍看起来很像第 7 章中介绍的策略 学习方法, 因为 DPG 的目的是学习一个策略 $\boldsymbol{\mu}$, 而价值网络 $q$ 只起辅助作用。然而 DPG 又很像第 4 章中介绍的 DQN, 两者都是异策略 (Off-policy), 而且两者存在高估问题。鉴 于 DPG 的重要性, 我们更深入分析 DPG。

### 1 从策略学习的角度看待 DPG

## 问题 $\mathbf{1 0 . 1}$

DPG 中有一个确定策略网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 和一个价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 。请问价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 是对动作价值函数 $Q_{\pi}(s, \boldsymbol{a})$ 的近似, 还是对最优动作价值函数 $Q_{\star}(s, \boldsymbol{a})$ 的近似 ?

答案是动作价值函数 $Q_{\pi}(s, \boldsymbol{a})$ 。上一节 DPG 的训练流程中, 更新价值网络用到 TD 目标：

$$
\widehat{y}_{j}=r_{j}+\gamma \cdot q\left(s_{j+1}, \boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}_{\text {now }}\right) ; \boldsymbol{w}_{\text {now }}\right) .
$$

很显然, 当前的策略 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}_{\text {now }}\right)$ 会直接影响价值网络 $q$ 。策略不同, 得到的价值网络 $q$ 就 不同。

虽然价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 通常是对动作价值函数 $Q_{\pi}(s, \boldsymbol{a})$ 的近似, 但是我们最终的 目标是让 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 趋近于最优动作价值函数 $Q_{\star}(s, \boldsymbol{a})$ 。回忆一下, 如果 $\pi$ 是最优策略 $\pi^{\star}$, 那么 $Q_{\pi}(s, \boldsymbol{a})$ 就等于 $Q_{\star}(s, \boldsymbol{a})$ 。训练 DPG 的目的是让 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 趋近于最优策略 $\pi^{\star}$, 那么理想情况下, $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 最终趋近于 $Q_{\star}(s, \boldsymbol{a})$ 。

## 问题 $10.2$

DPG 的训练中有行为策略 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}_{\text {old }}\right)+\boldsymbol{\epsilon}$ 和目标策略 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}_{\text {now }}\right)$ 。价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 近似动作价值函数 $Q_{\pi}(s, \boldsymbol{a})$ 。请问此处的 $\pi$ 指的是行为策略还是目标策略？

答案是目标策略 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}_{\text {now }}\right)$, 因为目标策略对价值网络的影响很大。在理想情况下, 行为策略对价值网络没有影响。我们用 $\mathrm{TD}$ 算法训练价值网络, TD 算法的目的在于鼓励 价值网络的预测趋近于 TD 目标。理想情况下,

$$
q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}\right)=\underbrace{r_{j}+\gamma \cdot Q\left(s_{j+1}, \boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}_{\text {now }}\right) ; \boldsymbol{w}_{\text {now }}\right)}_{\text {TD 目标 }}, \quad \forall\left(s_{j}, \boldsymbol{a}_{j}, r_{j}, s_{j+1}\right) .
$$

在收集经验的过程中, 行为策略决定了如何基于 $s_{j}$ 生成 $\boldsymbol{a}_{j}$, 然而这不重要。上面的公式 只希望等式左边去拟合等式右边, 而不在乎 $\boldsymbol{a}_{j}$ 是如何生成的。

### 2 从价值学习的角度看待 DPG

假如我们知道最优动作价值函数 $Q_{\star}(s, a ; \boldsymbol{w})$, 我们可以这样做决策：给定当前状态 $s_{t}$, 选择最大化 $\mathrm{Q}$ 值的动作

$$
a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q_{\star}\left(s_{t}, a\right) .
$$

$\mathrm{DQN}$ 记作 $Q(s, a ; \boldsymbol{w})$, 它是 $Q_{\star}(s, a ; \boldsymbol{w})$ 的函数近似。训练 $\mathrm{DQN}$ 的目的是让 $Q(s, a ; \boldsymbol{w})$ 趋 近 $Q_{\star}(s, a ; \boldsymbol{w}), \forall s \in \mathcal{S}, a \in \mathcal{A}$ 。在训练好 $\mathrm{DQN}$ 之后, 可以这样做决策：

$$
a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q\left(s_{t}, a ; \boldsymbol{w}\right) .
$$

如果动作空间 $\mathcal{A}$ 是离散集合, 那么上述最大化很容易实现。可是如果 $\mathcal{A}$ 是连续集合, 则 很难对 $Q$ 求最大化。

可以把 DPG 看做对最优动作价值函数 $Q_{\star}(s, \boldsymbol{a})$ 的另一种近似方式, 用于连续控制问 题。我们希望学到策略网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 和价值网络 $q(s, a ; \boldsymbol{w})$, 使得

$$
q(s, \boldsymbol{\mu}(s ; \boldsymbol{\theta}) ; \boldsymbol{w}) \approx \max _{\boldsymbol{a} \in \mathcal{A}} Q_{\star}(s, \boldsymbol{a}), \quad \forall s \in \mathcal{S} .
$$

我们可以把 $\boldsymbol{\mu}$ 和 $q$ 看做是 $Q_{\star}$ 的近似分解, 而这种分解的目的在于方便做决策:

$$
\begin{aligned}
\boldsymbol{a}_{t} & =\boldsymbol{\mu}\left(s_{t} ; \boldsymbol{\theta}\right) \\
& \approx \underset{\boldsymbol{a} \in \mathcal{A}}{\operatorname{argmax}} Q_{\star}\left(s_{t}, \boldsymbol{a}\right) .
\end{aligned}
$$

### DPG 的高估问题

在第 $6.2$ 节中, 我们讨过 DQN 的高估问题: 如果用 $\mathrm{Q}$ 学习算法训练 $D Q N$, 则 DQN 会高估真实最优价值函数 $Q_{\star}$ 。把 $\mathrm{DQN}$ 记作 $Q(s, a ; \boldsymbol{w})$ 。如果用 $\mathrm{Q}$ 学习算法训练 $\mathrm{DQN}$, 那么 TD 目标是

$$
\widehat{y}_{j}=r_{j}+\gamma \cdot \max _{a \in \mathcal{A}} Q\left(s_{j+1}, a ; \boldsymbol{w}\right) .
$$

第 $6.2$ 节得出结论: 如果 $Q(s, a ; \boldsymbol{w})$ 是最优动作价值函数 $Q_{\star}(s, a)$ 的无偏估计, 那么 $\widehat{y}_{j}$ 是 对 $Q_{\star}\left(s_{j}, a_{j}\right)$ 的高估。用 $\widehat{y}_{j}$ 作为目标去更新 $\mathrm{DQN}$, 会导致 $Q\left(s_{j}, a_{j} ; \boldsymbol{w}\right)$ 高估 $Q_{\star}\left(s_{j}, a_{j}\right)$ 。 第 $6.2$ 节的另一个结论是自举会导致高估的传播, 造成高估越来越严重。

DPG 也存在高估问题, 用上一节的算法训练出的价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 会高估真实动 作价值 $Q_{\pi}(s, \boldsymbol{a})$ 。造成 DPG 高估的原因与 DQN 类似：第一, TD 目标是对真实动作价值 的高估; 第二, 自举导致高估的传播。下面具体分析两个原因; 如果读者不感兴趣, 只 需要记住上述结论即可, 可以跳过下面的内容。

最大化造成高估: 训练策略网络的时候, 我们希望策略网络计算出的动作 $\widehat{a}=$ $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 能得到价值网络尽量高的评价, 也就是让 $q(s, \widehat{\boldsymbol{a}} ; \boldsymbol{w})$ 尽量大。我们通过求解下 面的优化模型来学习策略网络:

$$
\boldsymbol{\theta}^{\star}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \mathbb{E}_{S}[q(S, \widehat{A} ; \boldsymbol{w})], \quad \text { s.t. } \widehat{A}=\boldsymbol{\mu}(S ; \boldsymbol{\theta}) .
$$

这个公式的意思是 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}^{\star}\right)$ 是最优的确定策略网络。上面的公式与下面的公式意义相同 (虽然不严格等价)：

$$
\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}^{\star}\right)=\underset{\boldsymbol{a} \in \mathcal{A}}{\operatorname{argmax}} q(s, \boldsymbol{a} ; \boldsymbol{w}), \quad \forall s \in \mathcal{S} .
$$

这个公式的意思也是 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}^{\star}\right)$ 是最优的确定策略网络。训练价值网络 $q$ 时用的 TD 目标 是

$$
\begin{aligned}
\widehat{y}_{j} & =r_{j}+\gamma \cdot q\left(s_{j+1}, \boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}\right) ; \boldsymbol{w}\right) \\
& \approx r_{j}+\gamma \cdot \max _{\boldsymbol{a}_{j+1}} q\left(s_{j+1}, \boldsymbol{a}_{j+1} ; \boldsymbol{w}\right) .
\end{aligned}
$$

根据第 $6.2$ 节中分析, 上面公式中的 $\max$ 会导致 $\widehat{y}_{j}$ 高估真实动作价值 $Q_{\pi}\left(s_{j}, a_{j} ; \boldsymbol{w}\right)$ 。在 训练 $q$ 时, 我们把 $\widehat{y}_{j}$ 作为目标, 鼓励价值网络 $q\left(s_{j}, a_{j} ; \boldsymbol{w}\right)$ 接近 $\widehat{y}_{j}$, 这会导致 $q\left(s_{j}, a_{j} ; \boldsymbol{w}\right)$ 高估真实动作价值。

自举造成偏差传播：我们在第 $6.2$ 节讨论过自举（bootstrapping）造成偏差的传播。 TD 目标

$$
\widehat{y}_{j}=r_{j}+\gamma \cdot q\left(s_{j+1}, \boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}\right) ; \boldsymbol{w}\right)
$$

是用价值网络算出来的, 而它又被用于更新价值网络 $q$ 本身, 这属于自举。假如价值网络 $q\left(s_{j+1}, \boldsymbol{a}_{j+1} ; \boldsymbol{\theta}\right)$ 高估了真实动作价值 $Q_{\pi}\left(s_{j+1}, \boldsymbol{a}_{j+1}\right)$, 那么 TD 目标 $\widehat{y}_{j}$ 则是对 $Q_{\pi}\left(s_{j}, \boldsymbol{a}_{j}\right)$ 的高估, 这会导致 $q\left(s_{j}, a_{j} ; \boldsymbol{w}\right)$ 高估 $Q_{\pi}\left(s_{j}, \boldsymbol{a}_{j}\right)$ 。自举让高估从 $\left(s_{j+1}, \boldsymbol{a}_{j+1}\right)$ 传播到 $\left(s_{j}, \boldsymbol{a}_{j}\right)$ 。

## $10.4$ 双延时确定策略梯度 (TD3)

由于存在高估等问题, DPG 实际运行的效果并不好。本节介绍的 Twin Delayed Deep Deterministic Policy Gradient (TD3) 可以大幅提升算法的表现, 把策略网络和价值网络训 练得更好。注意, 本节只是改进训练用的算法, 并不改变神经网络的结构。

### 1 高估问题的解决方案

解决方案-一目标网络：为了解决自举和最大化造成的高估, 我们需要使用目标网 络 (Target Networks) 计算 TD 目标 $\widehat{y}_{j}$ 。训练中需要两个目标网络：

$$
q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{-}\right) \text {和 } \boldsymbol{\mu}\left(s ; \boldsymbol{\theta}^{-}\right) .
$$

它们与价值网络、策略网络的结构完全相同, 但是参数不同。TD 目标是用目标网络算的:

$$
\widehat{y}_{j}=r_{j}+\gamma \cdot q\left(s_{j+1}, \widehat{\boldsymbol{a}}_{j+1} ; \boldsymbol{w}^{-}\right) \text {, 其中 } \widehat{\boldsymbol{a}}_{j+1}=\boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}^{-}\right) \text {. }
$$

把 $\widehat{y}_{j}$ 作为目标, 更新 $\boldsymbol{w}$, 鼓励 $q\left(s_{j}, a_{j} ; \boldsymbol{w}\right)$ 接近 $\widehat{y}_{j}^{-}$。四个神经网络之间的关系如图 $10.7$ 所示。这种方法可以在一定程度上缓解高估, 但是实验表明高估仍然很严重。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-166.jpg?height=394&width=1402&top_left_y=1268&top_left_x=384)

图 10.7: 四个神经网络之间的关系。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-166.jpg?height=500&width=1417&top_left_y=1852&top_left_x=385)

图 10.8: 截断双 $\mathrm{Q}$ 学习算法中六个神经网络之间的关系。

更好的解决方案――截断双 $\mathbf{Q}$ 学习 (clipped double $\mathbf{Q}$-learning)：这种方法使用两 个价值网络和一个策略网络:

$$
q\left(s, \boldsymbol{a} ; \boldsymbol{w}_{1}\right), \quad q\left(s, \boldsymbol{a} ; \boldsymbol{w}_{2}\right), \quad \boldsymbol{\mu}(s ; \boldsymbol{\theta}) .
$$

三个神经网络各对应一个目标网络：

$$
q\left(s, \boldsymbol{a} ; \boldsymbol{w}_{1}^{-}\right), \quad q\left(s, \boldsymbol{a} ; \boldsymbol{w}_{2}^{-}\right), \quad \boldsymbol{\mu}\left(s ; \boldsymbol{\theta}^{-}\right) .
$$

用目标策略网络计算动作：

$$
\widehat{\boldsymbol{a}}_{j+1}^{-}=\boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}^{-}\right),
$$

然后用两个目标价值网络计算:

$$
\begin{aligned}
& \widehat{y}_{j, 1}=r_{j}+\gamma \cdot q\left(s_{j+1}, \widehat{\boldsymbol{a}}_{j+1}^{-} ; \boldsymbol{w}_{1}^{-}\right), \\
& \widehat{y}_{j, 2}=r_{j}+\gamma \cdot q\left(s_{j+1}, \widehat{\boldsymbol{a}}_{j+1}^{-} ; \boldsymbol{w}_{2}^{-}\right) .
\end{aligned}
$$

取两者较小者为 $\mathrm{TD}$ 目标:

$$
\widehat{y}_{j}=\min \left\{\widehat{y}_{j, 1}, \widehat{y}_{j, 2}\right\} .
$$

截断双 $\mathrm{Q}$ 学习中的六个神经网络的关系如图 $10.8$ 所示。

### 2 其他改进方法

可以在截断双 $\mathrm{Q}$ 学习算法的基础上做两处小的改进, 进一步提升算法的表现。两种 改进分别是往动作中加噪声、减小更新策略网络和目标网络的频率。

往动作中加噪声 : 上一小节中截断双 $\mathrm{Q}$ 学习用目标策略网络计算动作: $\widehat{\boldsymbol{a}}_{j+1}^{-}=$ $\boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}^{-}\right)$。把这一步改成:

$$
\widehat{\boldsymbol{a}}_{j+1}^{-}=\boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}^{-}\right)+\boldsymbol{\xi} .
$$

公式中的 $\boldsymbol{\xi}$ 是个随机向量, 表示噪声, 它的每一个元素独立随机从截断正态分布 (clipped normal distribution) 中抽取。把截断正态分布记作 $\mathcal{C N}\left(0, \sigma^{2},-c, c\right)$, 意思是均值为零, 标 准差为 $\sigma$ 的正态分布, 但是变量落在区间 $[-c, c]$ 之外的概率为零。正态分布与截断正态 分布的对比如图 $10.9$ 所示。使用截断正态分布, 而非正态分布, 是为了防止噪声 $\boldsymbol{\xi}$ 过大。 使用截断, 保证噪声大小不会超过 $-c$ 和 $c$ 。
![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-167.jpg?height=450&width=1390&top_left_y=1916&top_left_x=274)

图 10.9: 正态分布 $\mathcal{N}\left(0,1^{2}\right)$ 和截断正态分布 $\mathcal{C N}\left(0,1^{2},-3,3\right)$ 。

减小更新策略网络和目标网络的频率 : Actor-critic 用价值网络来指导策略网络的更 新。如果价值网络 $q$ 本身不可靠, 那么用价值网络 $q$ 给动作打的分数是不准确的, 无助 于改进策略网络 $\boldsymbol{\mu}$ 。在价值网络 $q$ 还很差的时候就急于更新 $\boldsymbol{\mu}$, 非但不能改进 $\boldsymbol{\mu}$, 反而 会由于 $\boldsymbol{\mu}$ 的变化导致 $q$ 的训练不稳定。

实验表明, 应当让策略网络 $\boldsymbol{\mu}$ 以及三个目标网络的更新慢于价值网络 $q$ 。传统的 actor-critic 的每一轮训练都对策略网络、价值网络、以及目标网络做一次更新。更好的方 法是每一轮更新一次价值网络, 但是每隔 $k$ 轮更新一次策略网络和三个目标网络。 $k$ 是 超参数, 需要调。

### 3 训练流程}

本节介绍了三种技巧, 改进 DPG 的训练。第一, 用截断双 $\mathrm{Q}$ 学习, 缓解价值网络 的高估。第二, 往目标策略网络中加噪声, 起到平滑作用。第三, 降低策略网络和三个 目标网络的更新频率。使用这三种技巧的算法被称作双延时确定策略梯度（twin delayed deep deterministic policy gradient), 缩写是 TD3。

TD3 与 DPG 都属于异策略 (off-policy), 可以用任意的行为策略收集经验, 事后做 经验回放训练策略网络和价值网络。收集经验的方式与原始的训练算法相同, 用 $\boldsymbol{a}_{t}=$ $\boldsymbol{\mu}\left(s_{t} ; \boldsymbol{\theta}\right)+\boldsymbol{\epsilon}$ 与环境交互, 把观测到的四元组 $\left(s_{t}, \boldsymbol{a}_{t}, r_{t}, s_{t+1}\right)$ 存入经验回放数组。

初始的时候, 策略网络和价值网络的参数都是随机的。这样初始化目标网络的参数:

$$
\boldsymbol{w}_{1}^{-} \leftarrow \boldsymbol{w}_{1}, \quad \boldsymbol{w}_{2}^{-} \leftarrow \boldsymbol{w}_{2}, \quad \boldsymbol{\theta}^{-} \leftarrow \boldsymbol{\theta} .
$$

训练策略网络和价值网络的时候, 每次从数组中随机抽取一个四元组, 记作 $\left(s_{j}, \boldsymbol{a}_{j}, r_{j}, s_{j+1}\right)$ 。 用下标 now 表示神经网络当前的参数, 用下标 new 表示更新后的参数。然后执行下面的 步骤, 更新价值网络、策略网络、目标网络。

1. 让目标策略网络做预测: $\widehat{\boldsymbol{a}}_{j+1}^{-}=\boldsymbol{\mu}\left(s_{j+1} ; \boldsymbol{\theta}_{\text {now }}^{-}\right)+\boldsymbol{\xi}$ 。其中向量 $\boldsymbol{\xi}$ 的每个元素都独立 从截断正态分布 $\mathcal{C N}\left(0, \sigma^{2},-c, c\right)$ 中抽取。

2. 让两个目标价值网络做预测:

$$
\widehat{q}_{1, j+1}^{-}=q\left(s_{j+1}, \widehat{\boldsymbol{a}}_{j+1}^{-} ; \boldsymbol{w}_{1, \text { now }}^{-}\right) \quad \text { 和 } \quad \widehat{q}_{2, j+1}^{-}=q\left(s_{j+1}, \widehat{\boldsymbol{a}}_{j+1}^{-} ; \boldsymbol{w}_{2, \text { now }}^{-}\right) .
$$

3. 计算 TD目标:

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-168.jpg?height=105&width=647&top_left_y=1832&top_left_x=819)

4. 让两个价值网络做预测 :

$$
\widehat{q}_{1, j}=q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}_{1, \text { now }}\right) \quad \text { 和 } \quad \widehat{q}_{2, j}=q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}_{2, \text { now }}\right) .
$$

5. 计算 TD 误差:

$$
\delta_{1, j}=\widehat{q}_{1, j}-\widehat{y}_{j} \quad \text { 和 } \quad \delta_{2, j}=\widehat{q}_{2, j}-\widehat{y}_{j} .
$$

6. 更新价值网络:

$$
\begin{aligned}
& \boldsymbol{w}_{1, \text { new }} \leftarrow \boldsymbol{w}_{1, \text { now }}-\alpha \cdot \delta_{1, j} \cdot \nabla_{\boldsymbol{w}} q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}_{1, \text { now }}\right), \\
& \boldsymbol{w}_{2, \text { new }} \leftarrow \boldsymbol{w}_{2, \text { now }}-\alpha \cdot \delta_{2, j} \cdot \nabla_{\boldsymbol{w}} q\left(s_{j}, \boldsymbol{a}_{j} ; \boldsymbol{w}_{2, \text { now }}\right)
\end{aligned}
$$

7. 每隔 $k$ 轮更新一次策略网络和三个目标网络:

- 让策略网络做预测: $\widehat{\boldsymbol{a}}_{j}=\boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}\right)$ 。然后更新策略网络:

$$
\boldsymbol{\theta}_{\text {new }} \leftarrow \boldsymbol{\theta}_{\text {now }}+\beta \cdot \nabla_{\boldsymbol{\theta}} \boldsymbol{\mu}\left(s_{j} ; \boldsymbol{\theta}_{\text {now }}\right) \cdot \nabla_{\boldsymbol{a}} q\left(s_{j}, \widehat{\boldsymbol{a}}_{j} ; \boldsymbol{w}_{1, \text { now }}\right) .
$$

- 更新目标网络的参数:

$$
\begin{aligned}
\boldsymbol{\theta}_{\text {new }}^{-} & \leftarrow \tau \boldsymbol{\theta}_{\text {new }}+(1-\tau) \boldsymbol{\theta}_{\text {now }}^{-}, \\
\boldsymbol{w}_{1, \text { new }}^{-} & \leftarrow \tau \boldsymbol{w}_{1, \text { new }}+(1-\tau) \boldsymbol{w}_{1, \text { now }}^{-}, \\
\boldsymbol{w}_{2, \text { new }}^{-} & \leftarrow \tau \boldsymbol{w}_{2 \text { new }}+(1-\tau) \boldsymbol{w}_{2, \text { now }}^{-} .
\end{aligned}
$$



## $10.5$ 随机高斯策略

上一节用确定策略网络解决连续控制问题。本节用不同的方法做连续控制, 本节的 策略网络是随机的, 它是随机正态分布 (也叫高斯分布)。

### 1 基本思路

我们先研究最简单的情形: 自由度等于 1 , 也就是说动作 $a$ 是实数, 动作空间 $\mathcal{A} \subset \mathbb{R}$ 。 把动作的均值记作 $\mu(s)$, 标准差记作 $\sigma(s)$, 它们都是状态 $s$ 的函数。用正态分布的概率 密度函数作为策略函数：

$$
\pi(a \mid s)=\frac{1}{\sqrt{6.28} \cdot \sigma(s)} \cdot \exp \left(-\frac{[a-\mu(s)]^{2}}{2 \cdot \sigma^{2}(s)}\right) .
$$

假如我们知道函数 $\mu(s)$ 和 $\sigma(s)$ 的解析表达式, 可以这样做控制:

1. 观测到当前状态 $s$, 预测均值 $\widehat{\mu}=\mu(s)$ 和标准差 $\widehat{\sigma}=\sigma(s)$ 。

2. 从正态分布中做随机抽样: $a \sim \mathcal{N}\left(\widehat{\mu}, \widehat{\sigma}^{2}\right)$; 智能体执行动作 $a$ 。 然而我们并不知道 $\mu(s)$ 和 $\sigma(s)$ 是怎么样的函数。一个很自然的想法是用神经网络来近 似这两个函数。把神经网络记作 $\mu(s ; \boldsymbol{\theta})$ 和 $\sigma(s ; \boldsymbol{\theta})$, 其中 $\boldsymbol{\theta}$ 表示神经网络中的可训练参 数。但实践中最好不要直接近似标准差 $\sigma$, 而是近似方差对数 $\ln \sigma^{22}$ 。定义两个神经网络:

$$
\mu(s ; \boldsymbol{\theta}) \text { 和 } \rho(s ; \boldsymbol{\theta}),
$$

分别用于预测均值和方差对数。可以按照图10.10来搭建神经网络。神经网络的输入是状 态 $s$, 通常是向量、矩阵、或者张量。神经网络有两个输出头, 分别记作 $\mu(s ; \boldsymbol{\theta})$ 和 $\rho(s ; \boldsymbol{\theta})$ 。 可以这样用神经网络做控制:

1. 观测到当前状态 $s$, 计算均值 $\widehat{\mu}=\mu(s ; \boldsymbol{\theta})$, 方差对数 $\widehat{\rho}=\rho(s ; \boldsymbol{\theta})$, 以及方差 $\widehat{\sigma}^{2}=$ $\exp (\widehat{\rho})$ 。

2. 从正态分布中做随机抽样: $a \sim \mathcal{N}\left(\widehat{\mu}, \widehat{\sigma}^{2}\right)$; 智能体执行动作 $a$ 。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-170.jpg?height=391&width=1425&top_left_y=1906&top_left_x=378)

均值网络和方差对数网络 (参数: $\boldsymbol{\theta})$

图 10.10: 高斯策略网络有两个头, 一个输出均值 $\widehat{\mu}$, 另一个输出方差对数 $\widehat{\rho} 。$

用神经网络近似均值和标准差之后, 公式 (10.2) 中的策略函数 $\pi(a \mid s)$ 变成了下面的

2标准差 $\sigma$ 必须非负, 如果把 $\sigma$ 作为优化变量, 那么优化模型有约束条件, 给求解造成困难。方差对数 $\rho$ 的 取值范围是所有实数, 因此不需要约束条件。 策略网络：

$$
\pi(a \mid s ; \boldsymbol{\theta})=\frac{1}{\sqrt{6.28 \cdot \exp [\rho(s ; \boldsymbol{\theta})]}} \cdot \exp \left(-\frac{[a-\mu(s ; \boldsymbol{\theta})]^{2}}{2 \cdot \exp [\rho(s ; \boldsymbol{\theta})]}\right) .
$$

实际做控制的时候, 我们只需要神经网络 $\mu(s ; \boldsymbol{\theta})$ 和 $\rho(s ; \boldsymbol{\theta})$, 用不到真正的策略网络 $\pi(a \mid s ; \boldsymbol{\theta})$ 。

### 2 随机高斯策略网络

上一小节假设控制问题的自由度是 $d=1$, 也就是说动作 $a$ 是标量。实际问题中的 自由度 $d$ 往往大于 1 , 那么动作 $\boldsymbol{a}$ 是 $d$ 维向量。对于这样的问题, 我们修改一下神经网 络结构, 让两个输出 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 和 $\boldsymbol{\rho}(s ; \boldsymbol{\theta})$ 都 $d$ 维向量; 见图10.11。

用标量 $a_{i}$ 表示动作向量 $\boldsymbol{a}$ 的第 $i$ 个元素。用函数 $\mu_{i}(s ; \boldsymbol{\theta})$ 和 $\rho_{i}(s ; \boldsymbol{\theta})$ 分别表示 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 和 $\boldsymbol{\rho}(s ; \boldsymbol{\theta})$ 的第 $i$ 个元素。我们用下面这个特殊的多元正态分布的概率密度函数作为策略 网络：

$$
\pi(\boldsymbol{a} \mid s ; \boldsymbol{\theta})=\prod_{i=1}^{d} \frac{1}{\sqrt{6.28 \cdot \exp \left[\rho_{i}(s ; \boldsymbol{\theta})\right]}} \cdot \exp \left(-\frac{\left[a_{i}-\mu_{i}(s ; \boldsymbol{\theta})\right]^{2}}{2 \cdot \exp \left[\rho_{i}(s ; \boldsymbol{\theta})\right]}\right) .
$$

做控制的时候只需要均值网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 和方差对数网络 $\boldsymbol{\rho}(s ; \boldsymbol{\theta})$, 不需要策略网络 $\pi(\boldsymbol{a} \mid s ; \boldsymbol{\theta})$ 。 做训练的时候也不需要 $\pi(\boldsymbol{a} \mid s ; \boldsymbol{\theta})$, 而是要用辅助网络 $f(s, \boldsymbol{a} ; \boldsymbol{\theta})$ 。总而言之, 策略网络 $\pi$ 只是帮助你理解本节的方法而已, 实际算法中不会出现 $\pi$ 。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-171.jpg?height=602&width=1442&top_left_y=1561&top_left_x=250)

辅助网 络

图 10.11: 辅助网络的结构示意图。辅助神经网络的输入是状态 $s$ 与动作 $\boldsymbol{a}$, 输出是实数 $f(s, \boldsymbol{a} ; \boldsymbol{\theta})$.

图10.11描述了辅助网络 $f(s, \boldsymbol{a} ; \boldsymbol{\theta})$ 与 $\boldsymbol{\mu} 、 \boldsymbol{\rho} 、 \boldsymbol{a}$ 的关系。辅助网络具体是这样定义的：

$$
f(s, \boldsymbol{a} ; \boldsymbol{\theta})=-\frac{1}{2} \sum_{i=1}^{d}\left(\rho_{i}(s ; \boldsymbol{\theta})+\frac{\left[a_{i}-\mu_{i}(s ; \boldsymbol{\theta})\right]^{2}}{\exp \left[\rho_{i}(s ; \boldsymbol{\theta})\right]}\right) .
$$

它的可训练参数 $\boldsymbol{\theta}$ 都是从 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 和 $\boldsymbol{\rho}(s ; \boldsymbol{\theta})$ 中来的。不难发现, 辅助网络与策略网络有 这样的关系:

$$
f(s, \boldsymbol{a} ; \boldsymbol{\theta})=\ln \pi(\boldsymbol{a} \mid s ; \boldsymbol{\theta})+\text { Constant } .
$$

### 3 策略梯度

回忆一下之前学过的内容。在 $t$ 时刻的折扣回报记作随机变量

$$
U_{t}=R_{t}+\gamma \cdot R_{t+1}+\gamma^{2} \cdot R_{t+2}+\cdots+\gamma^{n-t} \cdot R_{n} .
$$

动作价值函数 $Q_{\pi}\left(s_{t}, \boldsymbol{a}_{t}\right)$ 是对折扣回报 $U_{t}$ 的条件期望。前面章节推导过策略梯度的蒙特 卡洛近似：

$$
\boldsymbol{g}=Q_{\pi}(s, \boldsymbol{a}) \cdot \nabla_{\boldsymbol{\theta}} \ln \pi(\boldsymbol{a} \mid s ; \boldsymbol{\theta}) .
$$

由公式 (10.3) 可得:

$$
\boldsymbol{g}=Q_{\pi}(s, \boldsymbol{a}) \cdot \nabla_{\boldsymbol{\theta}} f(s, \boldsymbol{a} ; \boldsymbol{\theta}) .
$$

有了策略梯度, 就可以学习参数 $\boldsymbol{\theta}$ 。训练的过程大致如下:

1. 搭建均值网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 、方差对数网络 $\boldsymbol{\rho}(s ; \boldsymbol{\theta})$ 、辅助网络 $f(s, \boldsymbol{a} ; \boldsymbol{\theta})$ 。

2. 让智能体与环境交互, 记录每一步的状态、动作、奖励, 并对参数 $\boldsymbol{\theta}$ 做更新: (a). 观测到当前状态 $s$, 计算均值、方差对数、方差：

$$
\widehat{\boldsymbol{\mu}}=\boldsymbol{\mu}(s ; \boldsymbol{\theta}), \quad \widehat{\boldsymbol{\rho}}=\boldsymbol{\rho}(s ; \boldsymbol{\theta}), \quad \widehat{\boldsymbol{\sigma}}^{2}=\exp (\widehat{\boldsymbol{\rho}}) .
$$

此处的指数函数 $\exp (\cdot)$ 应用到向量的每一个元素上。

(b). 设 $\widehat{\mu}_{i}$ 和 $\widehat{\sigma}_{i}$ 分别是 $d$ 维向量 $\widehat{\boldsymbol{\mu}}$ 和 $\widehat{\boldsymbol{\sigma}}$ 的第 $i$ 个元素。从正态分布中做抽样:

$$
a_{i} \sim \mathcal{N}\left(\widehat{\mu}_{i}, \widehat{\sigma}_{i}^{2}\right), \quad \forall i=1, \cdots, d .
$$

把得到的动作记作 $\boldsymbol{a}=\left[a_{1}, \cdots, a_{d}\right]$ 。

(c). 近似计算动作价值： $\widehat{q} \approx Q_{\pi}(s, \boldsymbol{a})$ 。

(d). 用反向传播计算出辅助网络关于参数 $\boldsymbol{\theta}$ 的梯度： $\nabla_{\boldsymbol{\theta}} f(s, \boldsymbol{a} ; \boldsymbol{\theta})$ 。

(e). 用策略梯度上升更新参数：

$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\beta \cdot \widehat{q} \cdot \nabla_{\boldsymbol{\theta}} f(s, \boldsymbol{a} ; \boldsymbol{\theta}) .
$$

此处的 $\beta$ 是学习率。

但是算法中有一个没解决的问题：我们并不知道动作价值 $Q_{\pi}(s, \boldsymbol{a})$ 。有两种办法近似 $Q_{\pi}(s, \boldsymbol{a})$ : REINFORCE 用实际观测的折扣回报代替 $Q_{\pi}(s, \boldsymbol{a})$, actor-critic 用价值网络近 似 $Q_{\pi}$ 。后面两小节具体讲解这两种算法。

### 4 用 REINFORCE 学习参数

REINFORCE 用实际观测的折扣回报 $u_{t}=\sum_{k=t}^{n} \gamma^{k-t} \cdot r_{k}$ 代替动作价值 $Q_{\pi}\left(s_{t}, \boldsymbol{a}_{t}\right)$ 。 道理是这样的。动作价值是回报的期望:

$$
Q_{\pi}\left(s_{t}, \boldsymbol{a}_{t}\right)=\mathbb{E}\left[U_{t} \mid S_{t}=s_{t}, A_{t}=\boldsymbol{a}_{t}\right] .
$$

随机变量 $U_{t}$ 的一个实际观测值 $u_{t}$ 是期望的蒙特卡洛近似。这样一来, 公式 (10.4) 中的 策略梯度就能近似成

$$
\boldsymbol{g} \approx u_{t} \cdot \nabla_{\boldsymbol{\theta}} f(s, \boldsymbol{a} ; \boldsymbol{\theta})
$$

在搭建好均值网络 $\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 、方差对数网络 $\boldsymbol{\rho}(s ; \boldsymbol{\theta})$ 、辅助网络 $f(s, \boldsymbol{a} ; \boldsymbol{\theta})$ 之后, 我们用 REINFORCE 更新参数 $\boldsymbol{\theta}$ 。设当前参数为 $\boldsymbol{\theta}_{\text {now }}$ 。REINFORCE 重复以下步骤, 直到收玫：

1. 用 $\boldsymbol{\mu}\left(s ; \boldsymbol{\theta}_{\text {now }}\right)$ 和 $\boldsymbol{\rho}\left(s ; \boldsymbol{\theta}_{\text {now }}\right)$ 控制智能体与环境交互, 完成一局游戏, 得到一条轨迹:

$$
s_{1}, \boldsymbol{a}_{1}, r_{1}, \quad s_{2}, \boldsymbol{a}_{2}, r_{2}, \quad \cdots, \quad s_{n}, \boldsymbol{a}_{n}, r_{n} .
$$

2. 计算所有的回报：

$$
u_{t}=\sum_{k=t}^{T} \gamma^{k-t} \cdot r_{k}, \quad \forall t=1, \cdots, n .
$$

3. 对辅助网络做反向传播, 得到所有的梯度:

$$
\nabla_{\boldsymbol{\theta}} f\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{\theta}_{\text {now }}\right), \quad \forall t=1, \cdots, n .
$$

4. 用策略梯度上升更新参数：

$$
\boldsymbol{\theta}_{\text {new }} \leftarrow \boldsymbol{\theta}_{\text {now }}+\beta \cdot \sum_{t=1}^{n} \gamma^{t-1} \cdot u_{t} \cdot \nabla_{\boldsymbol{\theta}} f\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{\theta}_{\text {now }}\right)
$$

上述算法标准的 REINFORCE，效果不如使用基线的 REINFORCE。读者可以参考第 $8.2$ 节的内容, 把状态价值作为基线, 改进上面描述的算法。REINFORCE 算法属于同策略 (on-policy), 不能使用经验回放。

### 5 用 Actor-Critic 学习参数

Actor-critic 需要搭建一个价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$, 用于近似动作价值函数 $Q_{\pi}(s, \boldsymbol{a})$ 。价 值网络的结构如图 $10.12$ 所示。此外, 还需要一个目标价值网络 $q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{-}\right)$, 网络结构相 同, 但是参数不同。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-173.jpg?height=420&width=1408&top_left_y=1789&top_left_x=267)

价值网络

图 10.12: 价值网络 $q(s, \boldsymbol{a} ; \boldsymbol{w})$ 的结构。输入是状态 $s$ 和动作 $\boldsymbol{a}$, 输出是实数。

在搭建好均值网络 $\boldsymbol{\mu}$ 、方差对数网络 $\boldsymbol{\rho}$ 、辅助网络 $f$ 、价值网络 $q$ 之后, 我们用 SARSA 算法更新价值网络参数 $\boldsymbol{w}$, 用近似策略梯度更新控制器参数 $\boldsymbol{\theta}$ 。设当前参数为 $\boldsymbol{w}_{\text {now }}$ 和 $\theta_{\text {now }}$ 重复以下步骤更新价值网络参数、控制器参数, 直到收玫：

1. 实际观测到当前状态 $s_{t}$, 用控制器算出均值 $\boldsymbol{\mu}\left(s_{t} ; \boldsymbol{\theta}_{\text {now }}\right)$ 和方差对数 $\boldsymbol{\rho}\left(s_{t} ; \boldsymbol{\theta}_{\text {now }}\right)$, 然 后随机抽样得到动作 $\boldsymbol{a}_{t}$ 。 智能体执行动作 $\boldsymbol{a}_{t}$, 观测到奖励 $r_{t}$ 与新的状态 $s_{t+1}$ 。

2. 计算均值 $\boldsymbol{\mu}\left(s_{t+1} ; \boldsymbol{\theta}_{\text {now }}\right)$ 和方差对数 $\boldsymbol{\rho}\left(s_{t+1} ; \boldsymbol{\theta}_{\text {now }}\right)$, 然后随机抽样得到动作 $\tilde{\boldsymbol{a}}_{t+1}$ 。这 个动作只是假想动作, 智能体不予执行。

3. 用价值网络计算出:

$$
\widehat{q}_{t}=q\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{w}_{\text {now }}\right) .
$$

4. 用目标网络计算出:

$$
\widehat{q}_{t+1}=q\left(s_{t+1}, \tilde{\boldsymbol{a}}_{t+1} ; \boldsymbol{w}_{\text {now }}^{-}\right) .
$$

5. 计算 TD 目标和 TD 误差:

$$
\widehat{y}_{t}=r_{t}+\gamma \cdot \widehat{q}_{t+1}, \quad \delta_{t}=\widehat{q}_{t}-\widehat{y}_{t} .
$$

6. 更新价值网络的参数:

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\alpha \cdot \delta_{t} \cdot \nabla_{\boldsymbol{w}} q\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{w}_{\text {now }}\right) .
$$

7. 更新策略网络参数参数：

$$
\boldsymbol{\theta}_{\text {new }} \leftarrow \boldsymbol{\theta}_{\text {now }}+\beta \cdot \widehat{q}_{t} \cdot \nabla_{\boldsymbol{\theta}} f\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{\theta}_{\text {now }}\right)
$$

8. 更新目标网络参数：

$$
\boldsymbol{w}_{\text {new }}^{-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{-} .
$$

算法中的 $\alpha 、 \beta 、 \tau$ 都是超参数, 需要手动调整。上述算法是标准的 actor-critic, 效果不如 advantage actor-critic (A2C)。读者可以参考第 $8.3$ 节的内容, 用 $\mathrm{A} 2 \mathrm{C}$ 改进上面描述的算 法。

## 第 10 章 知识点

- 离散控制问题的动作空间 $\mathcal{A}$ 是个有限的离散集, 连续控制问题的动作空间 $\mathcal{A}$ 是个 连续集。如果想将 DQN 等离散控制方法应用到连续控制问题, 可以对连续动作空 间做离散化，但这只适用于自由度较小的问题。

- 可以用确定策略网络 $\boldsymbol{a}=\boldsymbol{\mu}(s ; \boldsymbol{\theta})$ 做连续控制。网络的输入是状态 $s$, 输出是动作 $\boldsymbol{a}, \boldsymbol{a}$ 是向量, 大小等于问题的自由度。

- 确定策略梯度 (DPG) 借助价值网络 $q(s, a ; \boldsymbol{w})$ 训练确定策略网络。DPG 属于异策 略, 用行为策略收集经验, 做经验回放更新策略网络和价值网络。

- DPG 与 DQN 有很多相似之处, 而且它们的训练都存在高估等问题。TD3 使用几种 技巧改进 DPG：截断双 $\mathrm{Q}$ 学习、往动作中加噪声、降低更新策略网络和目标网络 的频率。

- 可以用随机高斯策略做连续控制。用两个神经网络分别近似高斯分布的均值和方差 对数, 并用策略梯度更新两个神经网络的参数。

## 第 $\mathbf{1 0}$ 章 相关文献 $\boldsymbol{8}$

确定策略梯度 (deterministic policy gradient, DPG) 方法由 David Silver 等人在 2014 年 提出 ${ }^{[99]}$ 。随后同一批作者把相似的想法与深度学习结合起来, 提出深度确定策略梯度 (deep deterministic policy gradient, 缩写 DDPG), 文章在 2016 年发表 ${ }^{[67]}$ 。这两篇论文 使得 DPG 方法流行起来。但值得注意的是, 相似的想法在更早的论文中有提出: ${ }^{[45,85]}$ 。

2018 年的论文 ${ }^{[41]}$ 提出三种对 DPG 的改进方法, 并将改进的算法命名为 TD3。2017 年的论文 ${ }^{[44]}$ 提出了 soft actor-critic (SAC), 也可以解决连续控制问题。

Degris 等人在 2012 年发表的论文 ${ }^{[34]}$ 使用正态分布的概率密度函数作为策略函数, 并且用线性函数近似均值和方差对数。类似的连续控制方法最早由 Williams 在 1987 和 1992 年提出 125-126]。
