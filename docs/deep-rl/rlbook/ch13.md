
## 第 13 章 并行计算

机器学习的实践中普遍使用并行计算, 利用大量的计算资源（比如很多块 GPU）缩 短训练所需的时间, 用几个小时就能完成原本需要很多天才能完成的训练。深度强化学 习自然也不例外。可以用很多处理器同时收集经验、计算梯度, 让原本需要很长时间的 训练在较短的时间内完成。第 $13.1$ 以并行梯度下降为例讲解并行计算基础知识。第 $13.2$ 介绍异步并行梯度下降算法。第 $13.3$ 介绍两种异步强化学习算法。

## 1 并行计算基础

本节以并行梯度下降 (parallel gradient descent) 为例讲解并行计算的基础知识, 用 MapReduce 架构实现并行梯度下降, 并且分析并行计算中的时间开销。

### 1 并行梯度下降

本节用最小二乘回归 (least squares regression) 为例讲解并行梯度下降的基本原理。 把训练数据记作 $\left(\boldsymbol{x}_{1}, y_{1}\right), \cdots,\left(\boldsymbol{x}_{n}, y_{n}\right) \in \mathbb{R}^{d} \times \mathbb{R}$ 。最小二乘回归定义为:

$$
\min _{\boldsymbol{w}}\left\{L(\boldsymbol{w}) \triangleq \frac{1}{2 n} \sum_{j=1}^{n}\left(\boldsymbol{x}_{j}^{T} \boldsymbol{w}-y_{j}\right)^{2}\right\} .
$$

这个优化问题的目标是寻找向量 $\boldsymbol{w}^{\star} \in \mathbb{R}^{d}$, 使得对于所有的 $j$, 预测值 $\boldsymbol{x}_{j}^{T} \boldsymbol{w}^{\star}$ 都很接近 $y_{j}$ 。我们可以用梯度下降算法求解这个优化问题。梯度下降重复这个步骤, 直到收玫：

$$
\boldsymbol{w} \leftarrow \boldsymbol{w}-\eta \cdot \nabla_{\boldsymbol{w}} L(\boldsymbol{w}) .
$$

公式中的 $\eta$ 是学习率。如果 $\eta$ 的取值比较合理, 那么梯度下降可以保证 $\boldsymbol{w}$ 收玫到最优解 $\boldsymbol{w}^{\star}$ 。目标函数 $L(\boldsymbol{w})$ 的梯度可以写作:

$$
\nabla_{\boldsymbol{w}} L(\boldsymbol{w})=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{g}\left(\boldsymbol{x}_{j}, y_{j} ; \boldsymbol{w}\right), \quad \text { 其中 } \quad \boldsymbol{g}\left(\boldsymbol{x}_{j}, y_{j} ; \boldsymbol{w}\right) \triangleq\left(\boldsymbol{x}_{j}^{T} \boldsymbol{w}-y_{j}\right) \boldsymbol{x}_{j} \in \mathbb{R}^{d} \text {. }
$$

由于 $\boldsymbol{x}_{j}$ 和 $\boldsymbol{w}$ 都是 $d$ 维向量, 因此计算一个 $\boldsymbol{g}\left(\boldsymbol{x}_{j}, y_{j} ; \boldsymbol{w}\right)$ 的时间复杂度是 $\mathcal{O}(d)$ 。计算梯 度 $\nabla_{\boldsymbol{w}} L(\boldsymbol{w})$ 需要计算 $\boldsymbol{g}$ 函数 $n$ 次, 所以计算 $\nabla_{\boldsymbol{w}} L(\boldsymbol{w})$ 的时间复杂度是 $\mathcal{O}(n d)$ 。如果用 $m$ 块处理器做并行计算, 那么理想情况下每块处理器的计算量是 $\mathcal{O}\left(\frac{n d}{m}\right)$ 。

下面举一个简单的例子讲解并行梯度下降。假设我们有两块处理器。把梯度 $\nabla_{\boldsymbol{w}} L(\boldsymbol{w})$ 展开，得到：

$$
\begin{aligned}
& \nabla_{\boldsymbol{w}} L(\boldsymbol{w})
\end{aligned}
$$

两块处理器各承担一半的计算量, 分别输出 $d$ 维向量 $\tilde{\boldsymbol{g}}^{1}$ 和 $\tilde{\boldsymbol{g}}^{2}$ 。将两块处理器的结果汇 总, 得到梯度：

$$
\nabla_{\boldsymbol{w}} L(\boldsymbol{w})=\frac{1}{n}\left(\tilde{\boldsymbol{g}}^{1}+\tilde{\boldsymbol{g}}^{2}\right)
$$

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-203.jpg?height=157&width=1397&top_left_y=2340&top_left_x=275)

并行梯度下降中的 “计算” 非常简单; 并行计算的复杂之处在于通信。在一轮梯度下降开 始之前, 需要把最新的模型参数 $\boldsymbol{w}$ 发送给两块处理器, 否则处理器无法计算梯度。在两 块处理器完成计算之后, 需要做通信, 把结果 $\tilde{\boldsymbol{g}}^{1}$ 和 $\tilde{\boldsymbol{g}}^{2}$ 汇总到一块处理器上。下一小节 以 MapReduce 架构为例, 讲解并行梯度下降的实现。

### MapReduce}

并行计算需要在计算机集群上完成。一个集群有很多处理器和内存条, 它们被划分 到多个节点 (compute node) 上。一个节点上可以有多个处理器, 处理器可以共享内存。 节点之间不能共享内存, 即一个节点不能访问另一个节点的内存。如果两个节点相连接, 它们可以通过计算机网络通信（比如 TCP/IP 协议)。

为了协调节点的计算和通信, 需要有相应的软件系统。MapReduce 是由 Google 开发 的一种软件系统, 用于大规模的数据分析和机器学习。MapReduce 原本是软件系统的名 字, 但是后来人们把类似的系统架构都称作 MapReduce。除了 Google 自己的 MapReduce, 比较有名的系统还有 Hadoop ${ }^{1}$ 和 Spark ${ }^{2}$ 。MapReduce 属于 client-server 架构, 有一个节 点作为中央服务器, 其余节点作为 worker, 受服务器控制。服务器用于协调整个系统, 而 计算主要由 worker 节点并行完成。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-204.jpg?height=517&width=700&top_left_y=1352&top_left_x=378)

图 13.1: MapReduce 中的广播 (broadcast) 操 作。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-204.jpg?height=517&width=671&top_left_y=1352&top_left_x=1115)

图 13.2: MapReduce 中的映射 (map) 操作。

服务器可以与 worker 节点做通信传输数据 (但是 worker 节点之间不能相互通信)。一 种通信方式是广播 (broadcast), 即服务器将同一条信息同时发送给所有 worker 节点; 如 图 $13.1$ 所示。比如做并行梯度下降的时候, 服务器需要把更新过的参数 $\boldsymbol{w} \in \mathbb{R}^{d}$ 广播到 所有 worker 节点。MapReduce 架构不允许服务器将一条信息只发送给一号节点, 而将另 一条不同的信息只发送给二号节点。服务器必须把相同信息广播到所有节点。

每个节点都可以做计算。映射 (map) 操作让所有 worker 节点同时并行做计算; 如图 $13.2$ 所示。如果我们要编程实现一个算法, 需要自己定义一个函数, 它可以让每个 worker 节点把它的本地数据映射到一些输出值。比如做并行梯度下降的时候, 定义函数 $\boldsymbol{g}$ 把三

${ }^{1}$ https://hadoop.apache.org/

${ }^{2} \mathrm{https} / / /$ spark.apache.org/ 元组 $\left(\boldsymbol{x}_{j}, y_{j}, \boldsymbol{w}\right)$ 映射到向量

$$
\boldsymbol{z}_{j}=\left(\boldsymbol{x}_{j}^{T} \boldsymbol{w}-y_{j}\right) \boldsymbol{x}_{j}
$$

映射操作要求所有节点都要同时执行同一个函数, 比如 $\boldsymbol{g}\left(\boldsymbol{x}_{j}, y_{j}, \boldsymbol{w}\right)$ 。节点不能各自执行 不同的函数。

Worker 节点可以向服务器发送信 息, 最常用的通信操作是规约 (reduce) 。这种操作可以把 worker 节点上的数 据做归并, 并且传输到服务器上。如 图 $13.3$ 所示, 系统对 worker 节点输出 的蓝色向量做规约。如果执行 sum 规 约函数, 那么结果是四个蓝色向量的 加和。如果执行 mean 规约函数, 那么 结果是四个蓝色向量的均值。如果执 行 count 规约函数, 那么结果是整数 4 , 即蓝色向量的数量。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-205.jpg?height=528&width=711&top_left_y=570&top_left_x=935)

图 13.3: MapReduce 中的规约 (Reduce) 操作。

### 3 用 MapReduce 实现并行梯度下降

数据并发 (data parallelism)：为了使用 MapReduce 实现并行梯度下降, 我们需要把 数据集 $\left(\boldsymbol{x}_{1}, y_{1}\right), \cdots,\left(\boldsymbol{x}_{n}, y_{n}\right)$ 划分到 $m$ 个 worker 节点上, 每个节点上存一部分数据; 见 图 13.4。这种划分方式叫做数据并发。与数据并发相对的是模型并发 (model parallelism), 即将模型参数 $\boldsymbol{w}$ 划分到 $m$ 个 worker 节点上; 每个节点有全部数据, 但是只有一部分模 型参数。本书只介绍数据并发, 不讨论模型并发。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-205.jpg?height=411&width=1148&top_left_y=1728&top_left_x=388)

图 13.4: 将数据集 $\left(\boldsymbol{x}_{1}, y_{1}\right), \cdots,\left(\boldsymbol{x}_{n}, y_{n}\right)$ 划分到 $m$ 个 worker 节点上。

并行梯度下降的流程 : 用数据并发, 设集合 $\mathcal{I}_{1}, \cdots, \mathcal{I}_{m}$ 是集合 $\{1,2, \cdots, n\}$ 的划分; 集合 $\mathcal{I}_{k}$ 包含第 $k$ 个 worker 节点上所有样本的序号。并行梯度下降需要重复一一广播、映 射、规约、更新参数一一这四个步骤, 直到算法收玫；见示意图 13.5。

1. 广播 (broadcast)：服务器将当前的模型参数 $\boldsymbol{w}_{\text {now }}$ 广播到 $m$ 个 worker 节点。这样 一来, 所有节点都知道 $\boldsymbol{w}_{\text {now }}$ 。

2. 映射 (map)：这一步让 $m$ 个 worker 节点做并行计算, 用本地数据计算梯度。需要

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-206.jpg?height=1008&width=1465&top_left_y=233&top_left_x=361)

图 13.5: 并行梯度下降的流程。

在编程的时候定义这样一个映射函数：

$$
\boldsymbol{g}(\boldsymbol{x}, y, \boldsymbol{w})=\left(\boldsymbol{x}^{T} \boldsymbol{w}-y\right) \boldsymbol{x} .
$$

第 $k$ 号 worker 节点做如下映射:

$$
\boldsymbol{g}:\left(\boldsymbol{x}_{j}, y_{j}, \boldsymbol{w}_{\text {now }}\right) \mapsto \quad \boldsymbol{z}_{j}=\left(\boldsymbol{x}_{j}^{T} \boldsymbol{w}_{\text {now }}-y_{j}\right) \boldsymbol{x}_{j}, \quad \forall j \in \mathcal{I}_{k} .
$$

这样一来, 第 $k$ 号 worker 节点得到向量的集合 $\left\{\boldsymbol{z}_{j}\right\}_{j \in \mathcal{I}_{k}}$ 。

3. 规约 (reduce)：在做完映射之后, 向量 $\boldsymbol{z}_{1}, \cdots, \boldsymbol{z}_{n} \in \mathbb{R}^{d}$ 分布式存储在 $m$ 个 worker 节点上, 每个节点有一个子集。不难看出, 目标函数 $L(\boldsymbol{w})=\frac{1}{2 n} \sum_{j=1}^{n}\left(\boldsymbol{x}_{j}^{T} \boldsymbol{w}-y_{j}\right)^{2}$ 在 $\boldsymbol{w}_{\text {now }}$ 处的梯度等于:

$$
\nabla_{\boldsymbol{w}} L\left(\boldsymbol{w}_{\text {now }}\right)=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{z}_{j} .
$$

因此, 我们应该使用 sum 规约函数。每个Worker 节点首先会规约自己本地的 $\left\{\boldsymbol{z}_{j}\right\}_{j \in \mathcal{I}_{k}}$, 得到

$$
\tilde{\boldsymbol{g}}^{k} \triangleq \sum_{j \in \mathcal{I}_{k}} \boldsymbol{z}_{j}, \quad \forall k=1, \cdots, m .
$$

然后将 $\tilde{\boldsymbol{g}}_{k} \in \mathbb{R}^{d}$ 发送给服务器, 服务器对 $\tilde{\boldsymbol{g}}^{1}, \cdots, \tilde{\boldsymbol{g}}^{k}$ 求和, 再除以 $n$, 得到梯度:

$$
\nabla_{\boldsymbol{w}} L\left(\boldsymbol{w}_{\text {now }}\right) \leftarrow \frac{1}{n} \sum_{k=1}^{m} \tilde{\boldsymbol{g}}^{k} .
$$

先在本地做规约, 再做通信, 只需要传输 $m d$ 个浮点数; 如果不先在本地归约, 直 接把所有的 $\left\{\boldsymbol{z}_{j}\right\}_{j=1}^{n}$ 都发送给服务器, 那么需要传输 $n d$ 个浮点数, 通信代价大得 多。

4. 更新参数： 最后, 服务器在本地做梯度下降, 更新模型参数：

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\eta \cdot \nabla_{\boldsymbol{w}} L\left(\boldsymbol{w}_{\text {now }}\right) .
$$

这样就完成了一轮梯度下降, 对参数做了一次更新。

### 4 并行计算的代价

在做实验对比的时候, 可以用算法实际运行的时间来衡量并行算法的表现。时间有 两种定义, 请读者注意区分。

1. 钟表时间（wall-clock time）,也叫 elapsed real time, 意思是程序实际运行的时间。 可以这样理解钟表时间: 在程序开始运行的时候, 记录下墙上钟表的时刻; 在程序 结束的时候, 再记录钟表的时刻; 两者之差就是钟表时间。

2. 处理器时间 (CPU time 或 GPU time) 是所有处理器运行时间的总和。比如使用 4 块 $\mathrm{CPU}$ 做并行计算, 程序运行的钟表时间是 1 分钟, 期间 CPU 没有空闲, 那么系统 的 CPU 时间等于 4 分钟。

处理器数量越多, 每块处理器承担的计算量就越小, 那么程序运行速度就会越快。所以 并行计算可以让钟表时间更短。用多个处理器做并行计算, 然而总计算量没有减少, 因 此并行计算不会让处理器时间更短。

通常用加速比 (speedup ratio) 衡量并行计算带来的速度提升。加速比是这样计算的：

$$
\text { 加速比 }=\frac{\text { 使用一个节点所需的钟表时间 }}{\text { 使用 } m \text { 个节点所需的钟表时间 }} \text {. }
$$

通常来说, 节点数量越多, 算力越强, 加速比就越大。在实验报告中, 通常需要把加速 比绘制成一条曲线。把节点数量设置为不同的值, 比如 $m=1,2,4,8,16,32$, 得到相应 的加速比。把 $m$ 作为横轴, 把加速比作为纵轴, 绘制出加速比曲线; 见图 13.6。

在最理想的情况下, 使用 $m$ 个节 点, 每个节点承担 $\frac{1}{m}$ 的计算量, 那么 钟表时间会减小到原来的 $\frac{1}{m}$, 即加速 比等于 $m$ 。图 $13.6$ 中的蓝色直线是 理想情况下的加速比。但实际的加速 比往往是图中的红色曲线, 即加速比 小于 $m$ 。其原因在于计算所需时间只 占总的钟表时间的一部分。通信等操 作也要花费时间, 导致加速比达不到 $m$ 。下面分析并行计算中常见的时间

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-207.jpg?height=480&width=711&top_left_y=1779&top_left_x=935)

图 13.6: 加速比曲线。 开销。

通信量 (communication complexity) 的意思是有多少个比特或者浮点数在服务器与 worker 节点之间传输。在并行梯度下降的例子中, 每一轮梯度下降需要做两次通信: 服务 器将模型参数 $\boldsymbol{w} \in \mathbb{R}^{d}$ 广播给 $m$ 个 worker 节点, worker 节点将计算出的梯度 $\tilde{\boldsymbol{g}}^{1}, \cdots, \tilde{\boldsymbol{g}}^{m}$ 发送给服务器。因此每一轮梯度下降的通信量都是 $\mathcal{O}(m d)$ 。很显然, 通信量越大, 通信 花的时间越长。

延迟 (latency) 是由计算机网络的硬件和软件系统决定的。做通信的时候, 需要把 大的矩阵、向量拆分成小数据包, 通过计算机网络逐个传输数据包。即使数据包再小, 从 发送到接收之间也需要花费一定时间, 这个时间就是延迟。通常来说, 延迟与通信次数 成正比, 而跟通信量关系不大。

通信时间 主要由通信量和延迟造成。除非实际做实验测量, 我们无法准确预估通信 花费的钟表时间。但我们不妨用下面的公式粗略估计通信时间：

$$
\text { 通信时间 } \approx \frac{\text { 通信量 }}{\text { 带宽 }}+\text { 延迟. }
$$

在并行计算中, 通信时间是不容忽视的, 通信时间甚至有可能超过计算时间。降低通信 量和通信次数是设计并行算法的关键。只有当通信时间远低于计算时间, 才能取得较高 的加速比。

## $13.2$ 同步与异步

本节讨论同步算法、异步算法的区别, 重点介绍异步并行梯度下降。用在机器学习 中, 异步算法的表现通常优于同步算法。

### 1 同步算法

上一节介绍的并行梯度下降算法属于同步算法（synchronous algorithm）。如图 $13.7$ 所示, 在所有 worker 节点都完成映射 (map) 的计算之后, 系统才能执行规约 (reduce) 通 信。这意味着即使有些节点先完成计算, 也必须等待最慢节点; 在等待期间, 节点处于 空闲状态。图 $13.7$ 中黑色的坚线表示同步屏障, 即所有节点都完成计算之后才能开始通 信, 当通信完成之后才能开始下一轮计算。
![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-209.jpg?height=516&width=1148&top_left_y=1040&top_left_x=404)

图 13.7: 同步梯度下降中的计算、通信、同步。图中横向表示时间。

同步的代价：实际软硬件系统中存在负载不平衡、软硬件不稳定、I/O 速度不稳定 等因素。因此 worker 节点会有先后、快慢之分, 不会恰好在同一时刻完成任务。同步要 求每一轮都必须等待所有节点完成计算, 这势必导致 “短板效应”, 即任务所需时间取决于 最慢的节点。同步会造成很多节点处于空闲状态, 无法有效利用集群的算力。

Straggler effect 意思是一个节点的速度远慢于其余节点, 导致整个系统长时间处于 空闲状态, 等待最慢的节点。straggler 也叫 outlier, 字面意思是“掉队者”。产生 straggler 的原因有很多种, 比如在某个节点的硬件或软件出错之后, 节点死掉或者重启, 导致计 算时间多几倍。如果把 MapReduce 这样的需要同步的系统部署到廉价、可靠性低的硬件 上, straggler effect 可能会很严重。

### 2 异步算法

如果把图 $13.7$ 中的同步屏障去掉, 得到的算法就叫做异步算法 (asynchronous algorithm， 如图 $13.8$ 所示。在异步算法中, 一个 worker 节点无需等待其余节点完成计算或 通信。当一个 worker 节点完成计算, 它立刻跟 server 通信, 然后开始下一轮的计算。异 步算法避免了等待, 节点空闲的时间很短, 因此系统的利用率很高。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-210.jpg?height=509&width=1084&top_left_y=288&top_left_x=523)

图 13.8: 异步算法中的计算、通信、同步。图中横向表示时间。

下面介绍异步梯度下降算法。我们仍然采用数据并发的方式, 即把数据集 $\left\{\left(\boldsymbol{x}_{1}, y_{1}\right)\right.$, $\left.\cdots,\left(\boldsymbol{x}_{n}, y_{n}\right)\right\}$ 划分到 $m$ 个 worker 节点上。如图 $13.9$ 所示, 服务器可以单独与某个 worker 节点通信：worker 节点把计算出梯度发送给服务器, 服务器把最新的参数发送给这个 worker 节点。如果想要编程实现异步算法, 可以用 message passing interface（MPI）这样 底层的库, 也可以借助 Ray $^{3}$ 这样的框架。用户需要做的工作是编程实现 worker 端、服 务器端的计算。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-210.jpg?height=643&width=1425&top_left_y=1386&top_left_x=384)

图 13.9: 异步梯度下降。

Worker 端的计算：每个 worker 节点独立做计算, 独立与服务器通信; worker 节点 之间不通信，不等待。第 $k$ 号 worker 节点重复下面的步骤:

1. 向服务器发出请求, 索要最新的模型参数。把接收到的参数记作 $\boldsymbol{w}_{\text {now }}$

2. 利用本地的数据 $\left\{\left(\boldsymbol{x}_{j}, y_{j}\right)\right\}_{j \in \mathcal{I}_{k}}$ 和参数 $\boldsymbol{w}_{\text {now }}$ 计算本地的梯度:

$$
\tilde{\boldsymbol{g}}^{k}=\frac{1}{\left|\mathcal{I}_{k}\right|} \sum_{j \in \mathcal{I}_{k}}\left(\boldsymbol{x}_{j}^{T} \boldsymbol{w}_{\text {now }}-y_{j}\right) \boldsymbol{x}_{j} .
$$

${ }^{3}$ https://ray.io/ 3. 把计算出的梯度 $\tilde{\boldsymbol{g}}^{k}$ 发送给服务器。

服务器端的计算: 服务器上储存一份模型参数, 并且用 worker 发来的梯度更新参 数。每当收到一个 worker（比如第 $k$ 号 worker）发送来的梯度（记作 $\tilde{\boldsymbol{g}}^{k}$ ), 服务器就立 刻做梯度下降更新参数：

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\eta \cdot \tilde{\boldsymbol{g}}^{k} .
$$

服务器还需要监听 worker 发送的请求。如果有 worker 索要参数, 就把当前的参数 $\boldsymbol{w}_{\text {new }}$ 发送给这个 worker。

### 3 同步与异步梯度下降的对比

上一节介绍的同步并行梯度下降完全等价于标准的梯度下降, 只是把计算分配到了 多个 worker 节点上而已。然而异步梯度下降算法与标准的梯度下降是不等价的。同步与 异步梯度下降不只是编程实现有区别, 更是在算法上有本质区别。

1. 不难证明, 同步并行梯度下降更新参数的方式为:

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\eta \cdot \nabla_{\boldsymbol{w}} L\left(\boldsymbol{w}_{\text {now }}\right),
$$

即标准的梯度下降。在同一时刻, 所有 worker 节点上的参数是相同的, 都是 $\boldsymbol{w}_{\text {now }}$ 。 所有 worker 节点都基于相同的 $\boldsymbol{w}_{\text {now }}$ 计算梯度。

2. 对于异步并行梯度下降, 在同一时刻, 不同 worker 节点上的参数 $\boldsymbol{w}$ 通常是不同的。 比如两个 worker 分别在 $t_{1}$ 和 $t_{2}$ 时刻向服务器索要参数。在两个时刻之间, 服务器 可能已经对参数做了多次更新, 导致在 $t_{1}$ 和 $t_{2}$ 时刻取回的参数不同。两个 worker 节点会基于不同的参数计算梯度。

在理论上, 异步梯度下降的收玫速度慢于同步算法, 即需要更多的计算量才能达到 相同的精度。但是实践中异步梯度下降远比同步算法快 (用钟表时间衡量), 这是因为异 步算法无需等待, worker 节点几乎不会空闲, 利用率很高。

## $13.3$ 并行强化学习

并行强化学习的目的在于用更少的钟表时间完成训练。第 13.3.1、13.3.2 小节分别用 异步并行算法训练 DQN 和 actor-critic。本节介绍的异步算法与上一节的异步算法很类似, 都是由 worker 节点计算梯度, 由服务器更新模型参数。

### 1 异步并行双 $\mathbf{Q}$ 学习

$\mathbf{D Q N}$ 和双 $\mathbf{Q}$ 学习: $\mathrm{DQN}$ 是一个神经网络, 记作 $Q(s, a ; \boldsymbol{w})$, 其中 $s$ 是状态, $a$ 是动 作, $\boldsymbol{w}$ 表示神经网络参数（包含多个向量、矩阵、张量）。通常用双 $\mathrm{Q}$ 学习等算法训练 DQN。双 $\mathrm{Q}$ 学习需要目标网络 $Q\left(s, a ; \boldsymbol{w}^{-}\right)$, 它的结构与 $\mathrm{DQN}$ 相同, 但是参数不同。双 $\mathrm{Q}$ 学习属于异策略, 即由任意策略控制智能体收集经验, 事后做经验回放更新 DQN 参 数。第 6 章介绍的高级技巧可以很容易地与双 $\mathrm{Q}$ 学习结合, 此处就不详细解释了。

系统架构： 如图 $13.10$ 所示, 系统中有一个服务器和 $m$ 个 worker 节点。服务器可 以随时给某个 worker 发送一条信息, 一个 worker 也可以随时给服务器发送信息, 但是 worker 之间不能通信。服务器和 worker 都存储 DQN 的参数。服务器上的参数是最新 的, 服务器用 worker 发来的梯度对参数做更新。Worker 节点的参数可能是过时的, 所以 worker 需要频繁向服务器索要最新的参数。Worker 节点有自己的目标网络, 而服务器上 不存储目标网络。每个 worker 节点有自己的环境, 比如运行一个超级玛丽游戏, 用 DQN 控制智能体与环境交互, 收集经验, 把 $\left(s, a, r, s^{\prime}\right)$ 这样的四元组存储到本地的经验回放 数组。在收集经验的同时, worker 节点做经验回放, 计算梯度, 把梯度发送给服务器。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-212.jpg?height=780&width=1431&top_left_y=1600&top_left_x=378)

图 13.10: 用异步并行算法训练 DQN。图中没有画出目标网络。

Worker 端的计算 : 每个 worker 节点本地有独立的环境, 独立的经验回放数组, 还 有一个 DQN 和一个目标网络。（图 $13.10$ 中没有画出目标网络。）设某个 worker 节点当 前参数为 $\boldsymbol{w}_{\text {now }}$ 。它用 $\epsilon$-greedy 策略控制智能体与本地环境交互, 收集经验。 $\epsilon$-greedy 的 定义是:

$$
a_{t}= \begin{cases}\operatorname{argmax}_{a} Q\left(s_{t}, a ; \boldsymbol{w}_{\text {now }}\right), & \text { 以概率 }(1-\epsilon) ; \\ \text { 均匀抽取 } \mathcal{A} \text { 中的一个动作, } & \text { 以概率 } \epsilon .\end{cases}
$$

把收集到的经验 $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ 存入本地的经验回放数组。

与此同时, 所有的 worker 节点都要参与异步梯度下降。Worker 节点在本地做计算, 还要与服务器通信。第 $k$ 号 worker 节点重复下面的步骤:

1. 向服务器发出请求, 索要最新的 $\mathrm{DQN}$ 参数。把接收到的参数记作 $\boldsymbol{w}_{\text {new }}$ 。

2. 更新本地的目标网络：

$$
\boldsymbol{w}_{\text {new }}^{-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{-} .
$$

3. 在本地做经验回放, 计算本地梯度:

(a). 从本地的经验回放数组中随机抽取 $b$ 个四元组, 记作

$$
\left(s_{1}, a_{1}, r_{1}, s_{1}^{\prime}\right),\left(s_{2}, a_{2}, r_{2}, s_{2}^{\prime}\right), \cdots,\left(s_{b}, a_{b}, r_{b}, s_{b}^{\prime}\right) .
$$

$b$ 是批量的大小，由用户自己设定, 比如 $b=16$ 。

(b). 用双 $\mathrm{Q}$ 学习计算 $\mathrm{TD}$ 目标。对于所有的 $j=1, \cdots, b$, 分别计算

$$
\widehat{y}_{j}=r_{j}+\gamma \cdot Q\left(s_{j}^{\prime}, a_{j}^{\prime} ; \boldsymbol{w}_{\text {new }}^{-}\right), \quad \text { 其中 } a_{j}^{\prime}=\underset{a}{\operatorname{argmax}} Q\left(s_{j}^{\prime}, a ; \boldsymbol{w}_{\text {new }}\right) \text {. }
$$

(c). 定义目标函数：

$$
L(\boldsymbol{w}) \triangleq \frac{1}{2 b} \sum_{j=1}^{b}\left[Q\left(s_{j}, a_{j} ; \boldsymbol{w}\right)-\widehat{y}_{j}\right]^{2}
$$

(d). 计算梯度：

$$
\tilde{\boldsymbol{g}}^{k}=\nabla_{\boldsymbol{w}} L\left(\boldsymbol{w}_{\text {new }}\right) .
$$

4. 把计算出的梯度 $\tilde{\boldsymbol{g}}^{k}$ 发送给服务器。

服务器端的计算: 服务器上储存有一份模型参数, 记作 $\boldsymbol{w}_{\text {now }}$ 。每当一个 worker 节 点发来请求, 服务器就把 $\boldsymbol{w}_{\text {now }}$ 发送给该 worker 节点。每当一个 worker 节点发来梯度 $\tilde{\boldsymbol{g}}^{k}$, 服务器就立刻做梯度下降更新参数：

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\alpha \cdot \tilde{\boldsymbol{g}}^{k} .
$$

### A3C: 异步并行 A2C}

$\mathbf{A 2 C}$ 有一个策略网络 $\pi(a \mid s ; \boldsymbol{\theta})$ 和一个价值网络 $v(s ; \boldsymbol{w})$ 。通常用策略梯度更新策略 网络, 用 TD 算法更新价值网络。为了让 $\mathrm{TD}$ 算法更稳定, 需要一个目标网络 $v\left(s ; \boldsymbol{w}^{-}\right)$, 它的结构与价值网络相同, 但是参数不同。A2C 属于同策略, 不能使用经验回放。A2C 的实现详见第 $8.3$ 节。异步并行 A2C 被称作 asynchronous advantage actor-critic (A3C)。

系统架构 : 如图 $13.10$ 所示, 系统中有一个服务器和 $m$ 个 worker 节点。服务器维 护策略网络和价值网络最新的参数, 并用 worker 节点发来的梯度更新参数。每个 worker 节点有一份参数的拷贝, 并每隔一段时间向服务器索要最新的参数。每个 worker 节点有 一个目标网络, 而服务器上不储存目标网络。每个 worker 节点有独立的环境, 用本地的 策略网络控制智能体与环境交互, 用状态、动作、奖励计算梯度。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-214.jpg?height=759&width=1444&top_left_y=423&top_left_x=366)

图 13.11: $\mathrm{A} C \mathrm{C}$, 即异步并行 $\mathrm{A} 2 \mathrm{C}$ 。图中没有画出目标网络。

Worker 端的计算: 每个 worker 节点有独立的环境, 独立做计算, 随时可以与服务 器通信。每个 worker 节点本地有一个策略网络 $\pi(a \mid s ; \boldsymbol{\theta})$ 、一个价值网络 $v(s ; \boldsymbol{w})$ 、一个目 标网络 $v\left(s ; \boldsymbol{w}^{-}\right)$。设第 $k$ 个 worker 节点当前参数为 $\boldsymbol{\theta}_{\text {now }} 、 \boldsymbol{w}_{\text {now }} 、 \boldsymbol{w}_{\text {now }}^{-}$。第 $k$ 个 worker 节点重复下面的步骤:

1. 向服务器发出请求, 索要最新的参数。把接收到的参数记作 $\boldsymbol{\theta}_{\text {new }} 、 \boldsymbol{w}_{\text {new }}$ 。

2. 更新本地的目标网络：

$$
\boldsymbol{w}_{\text {new }}^{-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{-} .
$$

3. 重复下面的步骤 $b$ 次 ( $b$ 是用户设置的超参数), 或是从头到尾完成一回合游戏。让智 能体与环境交互, 计算策略梯度, 并累积策略梯度。全零初始化: $\tilde{\boldsymbol{g}}_{\theta}^{k} \leftarrow \mathbf{0} 、 \tilde{\boldsymbol{g}}_{w}^{k} \leftarrow \mathbf{0}$, 用它们累积梯度。

(a). 基于当前状态 $s_{t}$, 根据策略网络做决策 $a_{t} \sim \pi\left(\cdot \mid s_{t}, \boldsymbol{\theta}\right)$, 让智能体执行动作 $a_{t}$ 。随后观测到奖励 $r_{t}$ 和新状态 $s_{t+1}$ 。

(b). 计算 $\mathrm{TD}$ 目标 $\widehat{y}_{t}$ 和 $\mathrm{TD}$ 误差 $\delta_{t}: 4$

$$
\begin{aligned}
& \widehat{y}_{t}=r_{t}+\gamma \cdot v\left(s_{t+1} ; \boldsymbol{w}_{\text {new }}^{-}\right), \\
& \delta_{t}=v\left(s_{t} ; \boldsymbol{w}_{\text {new }}\right)-\widehat{y}_{t} .
\end{aligned}
$$

(c). 累积梯度：

$$
\begin{aligned}
\tilde{\boldsymbol{g}}_{w}^{k} & \leftarrow \tilde{\boldsymbol{g}}_{w}^{k}+\delta_{t} \cdot \nabla_{\boldsymbol{w}} v\left(s_{t} ; \boldsymbol{w}_{\text {new }}\right) \\
\tilde{\boldsymbol{g}}_{\theta}^{k} & \leftarrow \tilde{\boldsymbol{g}}_{\theta}^{k}+\delta_{t} \cdot \nabla_{\boldsymbol{\theta}} \ln \pi\left(a_{t} \mid s_{t} ; \boldsymbol{\theta}_{\text {new }}\right) .
\end{aligned}
$$

4此处可以用多步 TD 目标等技巧; 详见第 $5.3$ 节。 4. 把累积的梯度 $\tilde{\boldsymbol{g}}_{\theta}^{k}$ 和 $\tilde{\boldsymbol{g}}_{w}^{k}$ 发送给服务器。

服务器端的计算：服务器上储存有一份模型参数, 记作 $\boldsymbol{\theta}_{\text {now }}$ 和 $\boldsymbol{w}_{\text {now }}$ 每当一个 worker 节点发来请求, 服务器就把 $\boldsymbol{\theta}_{\text {now }}$ 和 $\boldsymbol{w}_{\text {now }}$ 发送给该 worker 节点。每当一个 worker 节点发来梯度 $\tilde{\boldsymbol{g}}_{\theta}^{k}$ 和 $\tilde{\boldsymbol{g}}_{w}^{k}$, 服务器就立刻做梯度下降更新参数：

$$
\begin{aligned}
\boldsymbol{w}_{\text {new }} & \leftarrow \boldsymbol{w}_{\text {now }}-\alpha \cdot \tilde{\boldsymbol{g}}_{w}^{k}, \\
\boldsymbol{\theta}_{\text {new }} & \leftarrow \boldsymbol{\theta}_{\text {now }}-\beta \cdot \tilde{\boldsymbol{g}}_{\theta}^{k} .
\end{aligned}
$$



## 第 13 章 知识点

- 并行计算用多个处理器、多台机器加速计算, 使得计算所需的钟表时间减少。使用 并行计算, 每块处理器承担的计算量会减小, 有利于减小钟表时间。

- 常用加速比作为评价并行算法的指标。理想情况下, 处理器数量增加 $m$ 倍, 加速 比就是 $m$ 。然而并行计算还有通信、同步等代价, 加速比通常小于 $m$ 。减小通信时 间、同步时间是设计并行算法的关键。

- 可以用 MapReduce 在集群上做并行计算。MapReduce 属于 client-server 架构, 需要 做同步。

- 同步算法每一轮更新模型之前, 要求所有节点都完成计算。这会造成空闲和等待, 影响整体的效率。而异步算法无需等待, 因此效率更高。在机器学习的实践中, 异 步并行算法比同步并行算法所需的钟表时间更短。

- 本章讲解了异步并行的双 $Q$ 学习算法与 $A 3 C$ 算法。两种算法都是让 worker 端并行 计算梯度, 在服务器端用梯度更新神经网络参数。

## 第 13 章 习题

1. 如果在 1 个节点 (compute node) 上运行某算法, 需要 100 秒才能收敛。如果在 8 个节点上运行该算法, 需要 20 秒收玫。加速比 (speedup ratio) 等于

2. 使用更多的节点做并行计算, 则通信所需的钟表时间会
A. 升高
B. 降低
C. 不变

3. 单机的梯度下降算法与同步并行梯度下降算法在数学上是等价的。
A. 这种说法正确。
B. 这种说法错误。

4. 同步并行梯度下降算法与异步并行梯度下降算法在数学上是等价的。
A. 这种说法正确。
B. 这种说法错误。

## 第 13 章 相关文献

MapReduce 原本是指 Google 内部使用的软件系统, 现在泛指这类系统架构。Google 的 MapReduce 系统不对外开源, 但是外界可以通过 2008 年的论文 ${ }^{[33]}$ 了解系统的设 计。外界有多个开源项目力图实现 MapReduce 系统, 其中最有名的是 Hadoop。后来基 于 Hadoop 等项目开发的 Spark 系统 ${ }^{[131]}$ 比 Hadoop MapReduce 的速度更快。本章介绍

的异步并行算法主要基于 parameter server ${ }^{[66]}$ 的思想。Ray ${ }^{[78]}$ 是一个开源的软件系统, 包含 parameter server 的功能。用 Ray 很容易实现异步并行算法, 而且 Ray 对强化学习有 很好的支持。

本章介绍的并行强化学习算法主要基于 2015 年的论文 ${ }^{[80]}$ 和 2016 年的论文 ${ }^{[75]}$ 。 两篇论文都是异步算法, 主要区别在于 2015 年的论文 ${ }^{[80]}$ 使用经验回放, 而 2016 年的 论文 ${ }^{[75]}$ 不用经验回放。对于 Atari 游戏这类问题, 获取经验非常容易, 于是使用经验 回放与否其实无关紧要。
