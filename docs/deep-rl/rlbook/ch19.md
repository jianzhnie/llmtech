## 第 19 章 现实世界中的应用

强化学习最成功的应用莫过于 Atari、围棋等游戏, 然而在现实中的落地应用还比较 少。本章简要介绍强化学习的几个实际应用: 神经网络超参数搜索、自动生成 SQL 语言、 推荐系统、网约车派单。本章最后两节讨论强化学习与监督学习适用的场景、以及什么 在制约强化学习的落地应用。

## 1 神经网络结构捜索

传统的神经网络结构通常是由人手动设计的。以卷积神经网络 (CNN) 为例, 众所周 知的神经网络结构包括 LeNet、AlexNet、ResNet、GoogLeNet、MobileNet, 它们都是由业 内专家根据经验设计的, 目的在于最大化测试准确率、或者最小化内存和计算开销。神 经网络结构搜索 (neural architecture search, 缩写 NAS) 的意思是自动寻找最优的神经网 络结构, 代替手动设计的神经网络。2017 年的论文 ${ }^{[136]}$ 开创性地将强化学习用于 NAS, 找到的 CNN 结构优于人工设计的 $\mathrm{CNN}$ 。这是强化学习非常成功的应用。遗㨔的是, 这 种方法很快就被不用强化学习的方法超越。尽管如此, 这篇论文的思想仍然具有启发意 义。本节简要描述这种方法的思想; 关心细节的读者可以去阅读原文。

### 1 超参数和交叉验证

为了解释神经网络结构搜索, 需 要先从超参数 (hyper-parameter) 讲 起。深度学习中有两类超参数：

- 结构超参数包括层数、层的类别、 层的大小等数值。以一个卷积层 为例, 其中的超参数包括卷积核 (filter) 的大小, 卷积核的数量, 步 长(stride) 的大小。这些超参数决 定了神经网络的结构。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-291.jpg?height=177&width=602&top_left_y=1576&top_left_x=1044)

模型参数

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-291.jpg?height=97&width=54&top_left_y=1788&top_left_x=1211)

测试集 $\longrightarrow$ 则试准确率

图 19.1：参数与超参数的关系。

- 算法超参数包括学习率 (learning rate)、批大小（batch size)、epoch 数量、正则等。 由于神经网络的非凸性, 用不同的算法超参数会得到不同的解。

图 $19.1$ 解释了超参数与参数之间的关系。模型参数受超参数的控制; 用不同的超参数, 会学出不同的模型参数, 从而会有不同的测试准确率。超参数与参数的区别是什么呢? 两 者之间末必有严格的界限。但通常来说, 损失函数关于模型参数可微, 因此可以用梯度算 法学出模型参数。而损失函数关于超参数不可微, 无法直接用梯度算法学出超参数。通 常需要用交叉验证 (cross validation) 等方法搜索超参数。

在搜索超参数之前, 需要手动指定候选的超参数。假设我们搭建 20 个卷积层, 想要 搜索其中的结构超参数。举个例子, 我们手动指定这些候选超参数： - 卷积核数量： $\{24,36,48,64\}$;

- 卷积核大小: $\{3 \times 3,5 \times 5,7 \times 7\}$;

- 步长大小: $\{1 \times 1,2 \times 2\}$ 。

搜索空间 (search space) 是一个集合, 其中包含所有超参数的组合。在上述例子中, 搜 索空间是这个笛卡尔积：

$$
\{24,36,48,64\}^{20} \times\{3 \times 3,5 \times 5,7 \times 7\}^{20} \times\{1 \times 1,2 \times 2\}^{20} .
$$

公式中的 20 是指 20 个卷积层。搜索空间中元素的数量等于 $(4 \times 3 \times 2)^{20} \approx 4 \times 10^{27}$ 。尽 管每个超参数只有 $2 \sim 4$ 个候选方案，但搜索空间却无比巨大。

如何用交叉验证搜索超参数呢? 首先将训练数据随机划分成两部分, 比如 $80 \%$ 做训 练集 (training set), $20 \%$ 做验证集（validation set)。然后重复下面的步骤很多次：

1. 从搜索空间中均匀随机选出一组超参数的组合, 搭建卷积神经网络。

2. 在训练集上训练神经网络, 从随机初始化开始, 一直到梯度算法收玫。

3. 在验证集评价神经网络, 记录下验证准确率。

最后, 选出最高的验证准确率对应的超参数组合, 完成超参数搜索。上述随机超参数搜 索的缺点是显而易见的:

- 第一, 每次搜索的代价都很大。从随机初始化到算法收玫, 花费的时间少则几十分 钟, 多则几天。如果 GPU 数量有限的话, 顶多只能尝试几千、几万种超参数组合。

- 第二, 搜索空间过于巨大。在上述例子中, 搜索空间中有 $4 \times 10^{27}$ 种超参数组合。 如果把搜索空间比做海洋, 那么几万种超参数组合相当于一克的水。随机搜索超参 数就像是海底捞针。

- 第三, 由于随机性, 验证准确率最高的超参数组合末必是最好的。随机性来自于随 机初始化、随机梯度、数据集的随机划分。在验证集上, 某个超参数的组合取得最 高的准确率, 其中有很大的运气成分; 在测试集上, 这个超参数的组合末必能取得 很高的准确率。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-292.jpg?height=432&width=1431&top_left_y=1840&top_left_x=378)

图 19.2: 图中是 RNN 策略网络, 它输出概率分布 $\boldsymbol{f}_{t}$ 。根据 $\boldsymbol{f}_{t}$ 抽样得到的动作 $a_{t}$ 是一个超参数。

### 2 强化学习方法

2017 年的论文 ${ }^{[136]}$ 设计一种强化学习方法, 用于学习神经网络结构。如图 $19.2$ 所 示, 策略网络是一个循环神经网络 (RNN); 不熟悉 RNN 的读者请回顾第 11 章。策略网 络的输入向量 $\boldsymbol{x}_{t}$ 是对上一个超参数 $a_{t-1}$ 做 embedding 得到的 1 。循环层的向量 $\boldsymbol{h}_{t}$ 可以 看做从序列 $\left[\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{t}\right]$ 中提取的特征。可以把 $s_{t}=\left[\boldsymbol{x}_{t} ; \boldsymbol{h}_{t-1}\right]$ 看做第 $t$ 个状态。策略网 络的输出向量 $\boldsymbol{f}_{t}$ 是一个概率分布。根据 $\boldsymbol{f}_{t}$ 做随机抽样, 得到动作 $a_{t}$, 即第 $t$ 个超参数。

策略网络是如何生成神经网络结构的?下面举一个具体的例子。假设我们搭建 20 个 卷积层, 每个层有 3 个超参数, 那么一共有 60 个超参数。每一层的 3 个超参数从下面的 候选方案中选择。

- 卷积核数量: $\{24,36,48,64\}$;

- 卷积核大小: $\{3 \times 3,5 \times 5,7 \times 7\}$;

- 步长大小: $\{1 \times 1,2 \times 2\}$ 。

按照图 $19.3$ 的描述, 依次生成每一层的卷积核数量、卷积核大小、步长大小。在 RNN 运 行 60 步之后, 得到 60 个超参数, 也就确定了 20 个卷积层的结构。

该如何训练策略网络呢? 为了训练策略网络, 我们需要定义奖励 $r_{t}$ 。在前 59 步, 奖 励全都是零: $r_{1}=\cdots=r_{59}=0$ 。在第 60 步之后, 得到了全部的超参数, 确定了神经网 络结构。然后搭建神经网络, 在训练集上学习神经网络参数, 直到梯度算法收玫。在验 证集上评价神经网络, 得到验证准确率, 作为奖励 $r_{60}$ 。由回报的定义 $u_{t}=r_{1}+\cdots+r_{t}$ 可得：

$$
u_{1}=u_{2}=\cdots=u_{60}=\text { 验证准确率. }
$$

我们希望通过更新 RNN 策略网络的参数, 使得回报越来越大, 即生成的 CNN 的验证准 确率越来越高。把策略网络记作

$$
\pi\left(a_{t} \mid s_{t} ; \boldsymbol{\theta}\right),
$$

其中 $a_{t}$ 是动作 (即超参数), $s_{t}=\left[\boldsymbol{x}_{t}, \boldsymbol{h}_{t-1}\right]$ 是状态, $\boldsymbol{\theta}$ 是 $\mathrm{RNN}$ 策略网络的参数。可以用 REINFORCE 算法更新参数 $\boldsymbol{\theta}$ :

$$
\boldsymbol{\theta}_{\text {new }} \longleftarrow \boldsymbol{\theta}_{\text {now }}+\beta \cdot \sum_{t=1}^{60} u_{t} \cdot \nabla_{\boldsymbol{\theta}} \ln \pi\left(a_{t} \mid s_{t} ; \boldsymbol{\theta}_{\text {now }}\right) .
$$

训练 RNN 策略网络的流程如图 $19.4$ 所示。我们的目标是找到一个好的 CNN 结构, 但是需要借助一个 RNN 策略网络。因为目标是让 CNN 获得尽量高的验证准确率, 所以 用验证准确率作为奖励。这种神经网络结构搜索的计算量非常巨大。每获得一个奖励 $r_{60}$, 都需要从随机初始化开始训练 $\mathrm{CNN}$, 直到梯度算法收玫; 这个过程少则几十分钟、多则 几天。需要重复图 $19.4$ 中流程上万次才能训练好 RNN 策略网络, 其计算代价可想而知。

请读者思考一个问题：为什么一定要用强化学习方法来训练 RNN 策略网络? 是不是 因为强化学习比传统监督学习更有优势? 答案恰恰相反, 强化学习并不好, 只是此处不 得不用而已。如果想要做传统的监督学习, 那么奖励或损失必须关于 RNN 策略网络参 数 $\boldsymbol{\theta}$ 可微; 本节介绍的方法显然不符合这个条件, 所以不能用监督学习训练 RNN 策略 网络。强化学习的奖励可以是任意的, 无需关于 $\boldsymbol{\theta}$ 可微, 因此在这里适用。应用强化学 习的代价是需要大量的训练样本, 至少上万个奖励, 即从初始化开始训练几万个 $\mathrm{CNN}$ 。

${ }^{1}$ 向量 $\boldsymbol{x}_{0}$ 是例外; $x_{0}$ 是用一种特殊的方法随机生成的。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-294.jpg?height=1596&width=1419&top_left_y=310&top_left_x=387)

图 19.3: 用 RNN 策略网络依次生成每一层的 3 个超参数。图中的向量 $\boldsymbol{x}_{t}$ 是 $a_{t-1}$ 做 embedding 得 到的。循环层共享参数, 而全连接层、embedding 层不共享参数。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-294.jpg?height=417&width=1151&top_left_y=2139&top_left_x=521)

图 19.4: 训练 RNN 策略网络的流程。 这种强化学习 NAS 方法的计算量非常大。在这种强化学习 NAS 方法提出之后, 很快就 有更好的 NAS 方法出现, 无需使用强化学习。有兴趣的读者可以了解一下 DARTS 方法 [71]；DARTS 及其变体是比较实用的 NAS 方法。

## 2 自动生成 SQL 语句

Structured Query Language (结构化查询语言), 缩写 SQL, 用于管理关系数据库。SQL 支持数据揷入、查询、更新、删除。将人的语言转化成 $\mathrm{SQL}$ 是自然语言处理领域的一个 重要问题。举个例子, 在订票网站自动对话系统中, 用户提出一个问题:

“请找出 2021 年 10 月 1 日从北京直飞纽约的航班，按照价格从低到高排序。”

程序需要生成 SQL 语言, 查找符合日期、起点、终点的直飞航班, 并且按照价格排序。 解决这个问题的方法类似于机器翻译, 即用 Transformer 等 seq 2 seq 模型将一句自然语言 翻译成 SQL 语言; 见图 19.5。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-296.jpg?height=397&width=1376&top_left_y=969&top_left_x=403)

请找出一的航班, 按照 $\cdots$ 排序

图 19.5: 用 Transformer 等 seq2seq 模型将自然语言翻译成 SQL。

该如何训练图 $19.5$ 这样的 seq2seq 机器翻译模型呢? 最简单的方式就是用监督学习。 事先准备一个数据集, 由人工将自然语言逐一翻译成 SQL 语句。训练的目标是鼓励解码 器输出的 SQL 语句接近人工标注的 $\mathrm{SQL}$ 语句。把解码器的输出、人工标注的 $\mathrm{SQL}$ 两者 的区别作为损失函数, 通过最小化损失函数的方式训练模型。这种单词匹配的训练方式 是可行的, 然而其存在一些局限性。

与标准机器翻译问题相比, SQL 语句的生成有其特殊性。如果是将一句汉语译作英 语, 那么个别单词的翻译错误、顺序错误不太影响人类对翻译结果的理解。对于汉语翻 译英语, 可以把单词的匹配作为评价机器翻译质量的标准。但是这种评价标准不适用于 SQL 语句。

- 即便两个 SQL 语句高度相似, 它们在数据库中执行得到的结果可能完全不同。即 便是一个字符的错误, 也可能导致生成的 SQL 语法错误, 无法执行。

- 哪怕两个 SQL 语句看似区别很大, 它们的作用是完全相同的, 它们在数据库中执 行得到的结果是相同的。

- SQL 的写法会影响执行的效率, 而从 SQL 语句的字面上难以看出它的效率。只有 真正在数据库中执行, 才知道 $\mathrm{SQL}$ 语句究竟花了多长时间。

以上论点说明不该用单词的匹配来衡量生成 SQL 语句的质量, 而应该看 SQL 语言实际 执行的结果是否符合预期。 2017 年的论文 ${ }^{[134]}$ 提出一种强化学习的方法训练 seq2seq 模型, 如图 $19.6$ 所示。可 以把 seq2seq 模型看做策略网络, 把输入的自然语言看做状态, 把生成的 SQL 看做动作。 他们这样定义奖励：

$$
r=\left\{\begin{array}{lc}
-2, & \text { 生成的 SQL 语句不能运行; } \\
-1, & \text { 生成的 SQL 语句可以运行, 但是结果不符合预期; } \\
+1, & \text { 生成的 SQL 语句可以运行, 而且结果符合预期. }
\end{array}\right.
$$

有了奖励, 可以用任意的策略学习算法, 比如 REINFORCE 和 actor-critic。论文 ${ }^{[134]}$ 使 用 REINFORCE 算法训练 seq2seq 模型。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-297.jpg?height=400&width=1260&top_left_y=811&top_left_x=341)

图 19.6: 用强化学习训练 seq2seq 模型的流程。

对比一下监督学习和强化学习方法。监督学习鼓励模型生成的 SQL 语句接近人类专 家写的 $\mathrm{SQL}$, 其本质是行为克隆, 即鼓励模型的决策接近人类专家的动作。而上述强化 学习则不同, 并没有简单模仿人类专家, 而是在数据库中实际执行 SQL 语句, 根据执行 的结果来更新策略。强化学习让策略（seq2seq 模型）与环境（数据库) 实际交互, 而监 督学习 (即行为克隆）并没有与环境交互。

本书介绍论文 ${ }^{[134]}$, 是因为这篇论文的想法比较有意思, 非常符合强化学习的设定, 强化学习可以克服传统监督学习的局限性。这篇论文的实验结果不够强, 很可能只是这 篇论文的方法和实现不够好而已, 不意味着强化学习不适用于 SQL 语句的生成。强化学 习的效果好坏取决于多重因素, 比如策略网络的设计、策略网络的初始化、策略学习的 算法、奖励的定义、甚至是超参数调得是否够好。每个因素都严重影响强化学习的实验 效果。除了本节介绍的 SQL 语句生成, 强化学习在 seq2seq 模型上有很多应用, 读者可 以参考 2019 年综述 [58] 以及其中的文献。

## $19.3$ 推荐系统

网站有海量的物品, 比如 YouTube 的视频、京东的商品、美团外卖的店铺。网站有 百万、甚至上亿的用户, 每个用户有各自的喜好, 喜好可以从他的点击、观看、购买等 历史记录中反映出来。个性化推荐的目标是将用户感兴趣的物品展示给用户, 从而最大 化某些指标（比如点击率、观看时长、购买率、消费金额）。

推荐系统是工业界最推崇的机器学习技术之一, 好的推荐系统可以带来大量的流量 和营收。推荐系统是一个历史悠久、而又热门的研究领域。近年来, 在应用深度学习技 术之后, 推荐系统的效果取得了大幅的提升。强化学习在推荐系统中有一些应用, 但应 用远不如传统监督学习推荐系统广泛。据本书作者了解, 强化学习尚末在工业界的推荐 系中统取得显著效果, 学术论文中报告的结果与工业界线上实测结果不一致。尽管如此, 强化学习仍然是工业界在探索的一个方向。

推荐系统的背景知识很多, 本书无法用较短的篇幅讲清楚强化学习推荐系统的原 理。下面只简单介绍其基本思想。对强化学习推荐系统感兴趣的读者可以阅读以下论文: YouTube 的推荐系统 ${ }^{[27]}$ 、京东的推荐系统 ${ }^{[133]}$ 、阿里巴巴的推荐系统 ${ }^{[54]}$ 。

如图 $19.7$ 所示, 强化学习推荐系统的策略是指根据用户的兴趣点, 从海量物品中选 出一个或几个, 展示给用户。用户的兴趣点就是状态 $s$, 可以从用户人口信息、地理位置、 社交关系、历史活动记录（包括点击、观看、购买记录）这些数据中反映出来。被选中的 物品就是动作 $a$ 。策略网络输出的向量 $\boldsymbol{f}$ 的维度是动作空间的大小 $|\mathcal{A}|$ 。商家的物品种类 非常多, 因此动作空间 $\mathcal{A}$ 非常巨大, $\boldsymbol{f}$ 的维度非常高。简单粗暴地训练策略网络是行不 通的, 必须使用很多技巧做训练; 具体可以参考 YouTube 论文 ${ }^{[27] 。}$

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-298.jpg?height=583&width=1425&top_left_y=1619&top_left_x=378)

图 19.7: 策略网络的一种设计方法。

强化学习推荐系统的奖励需要根据实际问题, 由系统的开发者自己来定。比如在 YouTube 视频网站上, 点击、观看时长、点赞都可以作为奖励。比如在京东购物网站上, 点击、浏览时间、加购物车、购买、消费金额都可以作为奖励。在设计奖励的时候, 需要 格外小心，避免造成意想不到的结果：

- 某视频网站、某新闻网站想提升点击率, 把点击率作为重要的奖励之一。结果大量 骗点击的标题党的排序大幅提升，用户满屏尽是“吓尿了”、“震惊了”。

- 某外卖平台想要提高用户使用 APP 的时长, 把时长作为重要的奖励之一。结果系 统把每个用户经常吃的店铺排到后面, 用户需要花更多时间翻页寻找自己想要的店 铺, 增加了使用 APP 的时间。

以上是网上流传的段子, 末必真实。但是如果你这样设计奖励, 你的产品可能会成为新 的段子。

强化学习推荐系统的一个难点在于探索过程的代价很大；此处的代价不是计算代价, 而是实实在在的金钱代价。强化学习要求智能体（即推荐系统）与环境（即用户）交互, 用收集到的奖励更新策略。如果直接把一个随机初始化的策略上线, 那么在初始探索阶 段, 这个策略会做出纯随机的推荐, 严重影响用户体验, 导致点击、观看、购买数量暴 跌, 给公司业务造成损失。在上线之前, 必须在线下用历史数据初步训练策略。最简单的 方法是在线下用监督学习的方式训练策略网络, 这很类似传统的深度学习推荐系统。阿 里巴巴提出的“虚拟淘宝”系统 ${ }^{[95]}$ 模仿人类用户，得到很多虚拟用户，把这些虚拟用户 作为模拟器的环境。把推荐系统作为智能体, 让它与虚拟用户交互, 利用虚拟的交互记 录来更新推荐系统的策略。等到在模拟器中把策略训练得足够好, 再让策略上线, 与真 实用户交互，进一步更新策略。

## $19.4$ 网约车调度

滴滴是中国最大的网约车平台。乘客在手机 APP 中指定起点和终点, 得到预估报价; 在乘客确认订单之后, 滴滴把订单派发给临近的司机。在同一时刻, 有多个用户下单, 附 近有多辆空车, 该如何派发订单才能最大化网约车司机的收入呢? 滴滴用强化学习方法 解决订单派发问题, 显著提高了网约车司机的收入 $[109] 。$

在讲解强化学习方法之前, 先来看两个具体的例子。如图 $19.8$ 所示, 两个乘客同时 下单, 而附近只有一辆空车, 该给司机派发谁的订单? 如图 19.9 所示, 一个乘客下单, 而 附近有两辆空车, 该把订单派发给哪个司机? 请注意, 滴滴派发订单的目的在于最大化 司机的总收入, 这样既有利于留住司机, 也可以最大化滴滴公司的抽成收入。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-300.jpg?height=445&width=1191&top_left_y=934&top_left_x=501)

图 19.8: 两个乘客同时下单, 附近只有一辆空车, 该给司机派发谁的订单?

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-300.jpg?height=540&width=1194&top_left_y=1506&top_left_x=494)

图 19.9: 一个乘客下单, 附近有两辆空车, 该把订单派发给哪个司机?

对于图 $19.8$ 中的例子, 假如不考虑目的地的热门程度 (即附近接单的容易程度), 则 应该给司机派发上面蓝色目的地的订单, 这样可以让司机在较短的时间内取得更高的收 入。但是这样其实不利于司机的总收入：在司机到达冷门地点之后, 需要等待较长的时 间才会有新的订单。假如给司机派发下面热门目的地的订单, 司机在完成这笔订单后, 立 刻就能接到下一笔订单; 这样虽然单笔收入低, 但是总收入高。

对于图 $19.9$ 中的例子, 很显然应该把订单派送给冷门地点的司机。热门地点的司机 得不到这笔订单几乎没有损失, 因为在很短的时间之后就会有新的订单。而这笔订单对 冷门地点的司机比较重要, 如果没有这笔订单, 司机还需要空等很久才有下一笔订单。

### 1 价值学习}

该如何量化一个地点的热门程度呢? 把司机每一笔订单的收入作为奖励, 把折扣回 报的期望作为状态价值函数 $V_{\pi}(s)$, 用它来衡量热门程度。公式中 $s=$ (地点, 时间) 是状 态, $\pi$ 是派单的策略。 $V_{\pi}(s)$ 可以衡量一个地点在具体某个时间的热门程度。滴滴 2019 年 论文 ${ }^{[109]}$ 的目标是学习 $V_{\pi}(s)$, 从而指导订单派发。这种强化学习方法属于价值学习。

状态价值函数 $V_{\pi}(s)$ 的作用在于预判某个地点在某个时间的热门程度。比如在早高 峰, 车流从居民区开往商业区, 导致商业区是冷门地点, 附近空车多, 订单少。而到了 晩高峰, 商业区是热门地点, 此时下班回家的需求大, 订单数量多。从大数据中不难找 出这种规律。

滴滴用价值网络近似 $V_{\pi}(s)$, 并且用 TD 算法训练价值网络。具体的实现比较复杂, 此处就不具体描述了。值得注意的是, 在学习的过程中要用正则项, 使得价值网络是光 滑的。为什么呢? 当状态 $s=$ (地点, 时间) 中的地点、时间发生较小的变化时, 价值网络 的输出不应该剧烈变化。

### 2 订单派单机制

在学到状态价值函数 $V_{\pi}$ (地点, 时间) 之后, 可以用它来预估任意地点、时间的网约 车的价值, 并利用这一信息来给网约车派发订单。主要想法是用负的 TD 误差来评价一 个订单给一个网约车带来的额外收益。在同一时刻, 某区域内有 $m$ 笔订单, 有 $n$ 个空 车, 那么计算所有 (订单, 空车) 二元组的 TD 误差, 得到一个 $m \times n$ 的矩阵。用二部 图（bipartite graph）匹配算法, 找订单一空车的最大匹配, 完成订单派发。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-301.jpg?height=254&width=1431&top_left_y=1632&top_left_x=244)

图 19.10: 某乘客在 9:10 的时候下单, 滴滴计算在 (起点, 9:10) 和 (终点, 9:43) 的状态价值, 从而 计算出 TD 误差。

首先用图 $19.10$ 中的例子解释如何计算 TD 误差。简单起见, 此处设折扣率 $\gamma=1$, 尽管滴滴使用的折扣率小于 1 。对于图中的例子, TD 目标等于：

$$
\widehat{y}=r+V_{\pi}(\text { 终点, 9:43) }=40+480=520 .
$$

可以这样理解 TD 目标 $\widehat{y}$ : 假设给该空车派发该订单, 那么该笔订单的价值 $r=40$ 加上末 来的状态价值,一共等于 $\widehat{y}=520$ 。但是司机接这笔订单是有机会成本的; 假如不接这笔订 单, 马上就会有别的订单, 可能会获得更高的 TD 目标。机会成本是 $V_{\pi}$ (起点, $\left.9: 10\right)=500$, 即从当前开始的一定时间内获得的总收入的期望等于 500 。用 TD 目标减去机会成本, 即 负的 TD 目标:

$$
-\delta=\widehat{y}-V_{\pi}(\text { 起点, } 9: 10)=520-500=20 \text {. }
$$

这意味着接这笔订单, 司机的收入高于期望收入 20 元。

滴滴的订单派发正是基于上述 TD 误差。举个例子, 在某个区域, 当前有 3 笔订单, 有 4 辆空车。滴滴计算每个 (订单, 空车) 二元组的 TD 误差, 得到图 $19.11$ 中大小为 $3 \times 4$ 矩阵。

\begin{tabular}{|c|c|c|c|c|}
\cline { 2 - 5 } \multicolumn{1}{c|}{} & 空车 \#1 & 空车 \#2 & 空车 \#3 & 空车 \#4 \\
\hline 订单 \#1 & $-\delta_{1,1}=20$ & $-\delta_{1,2}=10$ & $-\delta_{1,3}=12$ & $-\delta_{1,4}=-5$ \\
\hline 订单 \#2 & $-\delta_{2,1}=-2$ & $-\delta_{2,2}=7$ & $-\delta_{2,3}=0$ & $-\delta_{2,4}=-1$ \\
\hline 订单 \#3 & $-\delta_{3,1}=12$ & $-\delta_{3,2}=-3$ & $-\delta_{3,3}=3$ & $-\delta_{3,4}=3$ \\
\hline
\end{tabular}

图 19.11: 在某个区域, 当前有 3 笔订单, 有 4 两空车。滴滴计算每个（订单, 空车) 二元组的 TD 误差, 得到这个矩阵。

有了上面的矩阵, 可以调用二部图匹配算法（比如匈牙利算法）来匹配订单和空车。 图 19.12(左) 是最大匹配, 三条边的权重之和等于 31 , 滴滴按照这种匹配派发订单。图 19.12(右) 也是一种匹配方式, 但是三条边的权重之和只有 30 , 说明它不是最大匹配, 滴 滴不会这样派发订单。
![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-302.jpg?height=418&width=1400&top_left_y=1350&top_left_x=380)

图 19.12: 左图是最大匹配, 三条边的权重之和等于 31 。右图是另一种匹配, 但不是最大匹配, 三 条边的权重之和等于 30 。

## $19.5$ 强化学习与监督学习的对比

强化学习哪里都可以用, 但是多数场景下毫无使用强化学习的必要; 能用监督学习 很好解决的, 没必要用强化学习。本节讨论强化学习与监督学习的区别, 举例分析几种 强化学习有优势的场景。希望读者在理解本节内容之后, 有能力判断那些是强化学习有 前景的应用, 哪些是强化学习的“伪应用”。

### 1 决策是否改变环境?

监督学习假设模型的决策不会影响环境, 而强化学习假设模型的决策会改变环境。 在实际问题中, 模型的决策究竟会不会影响环境呢? 举个例子, 如果你是小散户, 你的交 易 (即动作) 几乎不会影响股价 (即环境); 如果你是大投资机构, 你的大笔交易肯定会 改变股价。如果你是小散户, 你手中有 100 支某股票, 股价是 50 元; 全部卖出得到的现 金是 5,000 元。如果你是投资机构, 你手上有一千万支该股票, 你在二级市场全部卖出; 卖出的过程可能会持续几个小时, 期间股价肯定会连续下跌, 你最终得到的现金会远小 于五亿元。假如投资机构用想用机器学习做股票交易, 必须要考虑到决策对环境的影响。

再举个例子, 如图 $19.13$ 所示, 在 Zillow 等房地产网站上, 待售房屋有卖家的标价, 下面还有 Zillow 自动评估出的参考价格。究竟 Zillow 具体如何给房屋估价, 我们无从得 知。假设由你来开发房屋估价模型, 请问你应该用监督学习, 还是用强化学习? 答案取 决于 Zillow 给出的估价是否会干扰成交价。如果 Zillow 给出的估价不影响买家心理, 不 干扰成交价, 那么直接用回归模型去拟合成交价即可。如果 Zillow 给出的估价会影响成 交价, 那么强化学习或许更为合适。可以把估价模型看做策略, 把计算出的价格看做动 作。将估价展示在Zillow 上, 可能会影响买家心理, 因此改变房地产市场 (环境), 影响 成交价。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-303.jpg?height=316&width=1088&top_left_y=1818&top_left_x=404)

图 19.13: Zillow 网站上待售房屋有两个价格, 一个是卖家标价, 另一个是 Zillow 给出的估价。

在推荐系统中, 推荐算法相当于策略, 而用户的兴趣点相当于环境, 推荐的内容 (动 作）会改变用户兴趣点 (环境)。举个例子, 我原本对养殖业没有兴趣, 但是在 YouTube 给我推送竹鼠养殖的视频之后, 我对此产生很大兴趣, 喜欢点击竹鼠的视频。这说明推 荐系统并非只能被动迎合用户喜好, 推荐系统完全可以主动创造用户的兴趣点。监督学 习假设用户兴趣点（环境）是固定的, 推荐系统只会拟合用户的喜好, 推荐相似的物品。 而强化学习则假设用户的兴趣点可以被改变, 学出的推荐策略会发掘用户新的兴趣点。

### 2 是否需要探索末知的动作 ?

继续讨论推荐系统。思考一下, 为什么强化学习推荐系统可以发现用户新的兴趣点, 而监督学习推荐系统却不可以呢? 这是因为强化学习允许探索, 尝试历史数据中不存在 的动作。比如, 给一个不看美食节目的用户推荐厨师王刚, 给不看农牧的用户推荐竹鼠 养殖, 给不懂编程的人推荐 Python 编程。说不定用户就点击视频了, 而且在看完之后对 此类内容产生浓厚兴趣, 观看更多此类视频。在这种情况下, 给策略反馈较高的奖励。受 到奖励的引导, 推荐策略学会开发用户新的兴趣点, 并在已有兴趣和新兴趣之间寻找平 衡。与强化学习不同, 监督学习通常不做探索, 只是拟合历史记录, 根据用户已有的兴 趣点做推荐, 无法学会挖掘用户新的兴趣点。

打个比方, 两位皮鞋推销员去了某国, 发现当地的人不穿鞋。推销员甲: “既然当地 人不穿鞋, 那么当地没有市场, 我们还是走吧。”推销员乙: “既然当地人不穿鞋, 那么每 个人都是潜在的客户, 应当给他们试穿, 培养他们穿鞋的习惯。”推销员甲相当于监督学 习, 依据已有兴趣点做推荐。推销员乙相当于强化学习, 会尝试新的动作, 发掘潜在的 兴趣点。

推荐系统需要探索, 否则无法挖掘用户的兴趣。但是目前强化学习还没有成功应用 到推荐系统中。那么现在推荐系统是如何做探索的呢? 一种常用的技术叫做多臂老虎机 (multi-arm bandit), 它起到探索的作用。其实多臂老虎机是强化学习的一种特例：多臂老 虎机没有状态、或者只有一个不会改变的状态。多臂老虎机是简化版本的强化学习。

传统监督学习通常不做探索, 但这也不是绝对的, 监督学习也可以做探索。比如试 验设计 (experimental design)、贝叶斯优化 (Baysian optimization) 研究的问题就是 “在 什么地方探索”。具体来说, 我们想要训练函数 $f(\boldsymbol{x})$ 拟合 $y$, 而样本 $(\boldsymbol{x}, y)$ 的数量非 常有限, 每获得一个新的样本的代价都非常大, 比如钻一个几百米的洞勘探矿藏、撞毁 一辆车判断其安全性、电话访谈一位客户了解其满意度。试验设计的目的是根据已知的 $\left(\boldsymbol{x}_{1}, y_{1}\right), \cdots,\left(\boldsymbol{x}_{n}, y_{n}\right)$, 计算出 $\boldsymbol{x}_{n+1}$, 然后基于 $\boldsymbol{x}_{n+1}$ 做试验得到 $y_{n+1}$ 。比如, 已经在 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{n}$ 这 $n$ 个位置钻了洞, 得到 $n$ 组数据, 下一步该在哪里钻洞, 即 $\boldsymbol{x}_{n+1}$ 取多少?

### 3 当前的奖励还是长线的回报 ?

使用监督学习或是强化学习, 还取决于目标是当前的奖励还是长线的回报。人脸识 别这类问题属于 “一锤子买卖”, 只需要关注当前的奖励即可, 因此适用于监督学习。象棋 等游戏则应该考虑长线回报：吃掉对方一个马, 虽然得到了眼前的利益, 但是可能不利 于赢得这局棋。

在滴滴派发订单的应用中, 存在当前奖励和长线回报的问题。眼前奖励就是从当前 订单中获取的收益, 即单位时间内获得的收入；以图 $19.14$ 为例, 单位时间的奖励是 $\frac{40}{33}$ 元。我们之前讨论过, 仅仅最大化眼前利益是不行的, 这样无法最大化长期回报（即总 收入)。一方面, 目的地有“冷”和“热”之分, 会影响司机后续的等待时间和长线收入。另 一方面, 接单虽然能立刻赚到钱, 但是会花费“机会成本”, 如果稍等一下可能会接到更好 的单。出于这两方面的考虑, 滴滴使用强化学习的方法, 最大化长线回报 (总收入), 而 不是眼前的奖励（单笔订单的收入)。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-305.jpg?height=249&width=1425&top_left_y=344&top_left_x=247)

图 19.14: 滴滴派发订单的例子中, 从接单到完成订单, 一共花费 33 分钟, 司机赚 40 元, 单位时 间的奖励是 $\frac{40}{33}$ 元。

在视频网站推荐系统的应用中, 推荐通常不是“一锤子买卖”, 而是为了最大化用户总 的观看时长。因此, 长线的回报比当前的奖励更重要。如图 $19.15$ 所示, 根据已有兴趣做 推荐, 立刻获得较高的奖励; 而尝试挖掘新的兴趣爱好, 眼前收益较小, 但是有利于获 得很高的长期回报。这就是为什么工业界有意愿去尝试强化学习推荐系统, 尽管还没有 取得实际收益。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-305.jpg?height=628&width=1442&top_left_y=1145&top_left_x=247)

图 19.15: 给用户推荐她感兴趣的内容, 点击率会比较高。如果尝试新的兴趣点, 点击率会很低。 可是一旦给用户培养了新的兴趣点, 用户会看更多相关内容, 总的观看时长会大幅增长。

## $19.6$ 什么在制约深度强化学习的应用？

到目前为止, 深度强化学习最成功、最有名的应用仍然是 Atari 游戏、围棋游戏、星 际争霸游戏。深度强化学习有很多现实中的应用, 但其中成功的应用并不多。本节探讨 究竟是什么在制约深度强化学习的落地应用。

### 1 所需的样本数量过大

深度强化学习一个严重的问题在于需要巨大的样本量。举个例子, 如图 $19.17$ 所示, Atari 游戏属于最简单的电子游戏, 在现实世界中找不到这么简单的问题。2015 年的论文 [77] 用 DQN 玩 Atari 游戏, 取得了超越人类玩家的分数, 在学术界内引起了轰动。2015 年提出的原始的 DQN 存在诸多问题, 实验效果不够好。 2018 年的论文 ${ }^{[48]}$ 提出 Rainbow $\mathrm{DQN}$, 将多种技巧结合, 让 DQN 的训练变得更快更好。论文 ${ }^{[48]}$ 在 57 种 Atari 游戏上 比较了原始 DQN、多种高级技巧、以及 Rainbow DQN。图 $19.17$ 中纵轴是算法的分数与 人类分数的比值, 并关于 57 种游戏求中位数; $100 \%$ 表示达到人类玩家的水准。图中横 轴是收集到的游戏帧数, 即样本数量。Rainbow DQN 需要 1 千 8 百万帧才能达到人类玩 家水平, 超过 1 亿帧还末收敛；前提是已经调优了超过 10 种超参数。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-306.jpg?height=571&width=528&top_left_y=1388&top_left_x=433)

图 19.16: Atari 游戏。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-306.jpg?height=611&width=643&top_left_y=1348&top_left_x=1095)

图 19.17: 使用多种技巧训练 DQN 玩 Atari 游戏。图片来自于论文 ${ }^{[48]}$ 。

再举几个例子。AlphaGo Zero ${ }^{[100]}$ 用了 2 千 9 百万局自我博恋, 每一局约有 100 个 状态和动作。TD3 算法 ${ }^{[41]}$ 在 MuJoCo 物理仿真环境中训练 Half-Cheetah、Ant、Hopper 等模拟机器人, 虽然只有几个关节需要控制, 但是在样本数量 100 万时尚末收玫。甚至 连 Pendulum、Reacher 这种只有一两个关节的最简单的控制问题, TD3 也需要超过 10 万 个样本。

现实世界中的问题远远比 Atari、MuJoCo 复杂, 其状态空间、动作空间都远大于 Atari、 MuJoCo。比如星际争霸这种现代的电子游戏, 其复杂度远大于上述的简单问题, 更不要 说自动驾驶这种物理世界中的控制问题了。对于简单的问题, 强化学习尚需要百万、千 万级的样本; 那么对于现实世界中复杂的问题, 强化学习需要多少样本呢? 在电子游戏中获取上亿样本并不困难, 但是在现实问题中每获取一个样本都是比较 困难的。在神经网络结构搜索的例子中, 每获取一个奖励, 需要训练一个 $\mathrm{CNN}$ 。从初始 化到梯度算法收玫, 需要一个 GPU 约一小时的计算量。物理世界的应用中获取奖励更为 困难。举个例子, 用机械手臂抓取一个物体至少需要几秒钟时间, 那么一天只能收集一 万个样本; 同时用十个机械手臂, 连续运转一百天, 才能收集到一千万个样本, 末必够 训练一个深度强化学习模型。强化学习所需的样本量太大, 这会限制强化学习在现实中 的应用。

### 2 探索阶段代价太大

强化学习要求智能体与环境交互, 用收集到的经验去更新策略。在交互的过程中, 智 能体会改变环境。在仿真、游戏的环境中, 智能体对环境造成任何影响都无所谓。但是 在现实世界中, 智能体对环境的影响可能会造成巨大的代价。

在强化学习初始的探索阶段, 策略几乎是随机的。如果是物理世界中的应用, 智能 体的动作难免造成很大的代价。如果应用到推荐系统中, 如果上线一个随机的推荐策略, 那么用户的体验会极差, 很低的点击率也会给网站造成收入的损失。如果应用到自动驾 驶中, 随机的控制策略会导致车辆撞毁。如果应用到医疗中, 随机的治疗方案会致死致 残。

在物理世界的应用中, 不能直接让初始的随机策略与环境交互, 而应该先对策略做 预训练, 再在真实环境中部署。一种方法是事先准备一个数据集, 用行为克隆等监督学 习方法做预训练。另一种方法是搭建模拟器, 在模拟器中预训练策略。比如阿里巴巴提 出的“虚拟淘宝”系统 ${ }^{[95]}$ 是对真实用户的模仿, 用这样的模拟器预训练推荐策略。离线 强化学习（offline RL）是一个热门而又有价值的研究方向, 建议读者阅读文献 [65]。

### 3 超参数的影响非常大

深度强化学习对超参数的设置极其敏感, 需要很小心调参才能找到好的超参数。超 参数分两种: 神经网络结构超参数、算法超参数。这两类超参数的设置都严重影响实验 效果。换句话说, 完全相同的方法, 由不同的人实现, 效果会有天壤之别。

结构超参数 : 神经网络结构超参数包括层的数量、宽度、激活函数, 这些都对结果 有很大影响。拿激活函数来说, 在监督学习中, 在隐层中用不同的激活函数（比如 ReLU、 Leaky ReLU）对结果影响很小, 因此总是用 ReLU 就可以。但是在深度强化学习中, 隐 层激活函数对结果的影响很大; 有时 ReLU 远好于 Leaky ReLU, 而有时 Leaky ReLU 远 好于 $\operatorname{ReLU}{ }^{[47]}$ 。由于这种不一致性, 我们在实践中不得不尝试不同的激活函数。

算法超参数 : 强化学习中的算法超参数很多, 包括学习率、批大小 (batch size)、经 验回放的参数、探索用的噪声。比如 Rainbow 的论文 ${ }^{[48]}$ 调了超过 10 种算法超参数。

- 学习率 (即梯度算法的步长) 对结果的影响非常大, 必须要很仔细地调。DDPG、 TD3、A2C 等方法中不止有一个学习率。策略网络、价值网络、目标网络中都有各 自的学习率。 - 如果用经验回放, 那么还需要调几个超参数, 比如回放数组的大小、经验回放的起 始时间等。论文 ${ }^{[36]}$ 中的实验显示回放数组的大小对结果有影响, 过大或者过小 的数组都不好。经验回放的起始时间需要调, 比如 Rainbow 在收集到 8 万条四元组 的时候开始经验回放, 而标准的 DQN 则最好是在收集到 20 万条之后开始经验回 放 ${ }^{[48]}$ 。

- 在探索阶段, DQN、DPG 等方法的动作中应当加入一定噪声。噪声的大小是需要 调的超参数, 它可以平衡探索 (exploration) 和利用 (exploitation)。除了设置初始 的噪声的幅度, 我们还需要设置噪声的衰减率, 让噪声逐渐变小。

实验效果严重依赖于实现的好坏：上面的讨论目的在于说明超参数对结果有重大影 响。对于相同的方法, 不同的人会有不同的实现, 比如用不同的网络结构、激活函数、训 练算法、学习率、经验回放、噪声。哪怕是一些细微的区别, 也会影响最终的效果。论文 [47]使用了几个比较有名的开源代码, 它们都有 TRPO 和 DDPG 方法在 Half-Cheetah 环 境中的实验。论文使用了它们的默认设置, 比较了实验结果, 如图 $19.18$ 所示。这组实验 说明用相同的方法, 但是由不同人的编程实现, 最终的效果差距巨大。
![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-308.jpg?height=388&width=1444&top_left_y=1191&top_left_x=366)

图 19.18: 左图是 TPRO 的三种实现, 右图是 DDPG 的三种实现。图片来自论文 ${ }^{[47]}$ 。

实验对比的可靠性问题: 如果一篇学术论文提出一种新的方法, 往往要在 Atari、 MuJoCo 等标准的实验环境中做实验, 并与 DQN、DDPG、TD3、A2C、TRPO 等有名的 基线做实验对照。通常只有当新的方法效果显著优于基线时, 论文才有可能发表。但是 论文实验中报告的结果真的可信吗? 从图 $19.18$ 中不难看出, 基线算法的表现严重依赖 于编程实现的好坏。如果你提出一种新的方法, 你把自己的方法实现得非常好, 而你从 开源的实现中选一个不那么好的基线做实验对比, 那么你可以轻松打败基线算法。

### 4 稳定性极差

强化学习训练的过程中充满了随机性。除了环境的随机性之外, 随机性还来自于神 经网络随机初始化、决策的随机性、经验回放的随机性。想必大家都有这样的经历：用 完全相同的程序、完全相同的超参数, 仅仅更改随机种子 (random seed), 就会导致训练 的效果有天壤之别。如示意图 $19.19$ 所示, 如果重复训练十次, 往往会有几次完全不收 敛。哪怕是非常简单的问题, 也会出现这种不收玫的情形。

在监督学习中, 由于随机初始化和随机梯度中的随机性, 即使用同样的超参数, 训 练出的模型表现也会不一致, 测试准确率可能会差几个百分点。但是监督学习中几乎不 会出现图 $19.19$ 中这种情形; 如果出现了, 几乎可以肯定代码中有错。但是强化学习确实 会出现完全不收玫的情形, 哪怕代码和超参数都是对的。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-309.jpg?height=556&width=985&top_left_y=413&top_left_x=473)

图 19.19: 用完全相同的超参数, 用不同的随机种子, 往往会得到截然不同的收玫曲线。

## 第 19 章 知识点 ro

- 神经网络结构搜索 (NAS) 的意思是在人工指定的范围内自动寻找最优的神经网络 结构。可以用循环神经网络作为策略网络, 每一步生成一个结构超参数。用 REINFORCE 算法训练策略网络, 回报是验证集上的准确率。这是强化学习的一个成功 应用，尽管这种方法已经不是最优 NAS 方法。

- 可以用 Transformer 等 seq2seq 模型, 对人的自然语言做结构化, 自动生成 SQL 语 言。可以把 Transformer 当做策略网络, 把生成的 SQL 在数据库中执行的效果看做 回报, 用强化学习训练 Transformer。这种方法比拟合专家写的 SQL 语言更合理。

- 工业界有很多强化学习推荐系统的尝试。强化学习看重长期回报, 而非短期奖励, 有利于挖掘用户的潜在和长期兴趣。已知的强化学习推荐系统在工业界的尝试并不 成功, 还没有取得正向收益。但这不意味着强化学习推荐系统没有用, 或许末来会 有成功的强化学习推荐系统。

- 网约车调度是强化学习非常成功的落地应用, 已经在工业界取得实际收益。基本原 理是价值学习, 用 TD 算法训练状态价值函数, 函数可以估算任意地点和时刻的价 值。在实际派单的时候, 用函数计算起点和终点的价值, 再计算 TD 误差, TD 误 差可以反映出订单扣除机会成本后的实际价值。

- 当前强化学习的落地应用并不多, 且最有名的应用大多是围棋、电子游戏等简单任 务。制约强化学习落地应用的原因包括所需样本数量过大、探索阶段代价过大、对 超参数敏感、算法稳定性差。
