
## 第 3 章 强化学习基本概

本章讲解强化学习的基本概念。第 $3.1$ 节介绍马尔可夫决策过程 (Markov decision process, 简称 MDP), 它是最常见的对强化学习建模的方法。第 $3.2$ 节定义策略函数, 包 括随机策略和确定策略。第 $3.3$ 节分析强化学习中的随机性的两个来源。第 $3.5$ 节定义回 报和折扣回报。第 $3.5$ 节定义动作价值函数和状态价值函数。第 $3.6$ 节介绍强化学习常用 的实验环境。

## $3.1$ 马尔可夫决策过

强化学习的主体被称为智能体 (agent)。通俗地说, 由谁做动作或决策, 谁就是智能 体。比如在超级玛丽游戏中, 玛丽奥就是智能体。在自动驾驶的应用中, 无人车就是智 能体。环境 (environment) 是与智能体交互的对象, 可以抽象地理解为交互过程中的规 则或机理。在超级玛丽的例子中, 游戏程序就是环境。在围棋、象棋的例子中, 游戏规 则就是环境。在无人驾驶的应用中，真实的物理世界则是环境。

强化学习的数学基础和建模工具是马尔可夫决策过程 (Markov decision process, MDP)。 一个 MDP 通常由状态空间、动作空间、状态转移函数、奖励函数、折扣因子等组成。

在每个时刻, 环境有一个状态 (state), 可以理解为对当前时刻环境的概括。在超级 玛丽的例子中, 可以把屏幕当前的画面（或者最近几帧画面）看做状态。玩家只需要知 道当前画面（或者最近几帧画面）就能够做出正确的决策, 决定下一步是让超级玛丽向 左、向右、或是向上。因此, 状态是做决策的依据。

再举一个例子, 在中国象棋、五子棋游戏中, 棋盘上所有棋子的位置就是状态, 因 为当前格局就足以供玩家做决策。假设你不是从头开始一局游戏, 而是接手别人的残局。 你只需要仔细观察棋盘上的格局, 你就能够做出决策。知道这局游戏的历史记录（即每 一步是怎么走的)，并不会给你提供额外的信息。

举一个反例。星际争霸、红色警戒、英雄联盟这些游戏中, 玩家屏幕上最近的 100 帧 画面并不是状态, 因为这些画面不是对当前环境完整的概括。在地图上某个你看不见的 角落里可能正在发生些事件, 这些事件足以改变游戏的结局。一个玩家屏幕上的画面只 是对环境的部分观测（partial observation)。最近的 100 帧画面并不足以供玩家做决策。

状态空间 (state space) 是指所有可能存在状态的集合, 记作花体字母 $\mathcal{S}$ 。状态空间 可以是离散的, 也可以是连续的。状态空间可以是有限集合, 也可以是无限可数集合。在 超级玛丽、星际争霸、无人驾驶这些例子中, 状态空间是无限集合, 存在无穷多种可能 的状态。围棋、五子棋、中国象棋这些游戏中, 状态空间是离散有限集合, 可以枚举出 所有可能存在的状态 (也就是棋盘上的格局)。

动作 (action) 是智能体基于当前状态所做出的决策。在超级玛丽的例子中, 假设玛 丽奥只能向左走、向右走、向上跳。那么动作就是左、右、上三者中的一种。在围棋游戏 中, 棋盘上有 361 个位置, 于是有 361 种动作, 第 $i$ 种动作是指把棋子放到第 $i$ 个位置 上。动作的选取可以是确定性的, 也可以是随机的。随机是指以一定概率选取一个动作, 后面将会具体讨论。

动作空间 (action space) 是指所有可能动作的集合, 记作花体字母 $\mathcal{A}$ 。在超级玛丽例 子中, 动作空间是 $\mathcal{A}=\{$ 左, 右, 上 $\}$ 。在围棋例子中, 动作空间是 $\mathcal{A}=\{1,2,3, \cdots, 361\}$ 。 动作空间可以是离散集合或连续集合, 可以是有限集合或无限集合。

奖励 (reward) 是指在智能体执行一个动作之后, 环境返回给智能体的一个数值。奖 励往往由我们自己来定义, 奖励定义得好坏非常影响强化学习的结果。比如可以这样定 义, 玛丽奥吃到一个金币, 获得奖励 $+1$; 如果玛丽奥通过一局关卡, 奖励是 $+1000$; 如 果玛丽奥碰到敌人, 游戏结束, 奖励是 $-1000$; 如果这一步什么都没发生, 奖励就是 0 。 怎么定义奖励就见仁见智了。我们应该把打赢游戏的奖励定义得大一些, 这样才能鼓励 玛丽奥通过关卡，而不是一味地收集金币。

通常假设奖励是当前状态 $s$ 、当前动作 $a$ 、下一时刻状态 $s^{\prime}$ 的函数, 把奖励函数记作 $r\left(s, a, s^{\prime}\right)$ 。 ${ }^{1}$ 有时假设奖励仅仅是 $s$ 和 $a$ 的函数, 记作 $r(s, a)$ 。我们总是假设奖励函数是 有界的, 即对于所有 $a \in \mathcal{A}$ 和 $s, s^{\prime} \in \mathcal{S}$, 有 $\left|r\left(s, a, s^{\prime}\right)\right|<\infty$ 。

状态转移 (state transition) 是指智能体从当前 $t$ 时刻的状态 $s$ 转移到下一个时刻状 态为 $s^{\prime}$ 的过程。在超级玛丽的例子中, 基于当前状态（屏幕上的画面， 玛丽奥向上跳 了一步, 那么环境（即游戏程序）就会计算出新的状态（即下一帧画面）。在中国象棋的 例子中, 基于当前状态（棋盘上的格局), 红方让“车”走到黑方“马”的位置上, 那么环境 （即游戏规则）就会将黑方的“马”移除, 生成新的状态（棋盘上新的格局）。

状态转移可能是随机的, 而且强化学习通常假设状态转移是随机的, 随机性来自于 环境。图 $3.1$ 中的例子说明状态转移的随机性。我们用状态转移概率函数 (state transition probability function) 来描述状态转移, 记作

$$
p_{t}\left(s^{\prime} \mid s, a\right)=\mathbb{P}\left(S_{t+1}^{\prime}=s^{\prime} \mid S_{t}=s, A_{t}=a\right),
$$

表示这个事件的概率：在当前状态 $s$, 智能体执行动作 $a$, 环境的状态变成 $s^{\prime}$ 。

状态转移可以是确定性的。给定当前的状态 $s$, 智能体执行动作 $a$, 环境用某个函数 $\tau_{t}$ 计算出新的状态 $s^{\prime}=\tau_{t}(s, a)$ 。比如中国象棋中的状态转移就是确定性的, 下一个状态 $s^{\prime}$ 完全由 $s$ 和 $a$ 决定, 环境中不存在随机性。确定状态转移是随机状态转移的一个特例, 即概率全部集中在一个状态 $s^{\prime}$ 上:

$$
p_{t}\left(s^{\prime} \mid s, a\right)= \begin{cases}1, & \text { if } \tau_{t}(s, a)=s^{\prime} \\ 0, & \text { otherwise. }\end{cases}
$$

因此, 本书只考虑随机状态转移。实际中, 通常假设状态转移概率函数是平稳的 (简记为 $p\left(s^{\prime} \mid s, a\right)$ 或 $\left.\tau(s, a)\right)$, 即函数不会随着时刻 $t$ 变化。

${ }^{1}$ 此处隐含的假设是奖励函数是平稳的 (stationary), 即它不随着时刻 $t$ 变化。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-041.jpg?height=577&width=856&top_left_y=257&top_left_x=540)

图 3.1: 这个例子说明状态转移的随机性。如果玛丽奥向上跳, 玛丽奥的位置就到上面来了, 这个 是确定的。但是标出的敌人 Goomba 有可能往左, 也有可能往右。Goomba 移动的方向可以是随 机的。即使当前状态 $s$ 和智能体的动作 $a$ 确定了, 也无法确定下一个状态 $s^{\prime}$ 。

## $3.2$ 策略

策略（policy）的意思是根据观测到的状态, 如何做出决策, 即如何从动作空间中选 取一个动作。举个例子, 假设你在玩超级玛丽游戏, 当前屏幕上的画面是图 3.1, 请问你 该做什么决策? 有很大概率你会决定向上跳, 这样可以避开敌人, 还能吃到金币。向上 跳这个动作就是你大脑中的策略做出的决策。

强化学习的目标就是得到一个策略函数, 在每个时刻根据观测到的状态做出决策。 策略可以是确定性的, 也可以是随机性的, 两种都非常有用。我们现在较为具体讨论策 略函数, 首先假设策略仅仅依赖于当前状态, 而不依赖于历史状态。在第章我们将有严 格的描述。

随机策略。把状态记作 $S$ 或 $s$, 动作记作 $A$ 或 $a$, 随机策略函数 $\pi: \mathcal{S} \times \mathcal{A} \mapsto[0,1]$ 是一个概率密度函数：

$$
\pi(a \mid s)=\mathbb{P}(A=a \mid S=s) .
$$

策略函数的输入是状态 $s$ 和动作 $a$, 输出是一个 0 到 1 之间的概率值。以超级玛丽为例, 状态是游戏屏幕画面, 把它作为策略函数的输入, 策略函数可以告诉我每个动作的概率 值：

$$
\begin{aligned}
& \pi(\text { 左 } \mid s)=0.2, \\
& \pi(\text { 右 } \mid s)=0.1, \\
& \pi(\text { 上 } \mid s)=0.7 .
\end{aligned}
$$

如果你让策略函数 $\pi$ 来自动操作玛丽奥打游戏, 它就会做一个随机抽样: 以 $0.2$ 的概率 向左走, $0.1$ 的概率向右走, $0.7$ 的概率向上跳。三种动作都有可能发生, 但是向上的概 率最大, 向左概率较小, 向右概率很小。

确定策略。确定策略记作 $\mu: \mathcal{S} \mapsto \mathcal{A}$, 它把状态 $s$ 作为输入, 直接输出动作 $a=\mu(s)$, 而不是输出概率值。对于给定的状态 $s$, 做出的决策 $a$ 是确定的, 没有随机性。可以把确 定策略看做随机策略的一种特例, 即概率全部集中在一个动作上:

$$
\pi(a \mid s)= \begin{cases}1, & \text { if } \mu(s)=a \\ 0, & \text { otherwise. }\end{cases}
$$

智能体与环境交互 (agent environment interaction) 是指智能体观测到环境的状态 $s$, 做出动作 $a$, 动作会改变环境的状态, 环境反馈给智能体奖励 $r$ 以及新的状态 $s^{\prime}$ 。图 $3.2$ 是智能体与环境交互的示意图。在超级玛丽游戏中, 智能体是玛丽奥, 环境是游戏 程序。AI 以下面的方式控制玛丽奥跟游戏程序交互。观测到当前状态 $s, \mathrm{AI}$ 用决策规则 $\pi(a \mid s)$ 算出所有动作的概率, 比如算出

$$
\pi(\text { 左 } \mid s)=0.2, \quad \pi(\text { 右 } \mid s)=0.1, \quad \pi(\text { 上 } \mid s)=0.7 \text {. }
$$

按照概率做随机抽样, 得到其中一个动作 (比如向上), 记作 $a$, 然后玛丽奥执行这个动 作。游戏程序会用状态转移函数 $p_{t}\left(s^{\prime} \mid s, a\right)$ 随机生成新的状态 $s^{\prime}$, 并反馈给玛丽奥一个奖 励 $r\left(s, a, s^{\prime}\right)$ 。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-043.jpg?height=374&width=1145&top_left_y=281&top_left_x=387)

图 3.2: 智能体与环境交互。

强化学习中常提到回合 (episodes), 特别常见于游戏类的场景。 “回合”的概念来自游 戏, 指智能体从游戏开始到通关或者结束的过程。强化学习对样本数量的要求很高, 即 便是个简单的游戏, 也需要玩上万回合游戏才能学到好的策略。Epoch 是一个类似而又 有所区别的概念, 常用于监督学习。一个 epoch 意思是用所有训练数据进行前向计算和 反向传播, 而且每条数据恰好只用一次。

## 3 随机性的来源

这一节的内容是强化学习中的随机 性。随机性有两个来源：策略函数与状态 转移函数。搞明白随机性的两个来源, 对 之后的学习很有帮助。本书中用 $S_{t}$ 和 $s_{t}$ 分 别表示 $t$ 时刻的状态及其观测值, 用 $A_{t}$ 和 $a_{t}$ 分别表示 $t$ 时刻的动作及其观测值。

动作的随机性来自于随机决策。给定 当前状态 $s$, 策略函数 $\pi(a \mid s)$ 会算出动作

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-044.jpg?height=366&width=648&top_left_y=314&top_left_x=1138)

图 3.3: 状态空间是 $\mathcal{A}=\{$ 左, 中, 右 $\}$ 。把当前状 态 $s$ 输入策略函数, 策略函数输出三个概率值: $0.2,0.1,0.7$ 。所以, 对于确定的状态 $s$, 智能体执 行的动作是不确定的, 三个动作都可能被执行。 空间 $\mathcal{A}$ 中每个动作 $a$ 的概率值。智能体执 行的动作是随机抽样的结果, 所以带有随机性。见图 $3.3$ 中的例子。

状态的随机性来自于状态转移函数。 当状态 $s$ 和动作 $a$ 都被确定下来, 下一个 状态仍然有随机性。环境 (比如游戏程序) 用状态转移函数 $p\left(s^{\prime} \mid s, a\right)$ 计算所有可能的 状态的概率, 然后做随机抽样, 得到新的 状态。见图 $3.4$ 中的例子。

奖励是状态和动作的函数。方便起见, 此处我们假设 $t$ 时刻的奖励是 $\left(s_{t}, a_{t}\right)$ 的函 数 ${ }^{2}$, 记作:

$$
r_{t}=r\left(s_{t}, a_{t}\right) .
$$

所有可能出现 的新状态

概 率值
![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-044.jpg?height=552&width=468&top_left_y=1040&top_left_x=1336)

图 3.4: 已知当前状态 $s$, 智能体已经做出决策 一向上跳, 那么环境会更新状态。环境把 $s$ 和 $a$ 输入状态转移函数, 得到所有可能的状态的概 率值。环境根据概率值做随机抽样, 得到新的状 态 $s^{\prime}$ 。

基于这种假设, 给定当前状态 $s_{t}$ 和动作 $a_{t}$, 那么奖励 $r_{t}$ 就是唯一确定的。如果 $A_{t}$ 还 没被观测到, 或者 $\left(S_{t}, A_{t}\right)$ 都没被观测到, 那么 $t$ 时刻的奖励就有不确定性。我们用

$$
R_{t}=r\left(s_{t}, A_{t}\right) \quad \text { 或 } \quad R_{t}=r\left(S_{t}, A_{t}\right)
$$

表示 $t$ 时刻的奖励随机变量, 它的随机性来自于 $A_{t}$ 或者 $\left(S_{t}, A_{t}\right)$ 。

马尔可夫性质 (Markov property)。上文在讲解状态转移的时候, 假设状态转移具 有马尔可夫性质, 即:

$$
\mathbb{P}\left(S_{t+1} \mid S_{t}, A_{t}\right)=\mathbb{P}\left(S_{t+1} \mid S_{1}, A_{1}, S_{2}, A_{2}, \cdots, S_{t}, A_{t}\right) .
$$

公式的意思是下一时刻状态 $S_{t+1}$ 仅依赖于当前状态 $S_{t}$ 和动作 $A_{t}$, 而不依赖于过去的状 态和动作。

轨迹 (trajectory) 是指一回合 (episode) 游戏中, 智能体观测到的所有的状态、动 作、奖励：

$$
s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, s_{3}, a_{3}, r_{3}, \cdots
$$

2很多情况下, $t$ 时刻的奖励是 $\left(s_{t}, a_{t}, s_{t+1}\right)$ 的函数。 图 $3.5$ 描绘了轨迹中状态、动作、奖励的顺序。在 $t$ 时刻, 给定状态 $S_{t}=s_{t}$, 下面这些 都是观测到的值:

$$
s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \cdots, s_{t-1}, a_{t-1}, r_{t-1}, s_{t},
$$

而下面这些都是随机变量（尚末被观测到)：

$$
A_{t}, R_{t}, S_{t+1}, A_{t+1}, R_{t+1}, \quad S_{t+2}, A_{t+2}, R_{t+2}, \cdots
$$

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-045.jpg?height=217&width=1408&top_left_y=640&top_left_x=264)

图 3.5: 智能体的轨迹。

## $3.4$ 回报与折扣回报

本节介绍回报（return）和折扣回报（discounted return）这两个概念, 并且讨论其随 机性来源。由于回报是折扣率等于 1 的特殊折扣回报, 后面的章节中用“回报”指代“折扣 回报”,不再区分两者。本节我们用 $R_{t}$ 和 $r_{t}$ 表示 $t$ 时刻奖励随机变量及其观测值。

# 1 回报

回报（return） 是从当前时刻开始到本回合结束的所有奖励的总和, 所以回报也叫 做累计奖励 (cumulative future reward) 。把 $t$ 时刻的回报记作随机变量 $U_{t}$ 。如果一回合 游戏结束, 已经观测到所有奖励, 那么就把回报记作 $u_{t}$ 。设本回合在时刻 $n$ 结束。定义 回报为:

$$
U_{t}=R_{t}+R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_{n} .
$$

回报有什么用呢? 回报是末来获得的奖励总和, 所以智能体的目标就是让回报尽量大, 越 大越好。强化学习的目标就是寻找一个策略, 使得回报的期望最大化。这个策略称为最 优策略 (optimum policy)。

强化学习的目标是最大化回报, 而不是最大化当前的奖励。打个比方, 下棋的时候, 你的目标是赢得一局比赛 (回报), 而非吃掉对方当前的一个棋子（奖励)。

### 2 折扣回报}

思考一个问题: 在 $t$ 时刻, 请问奖励 $r_{t}$ 和 $r_{t+1}$ 同等重要吗? 假如我给你两个选项： 第一, 现在我立刻给你 100 元钱; 第二, 等一年后我给你 100 元钱。你选哪个? 理性人应 该都会选现在得到 100 元钱。这是因为末来的不确定性很大, 即使我现在答应明年给你 100 元, 你也末必能拿到。大家都明白这个道理：明年得到 100 元不如现在立刻拿到 100 元。

要是换一个问题, 现在我立刻给你 80 元钱, 或者是明年我给你 100 元钱。你选哪一 个? 或许大家会做不同的选择, 有的人愿意拿现在的 80 , 有的人愿意等一年拿 100 。如 果两种选择一样好, 那么就意味着一年后的奖励的重要性只有今天的 $\gamma=0.8$ 倍。这里 的 $\gamma=0.8$ 就是折扣率（discount factor）。这些例子都隐含奖励函数是平稳的。

同理, 在 MDP 中, 通常使用折扣回报（discounted return），给末来的奖励做折扣。 这是折扣回报的定义：

$$
U_{t}=R_{t}+\gamma \cdot R_{t+1}+\gamma^{2} \cdot R_{t+2}+\gamma^{3} \cdot R_{t+3}+\cdots
$$

这里的 $\gamma \in[0,1]$ 叫做折扣率。对待越久远的末来, 给奖励打的折扣越大。在有限期 MDP 中, 折扣可以被吸收到奖励函数中, 即把 $\gamma^{i} \cdot R_{t+i}$ 当作一个新奖励函数 $R_{t+i}$ 。注意此时 新奖励函数不再是平稳的, 即使原来奖励函数是平稳的。在无限期 MDP 中, 折扣因子起 着重要作用, 它和奖励函数有界性一起能保证上面无穷求和级数的收玫性。无论是有限 期 MDP 还是无限期 MDP, 只要当假定奖励函数是平稳的, 本书总是考虑折扣奖励。

### 3 回报中的随机性

假设一回合游戏一共有 $n$ 步。当完成这一回合之后, 我们观测到所有 $n$ 个奖励: $r_{1}, r_{2}, \cdots, r_{n}$ 。此时这些奖励不是随机变量, 而是实际观测到的数值。此时我们可以实 际计算出折扣回报

$$
u_{t}=r_{t}+\gamma \cdot r_{t+1}+\gamma^{2} \cdot r_{t+2}+\cdots+\gamma^{n-t} \cdot r_{n}, \quad \forall t=1, \cdots, n .
$$

此时的折扣回报 $u_{t}$ 是实际观测到的数值, 不具有随机性。

$$
\begin{aligned}
& \text { 观测值 末知变量 }
\end{aligned}
$$

图 3.6: 智能体的轨迹中 $s_{t}$ 及其之前的状态、动作、奖励都被观测到, 而 $A_{t}$ 及其之后的状态、动 作、奖励都是末知变量。

假设我们此时在第 $t$ 时刻, 只观测到 $s_{t}$ 及其之前的状态、动作、奖励

$$
s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \cdots, s_{t-1}, a_{t-1}, r_{t-1}, s_{t},
$$

而下面这些都是随机变量（尚末被观测到）:

$$
A_{t}, R_{t}, S_{t+1}, A_{t+1}, R_{t+1}, \cdots, S_{n}, A_{n}, R_{n} .
$$

见图 3.6。回报 $U_{t}$ 依赖于奖励 $R_{t}, R_{t+1}, \cdots, R_{n}$, 而这些奖励全都是末知的随机变量, 所 以 $U_{t}$ 也是末知的随机变量。

请问回报 $U_{t}$ 的随机性的来源是什么? 奖励 $R_{t}$ 依赖于状态 $s_{t}$ （已观测到）与动作 $A_{t}$ (末知变量), 奖励 $R_{t+1}$ 依赖于 $S_{t+1}$ 和 $A_{t+1}$ (末知变量), 奖励 $R_{t+2}$ 依赖于 $S_{t+2}$ 和 $A_{t+2}$ (末知变量), 以此类推。所以 $U_{t}$ 的随机性来自于这些动作和状态:

$$
A_{t}, S_{t+1}, A_{t+1}, S_{t+2}, A_{t+2}, \cdots, S_{n}, A_{n} .
$$

动作的随机性来自于策略, 状态的随机性来自于状态转移概率函数。

### 4 有限期 MDP 和无限期 MDP

MDP 的时间步可以是有限期（finite-horizon）或无限期（infinite-horizon）。有限期 MDP 存在一个终止状态（terminal state), 该状态被智能体触发后, 一个回合 (episode) 结束。与之对应的是无限期 MDP, 即环境中不存在终止状态, 这会导致奖励的加和趋于 无穷。

回顾折扣回报的定义: $u_{t}=\sum_{i=t}^{n} \gamma^{i-t} r_{i}$ 。对于无限期 MDP, 使用 $\gamma=1$ 会导致回报 等于无穷:

$$
\lim _{n \rightarrow \infty} \sum_{i=t}^{n} r_{i}=\infty .
$$

这是不合适的。因此, 如果 $n$ 很大、甚至无穷, 则设置一个小于 1 的折扣率是非常必要 的。假设对于所有的 $s \in \mathcal{S}$ 和 $a \in \mathcal{A}$, 回报函数有界 $|r(s, a)|<b$ 。那么对于 $\gamma \in[0,1)$,

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-047.jpg?height=123&width=1411&top_left_y=738&top_left_x=254)

且 $r(s, a)$, 有这样的性质:

$$
\left|\lim _{n \rightarrow \infty} \sum_{i=t}^{n} \gamma^{i-t} r_{i}\right| \leq \frac{b}{1-\gamma} .
$$

这个公式说明使用小于 1 的折扣率的话, 无限时间步的回报是有界的。

本书后面章节统一用 $n$ 表示回合的长度。方便起见, 我们就不再严格区分有限期和 无限期的情况, 即不区分 $n$ 是有界、还是 $n \rightarrow \infty$ 。

## $3.5$ 价值函数

这一节介绍动作价值函数 $Q_{\pi}(s, a)$, 最优动作价值函数 $Q_{\star}(s, a)$, 状态价值函数 $V_{\pi}(s)$ 。 它们都是回报的期望。

# 1 动作价值函数}

上一节介绍了（折扣）回报 $U_{t}$, 它是从 $t$ 时刻起, 末来所有奖励的（加权）和。在 $t$ 时刻, 假如我们知道 $U_{t}$ 的值, 我们就知道游戏是快赢了还是快输了。然而在 $t$ 时刻我 们并不知道 $U_{t}$ 的值, 因为此时 $U_{t}$ 仍然是个随机变量。在 $t$ 时刻, 我们不知道 $U_{t}$ 的值, 而我们又想预判 $U_{t}$ 的值从而知道局势的好坏。该怎么办呢? 解决方案就是对 $U_{t}$ 求期望, 消除掉其中的随机性。

假设我们已经观测到状态 $s_{t}$, 而且做完决策, 选中动作 $a_{t}$ 。那么 $U_{t}$ 中的随机性来自 于 $t+1$ 时刻起的所有的状态和动作:

$$
S_{t+1}, A_{t+1}, S_{t+2}, A_{t+2}, \cdots, S_{n}, A_{n} .
$$

对 $U_{t}$ 关于变量 $S_{t+1}, A_{t+1}, \cdots, S_{n}, A_{n}$ 求条件期望，得到

$$
Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}_{S_{t+1}, A_{t+1}, \cdots, S_{n}, A_{n}}\left[U_{t} \mid S_{t}=s_{t}, A_{t}=a_{t}\right] .
$$

期望中的 $S_{t}=s_{t}$ 和 $A_{t}=a_{t}$ 是条件, 意思是已经观测到 $S_{t}$ 与 $A_{t}$ 的值。条件期望的结果 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 被称作动作价值函数 (action-value function)。 ${ }^{3}$

动作价值函数 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 依赖于 $s_{t}$ 与 $a_{t}$, 而不依赖于 $t+1$ 时刻及其之后的状态和 动作, 因为随机变量 $S_{t+1}, A_{t+1}, \cdots, S_{n}, A_{n}$ 都被期望消除了。由于动作 $A_{t+1}, \cdots, A_{n}$ 的 概率质量函数都是 $\pi$, 公式 (3.1) 中的期望依赖于 $\pi$; 用不同的 $\pi$, 求期望得出的结果就 会不同。因此 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 依赖于 $\pi$, 这就是为什么动作价值函数有下标 $\pi$ 。

综上所述, $t$ 时刻的动作价值函数 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 依赖于以下三个因素：

- 第一, 当前状态 $s_{t}$ 。当前状态越好, 那么价值 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 越大, 也就是说回报的期 望值越大。在超级玛丽的游戏中, 如果玛丽奥当前已经接近终点, 那么 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 就非常大。

- 第二, 当前动作 $a_{t}$ 。智能体执行的动作越好, 那么价值 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 越大。举个例子, 如果玛丽奥做正常的动作, 那么 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 就比较正常。如果玛丽奥的动作 $a_{t}$ 是跳 下悬崖, 那么 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 就会非常小。

- 第三, 策略函数 $\pi$ 。策略决定末来的动作 $A_{t+1}, A_{t+2}, \cdots, A_{n}$ 的好坏: 策略越好, 那 么 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 就越大。举个例子, 顶级玩家相当于好的策略 $\pi$, 新手相当于差的策 略。让顶级玩家操作游戏, 回报的期望非常高。换新手操作游戏, 从相同的状态出 发，回报的期望会很低。

3更准确地说, 应该叫“动作状态价值函数”, 但是大家习惯性地称之为“动作价值函数”。

### 2 最优动作价值函数

怎么样才能排除掉策略 $\pi$ 的影响, 只评价当前状态和动作的好坏呢? 解决方案就是 最优动作价值函数 (optimal action-value function)：

$$
Q_{\star}\left(s_{t}, a_{t}\right)=\max _{\pi} Q_{\pi}\left(s_{t}, a_{t}\right), \quad \forall s_{t} \in \mathcal{S}, \quad a_{t} \in \mathcal{A} .
$$

意思就是有很多种策略函数 $\pi$ 可供选择, 而我们选择最好的策略函数：

$$
\pi^{\star}=\underset{\pi}{\operatorname{argmax}} Q_{\pi}\left(s_{t}, a_{t}\right), \quad \forall s_{t} \in \mathcal{S}, \quad a_{t} \in \mathcal{A} .
$$

最优动作价值函数 $Q_{\star}\left(s_{t}, a_{t}\right)$ 只依赖于 $s_{t}$ 和 $a_{t}$, 而与策略 $\pi$ 无关。

最优动作价值函数 $Q_{\star}$ 非常有用, 它就像是一个先知, 能指引智能体做出正确决策。 比如玩超级玛丽, 给定当前状态 $s_{t}$, 智能体该执行动作空间 $\mathcal{A}=\{$ 左, 右, 上 $\}$ 中的哪个动 作呢? 假设我们已知 $Q_{\star}$ 函数, 那么我们就让 $Q_{\star}$ 给三个动作打分, 比如：

$$
Q_{\star}\left(s_{t}, \text { 左 }\right)=130, \quad Q_{\star}\left(s_{t}, \text { 右 }\right)=-50, \quad Q_{\star}\left(s_{t}, \text { 上 }\right)=296 \text {. }
$$

这三个值是什么意思呢? $Q_{\star}\left(s_{t}\right.$, 左 $)=130$ 的意思是：如果现在智能体选择向左走, 那 么不管以后智能体用什么策略函数 $\pi$, 回报 $U_{t}$ 的期望最多不会超过 130 。同理, 如果现 在向右走, 则回报的期望最多不超过 $-50$ 。如果现在向上跳, 则回报的期望最多不超过 296。智能体应该执行哪个动作呢? 毫无疑问, 智能体当然应该向上跳, 这样才能有希望 获得尽量高的回报。

# 3 状态价值函数

假设 $\mathrm{AI}$ 用策略函数 $\pi$ 下围棋。 $\mathrm{AI}$ 想知道当前状态 $s_{t}$ （即棋盘上的格局）是否对自 己有利, 以及自己和对手的胜算各有多大。该用什么来量化双方的胜算呢? 答案是状态 价值函数 (state-value function):

$$
\begin{aligned}
V_{\pi}\left(s_{t}\right) & =\mathbb{E}_{A_{t} \sim \pi\left(\cdot \mid s_{t}\right)}\left[Q_{\pi}\left(s_{t}, A_{t}\right)\right] \\
& =\sum_{a \in \mathcal{A}} \pi\left(a \mid s_{t}\right) \cdot Q_{\pi}\left(s_{t}, a\right) .
\end{aligned}
$$

公式里把动作 $A_{t}$ 作为随机变量, 然后关于 $A_{t}$ 求期望, 把 $A_{t}$ 消掉。得到的状态价值函 数 $V_{\pi}\left(s_{t}\right)$ 只依赖于策略 $\pi$ 与当前状态 $s_{t}$, 不依赖于动作。状态价值函数 $V_{\pi}\left(s_{t}\right)$ 也是回报 $U_{t}$ 的期望：

$$
V_{\pi}\left(s_{t}\right)=\mathbb{E}_{A_{t}, S_{t+1}, A_{t+1}, \cdots, S_{n}, A_{n}}\left[U_{t} \mid S_{t}=s_{t}\right] .
$$

期望消掉了 $U_{t}$ 依赖的随机变量 $A_{t}, S_{t+1}, A_{t+1}, \cdots, S_{n}, A_{n}$ 。状态价值越大, 就意味着回 报的期望越大。用状态价值可以衡量策略 $\pi$ 与状态 $s_{t}$ 的好坏。

## $3.6$ 实验环境

如果你设计出一种新的强化学习方法, 你应该将其与已有的标准方法做比较, 看新 的方法是否有优势。比较和评价强化学习算法最常用的是 OpenAI Gym, 它相当于计算 机视觉中的 ImageNet 数据集。Gym 有几大类控制问题, 比如经典控制问题、Atari 游戏、 机器人。

Gym 中第一类是经典控制问题, 都是小规模的简单问题, 比如 Cart Pole 和 Pendulum, 见图 3.7。Cart Pole 要求给小车向左或向右的力, 移动小车, 让上面的杆子能坚起来。 Pendulum 要求给钟摆一个力, 让钟摆恰好能坚起来。Cart Pole 和 Pendulum 都是典型的 无限期 MDP，即不存在终止状态。
![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-051.jpg?height=138&width=944&top_left_y=950&top_left_x=411)

Cart Pole

Pendulum

图 3.7: 经典控制问题。

第二类问题是 Atari 游戏, 就是八、九十年代小霸王游戏机上拿手柄玩的那种游戏, 见图 3.8。Pong 中的智能体是乒乓球拍, 球拍可以上下运动, 目标是接住对手的球, 尽 量让对手接不住球。Space Invader 中的智能体是小飞机, 可以左右移动, 可以发射炮弹。 Breakout 中的智能体是下面的球拍, 可以左右移动, 目标是接住球, 并且把上面的砖块 都打掉。Atari 游戏大多是有限期 MDP, 即存在一个终止状态, 一旦进入该状态, 则游戏 会终止。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-051.jpg?height=354&width=292&top_left_y=1810&top_left_x=428)

Pong

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-051.jpg?height=380&width=297&top_left_y=1809&top_left_x=822)

space Invader

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-051.jpg?height=369&width=277&top_left_y=1820&top_left_x=1226)

Breakout

图 3.8: Atari 游戏。

第三类问题是机器人连续的控制问题, 比如控制蚂蚁、人、猎豹等机器人走路, 见 图 3.9。这个模拟器叫做 MuJoCo, 它可以模拟重力等物理量。机器人是智能体, AI 需要 控制这些机器人站立和走路。MuJoCo 是付费软件, 但是可以申请免费试用 license。

想要使用 Gym, 应该先按照官方文档安装 https://gym.openai.com/。安装之后就

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-052.jpg?height=343&width=280&top_left_y=268&top_left_x=548)

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-052.jpg?height=271&width=277&top_left_y=273&top_left_x=935)

Humanoid

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-052.jpg?height=271&width=265&top_left_y=276&top_left_x=1341)

Half cheetah

图 3.9: 机器人连续的控制问题, 用到 MuJoCo 物理模拟器。

可以在 Python 里面调用 Gym 库中的函数了。下面的程序以 Cart Pole 这个控制任务为例, 说明怎么样使用 Gym 标准库。通过阅读这段程序, 读者可以更好理解智能体与环境的交 互。

import gym

生成环境。此处的环境是CartPole游戏程序。

env $=$ gym.make ('Cartpole-v0') 重置环境, 让小车回到起点。并输出初始状态。

for $t$ in range $(100)$ : 弹出窗口,把游戏中发生的显示到屏幕上。 env.render () 方便起见, 此处均匀抽样生成一 个动作。在实际应用中, 应当依 据状态, 用策略函数生成动作。

action $=$ env $\cdot$ action_space. sample () 智能体真正执行动作。

state, reward, done, info = env.step(action) 然后环境更新状态, | done等于 1 意味着斿戏结束; done等于 0 意味着游戏继续。 并反馈一个奖励。

if done

print ('Finished')

break

env.close()

## 第 3 章 知识点

- 请读者理解并记忆强化学习中的基本术语：状态（state）、状态空间 (state space)、 动作 (action)、动作空间 (action space)、智能体（agent)、环境 (environment)、策 略（policy)、奖励 (reward)、状态转移 (state transition)。

- 马尔可夫决策过程 (MDP) 通常指的是四元组 $(\mathcal{S}, \mathcal{A}, p, r)$, 其中 $\mathcal{S}$ 是状态空间, $\mathcal{A}$ 是 动作空间, $p$ 是状态转移函数, $r$ 是奖励函数。有时 MDP 指的是五元组 $(\mathcal{S}, \mathcal{A}, p, r, \gamma)$, 其中 $\gamma$ 是折扣率。

- 强化学习中的随机性来自于状态和动作。状态的随机性来源于状态转移, 动作的随 机性来源于策略。奖励依赖于状态和动作, 因此奖励也具有随机性。请读者理解随 机性的两个来源, 这对理解强化学习至关重要。

- 回报（或折扣回报）是末来所有奖励的加和（或加权和）。回报有随机性, 它的随 机性来自于末来的状态和动作。请读者注意奖励与回报的区别。强化学习的目标是 最大化回报，而不是最大化奖励。

- 价值函数是回报的期望, 可以反映出当前状态、动作的好坏程度。之后所有章节都 会用到价值函数, 所以请读者务必理解和记忆价值函数的定义。请读者注意区分动 作价值函数 $Q_{\pi}(s, a)$ 、最优动作价值函数 $Q_{\star}(s, a)$ 、状态价值函数 $V_{\pi}(s)$ 。

- 强化学习分为基于模型的方法、无模型方法两大类。其中无模型方法又分为价值学 习、策略学习两类。本书第二部分、第三部分会详细讲解价值学习和策略学习; 第 18 章用 AlphaGo 的例子讲解基于模型的方法。

- OpenAI Gym 是强化学习中最常用的实验环境。如果你想比较几种强化学习算法的 优劣, 你需要在 Gym 的多个环境中进行实验对比。

## 第 3 章 习题

1. 设 $\mathcal{A}=\{$ 上, 下, 左, 右 $\}$ 为动作空间, $s_{t}$ 为当前状态, $\pi$ 为策略函数。策略函数 输出:

$$
\begin{aligned}
& \pi\left(\text { 上 } \mid s_{t}\right)=0.2, \\
& \pi\left(\text { 下 } \mid s_{t}\right)=0.05, \\
& \pi\left(\text { 左 } \mid s_{t}\right)=0.7, \\
& \pi\left(\text { 右 } \mid s_{t}\right)=0.15 .
\end{aligned}
$$

请问哪个动作会成为 $a_{t}$ ?
A. 下。
B. 左。
C. 四种动作都有可能。

2. 设随机变量 $U_{t}$ 为 $t$ 时刻的回报。请问 $U_{t}$ 依赖于哪些变量?
A. $t$ 时刻的状态 $s_{t}$ 。
B. $t$ 时刻的动作 $a_{t}$ 。
C. $s_{t}$ 和 $a_{t}$ 。
D. $s_{t}, s_{t+1}, s_{t+2}, \cdots$ 和 $a_{t}, a_{t+1}, a_{t+2}, \cdots$ 。

3. 动作价值函数是 的期望。
A. 奖励。
B. 回报。
C. 状态。
C. 动作。

4. 最优动作价值函数 $Q_{\star}$ 依赖于
A. 当前的状态 $s_{t}$ 。
B. 当前的动作 $a_{t}$ 。
C. 末来所有的状态 $s_{t+1}, s_{t+2}, \cdots$ 。
D. 末来所有的动作 $a_{t+1}, a_{t+2}, \cdots$ 。
E. $A$ 和 $B$ 都对。
F. A、B、C、D 都对。
