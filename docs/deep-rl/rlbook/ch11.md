
## 第 11 章 对状态的不完全观测

在很多应用中, 智能体只能部分观测到当前环境的状态, 这会给决策造成困难。本 章内容分三节, 分别介绍不完全观测问题、循环神经网络（RNN）、用 RNN 策略网络解 决不完全观测问题。

## 1 不完全观测问题

之前章节中的 DQN $Q(s, a ; \boldsymbol{w})$, 策略网络 $\pi(a \mid s ; \boldsymbol{\theta}) 、 \boldsymbol{\mu}(s ; \boldsymbol{\theta})$, 价值网络 $q(s, a ; \boldsymbol{w})$ 、 $v(s ; \boldsymbol{w})$ 都需要把当前状态 $s$ 作为输入。之前我们一直假设可以完全观测到状态 $s$; 在围 棋、象棋、五子棋等简单的游戏中, 棋盘上当前的格局就是完整的状态, 符合完全观测 的假设。但是在很多实际应用中, 完全观测假设往往不符合实际。比如在星际争霸、英 雄联盟等电子游戏中, 屏幕上当前的画面并不能完整反映出游戏的状态, 因为观测只是 地图的一小部分; 甚至最近的 100 帧也无法反映出游戏真实的状态。

把 $t$ 时刻的状态记作 $s_{t}$, 把观测记作 $o_{t}$ 。观测 $o_{t}$ 可以是当前游戏屏幕上的画面, 也 可以是最近 100 帧画面。我们无法用 $\pi\left(a_{t} \mid s_{t} ; \boldsymbol{\theta}\right)$ 做决策, 因为我们不知道 $s_{t}$ 。最简单的 解决办法就是用当前观测 $o_{t}$ 代替状态 $s_{t}$, 用 $\pi\left(a_{t} \mid o_{t} ; \boldsymbol{\theta}\right)$ 做决策。同理, 对于 DQN 和价 值网络, 也用 $o_{t}$ 代替 $s_{t}$ 。虽然这种简单的方法可行, 但是效果恐怕不好。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-177.jpg?height=448&width=439&top_left_y=1512&top_left_x=246)

(a) 对状态的完全观测

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-177.jpg?height=174&width=163&top_left_y=1558&top_left_x=838)

不完全观测

(b) 对状态的不完全观测

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-177.jpg?height=357&width=456&top_left_y=1512&top_left_x=1234)

(c) 记忆过去的观测

图 11.1: 在迷宫问题中, 智能体可能知道迷宫的整体格局, 也可能仅仅知道自己附近的格局。

图 $11.1$ 的例子是让智能体走迷宫。图 11.1(a) 中智能体可以完整观测到迷宫 $s$; 这种 问题最容易解决。图 11.1(b) 中智能体只能观测到自身附近一小块区域 $o_{t}$, 这属于不完全 观测问题, 这种问题较难解决。如果仅仅靠当前观测 $o_{t}$ 做决策, 智能体做出的决策是非 常盲目的, 很难走出迷宫。如图 11.1(c) 所示, 一种更合理的办法是让智能体记住过去的 观测, 这样的话对状态的观测会越来越完整, 做出的决策会更合理。

对于不完全观测的强化学习问题, 应当记忆过去的观测, 用所有已知的信息做决策。 这正是人类解决不完全观测问题的方式。对于星际争霸、扑克牌、麻将等不完全观测的 游戏, 人类玩家也需要记忆; 人类玩家的决策不止依赖于当前时刻的观测 $o_{t}$, 而且依赖 于过去所有的观测 $o_{1}, \cdots, o_{t-1}$ 。把从初始到 $t$ 时刻为止的所有观测记作:

$$
\boldsymbol{o}_{1: t}=\left[o_{1}, o_{2}, \cdots, o_{t}\right]
$$

可以用 $\boldsymbol{o}_{1: t}$ 代替状态 $s$, 作为策略网络的输入, 那么策略网络就记作:

$$
\pi\left(a_{t} \mid \boldsymbol{o}_{1: t} ; \boldsymbol{\theta}\right) .
$$

该如何实现这样一个策略网络呢? 请注意, $\boldsymbol{o}_{1: t}$ 的大小是变化的。如果 $o_{1}, \cdots, o_{t}$ 都是 $d \times 1$ 的向量, 那么 $\boldsymbol{o}_{1: t}$ 是 $d \times t$ 的矩阵或 $d t \times 1$ 的向量, 它的大小随 $t$ 增长。卷积层和 全连接层都要求输入大小固定, 因此不能简单地用卷积层和全连接层实现策略网络。一 种可行的办法是将卷积层、全连接层与循环层结合, 这样就能处理不固定长度的输入。

## 2 循环神经网络 (RNN)

循环神经网络 (recurrent neural network), 缩写 RNN, 是一类神经网络的总称, 由 循环层（recurrent layers）和其他种类的层组成。循环层的作用是把一个序列（比如时间 序列、文本、语音）映射到一个特征向量。设向量 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{n}$ 是一个序列。对于所有的 $t=1, \cdots, n$, 循环层把 $\left[\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{t}\right]$ 映射到特征向量 $\boldsymbol{h}_{t}$ 。依次把 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{n}$ 输入循环层, 会得到：

$$
\begin{array}{lll}
\left(\boldsymbol{x}_{1}\right) & \Longrightarrow & \boldsymbol{h}_{1}, \\
\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right) & \Longrightarrow & \boldsymbol{h}_{2}, \\
\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \boldsymbol{x}_{3}\right) & \Longrightarrow & \boldsymbol{h}_{3}, \\
& \vdots & \\
\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \boldsymbol{x}_{3}, \cdots, \boldsymbol{x}_{n-1}\right) & \Longrightarrow & \boldsymbol{h}_{n-1}, \\
\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \boldsymbol{x}_{3}, \cdots, \boldsymbol{x}_{n-1}, \boldsymbol{x}_{n}\right) & \Longrightarrow & \boldsymbol{h}_{n}
\end{array}
$$

$\mathrm{RNN}$ 的好处在于不论输入序列的长度 $t$ 是多少, 从序列中提取出的特征向量 $\boldsymbol{h}_{t}$ 的大小 是固定的。请特别注意, $\boldsymbol{h}_{t}$ 并非只依赖于 $\boldsymbol{x}_{t}$ 这一个向量, 而是依赖于 $\left[\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{t}\right]$; 理 想情况下, $\boldsymbol{h}_{t}$ 记住了 $\left[\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{t}\right]$ 中的主要信息。例如 $\boldsymbol{h}_{3}$ 是对 $\left[\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \boldsymbol{x}_{3}\right]$ 的概要, 而非 是对 $\boldsymbol{x}_{3}$ 这一个向量的概要。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-179.jpg?height=797&width=1410&top_left_y=1383&top_left_x=263)

图 11.2: 输入是序列 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{t}$ 。向量 $\boldsymbol{h}_{t}$ 是从所有 $t$ 个输入中提取的特征, 可以把它看做输入序 列的一个概要。把 $\boldsymbol{h}_{t}$ 输入全连接层 (带 sigmoid 激活函数), 得到分类结果 $\widehat{p}_{\text {。 }}$

举个例子, 用户给商品写的评论由 $n$ 个字组成（不同的评论有不同的 $n$ ), 我们 想要判断评论是正面的还是负面的, 这是个二元分类问题。用词嵌入 (word embedding) 把每个字映射到一个向量, 得到 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{n}$, 把它们依次输入循环层。循环层依次输出 $\boldsymbol{h}_{1}, \cdots, \boldsymbol{h}_{n}$ 。我们只需要用 $\boldsymbol{h}_{n}$, 因为它是从全部输入 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{n}$ 中提取的特征; 可以忽 略掉 $\boldsymbol{h}_{1}, \cdots, \boldsymbol{h}_{n-1}$ 。最后, 二元分类器把 $\boldsymbol{h}_{n}$ 作为输入, 输出一个介于 0 到 1 之间的数 $\widehat{p}$, 0 代表负面, 1 代表正面。图 $11.2$ 描述了神经网络的结构。

循环层的种类有很多, 常见的包括简单循环层、LSTM、GRU。本书只介绍简单循环 层。LSTM、GRU 是对简单循环层的改进, 结构更复杂, 效果更好。但是它们的原理与 简单循环层基本相同。读者只需要理解简单循环层就足够了。用 TensorFlow、PyTorch、 Keras 编程实现的话, 几种循环层的使用方法完全相同 (唯一区别是函数名)。

简单循环层的输入记作 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{n} \in \mathbb{R}^{d_{\mathrm{in}}}$, 输出记作 $\boldsymbol{h}_{1}, \cdots, \boldsymbol{h}_{n} \in \mathbb{R}^{d_{\mathrm{out}}}$ 。循环层的

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-180.jpg?height=68&width=1465&top_left_y=620&top_left_x=358)
$t=1, \cdots, n$, 依次计算

$$
\boldsymbol{h}_{t}=\tanh \left(\boldsymbol{W}\left[\boldsymbol{h}_{t-1} ; \boldsymbol{x}_{t}\right]+\boldsymbol{b}\right) .
$$

图 $11.3$ 解释上面的公式。注意, 不论输入序列长度 $n$ 是多少, 简单循环层的参数只有唯 一的 $\boldsymbol{W}$ 和 $\boldsymbol{b}$ 。公式中的 $\tanh$ 是双曲正切函数, 见图 11.4。tanh 是标量函数; 如果输入 是向量, 那么 $\tanh$ 应用到向量的每一个元素上。对于 $d \times 1$ 的向量 $\boldsymbol{z}$, 有

$$
\tanh (\boldsymbol{z})=\left[\tanh \left(z_{1}\right), \tanh \left(z_{2}\right), \cdots, \tanh \left(z_{d}\right)\right]^{T} .
$$

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-180.jpg?height=272&width=831&top_left_y=1166&top_left_x=384)

图 11.3: 简单循环层。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-180.jpg?height=274&width=503&top_left_y=1168&top_left_x=1302)

图 11.4: 双曲正切函数。

### RNN 作为策略网络

在不完全观测的设定下, 我们希望策略网络能利用所有已经收集的观测 $\boldsymbol{o}_{1: t}=\left[o_{1}\right.$, $\left.\cdots, o_{t}\right]$ 做决策。定义策略网络为

$$
\boldsymbol{f}_{t}=\pi\left(a_{t} \mid \boldsymbol{o}_{1: t} ; \boldsymbol{\theta}\right),
$$

结构如图 $11.5$ 所示。在第 $t$ 时刻, 观测到 $o_{t}$, 用卷积网络提取特征, 得到向量 $\boldsymbol{x}_{t}$ 。循 环层把 $\boldsymbol{x}_{t}$ 作为输入, 然后输出 $\boldsymbol{h}_{t}$ 。 $\boldsymbol{h}_{t}$ 是从 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{t}$ 中提取出的特征, 是对所有观测 $\boldsymbol{o}_{1: t}=\left[o_{1}, \cdots, o_{t}\right]$ 的一个概要。全连接网络（输出层激活函数是 softmax）把 $\boldsymbol{h}_{t}$ 作为输 入, 然后输出向量 $\boldsymbol{f}_{t}$, 作为 $t$ 时刻决策的依据。 $\boldsymbol{f}_{t}$ 的维度是动作空间的大小 $|\mathcal{A}|$, 它的 每个元素对应一个动作, 表示选择该动作的概率。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-181.jpg?height=1082&width=1425&top_left_y=921&top_left_x=247)

图 11.5: 基于 RNN 的策略网络。图中所有的全连接网络都有相同的参数; 所有的循环层都有相 同的参数; 所有的卷积层都有相同的参数。

对于不完全观测问题, 我们可以类似地搭建 $\mathrm{DQN}$ 和价值网络。DQN 可以定义为:

$$
Q\left(\boldsymbol{o}_{1: t}, a_{t} ; \boldsymbol{w}\right) .
$$

价值网络可以定义为：

$$
q\left(\boldsymbol{o}_{1: t}, a_{t} ; \boldsymbol{w}\right) \text { 或 } v\left(\boldsymbol{o}_{1: t} ; \boldsymbol{w}\right) .
$$

这些神经网络与图 $11.5$ 中策略网络的区别仅在于全连接网络的结构而已; 它们使用的卷 积网络、循环层与图 $11.5$ 相同。

## 第 11 章 知识点

- 在很多强化学习的应用中, 智能体无法完整观测到环境当前的状态 $s_{t}$ 。我们把观测 记作 $o_{t}$, 以区别完整的状态。仅仅基于当前观测 $o_{t}$ 做决策, 效果会不理想。

-一种合理的解决方案是记忆过去的状态, 基于历史上全部的观测 $o_{1}, \cdots, o_{t}$ 做决策。 常用循环神经网络（RNN）做为策略函数, 做出的决策依赖于历史上全部的观测。

## 第 11 章 习题

1. (多选) 图 $11.6$ 中是一个循环神经网络。请问输出值 $p_{3}$ 依赖于下面哪些值? 换句话 说, 改变下面哪些值, 会影响 $p_{3}$ ?
A. $\boldsymbol{h}_{0}$ 。
B. $\boldsymbol{x}_{1}$ 。
C. $p_{2}$ 。
D. $h_{3}$ 。
E. $\boldsymbol{x}_{4}$ 。
F. $h_{5}$ 。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-183.jpg?height=794&width=1408&top_left_y=882&top_left_x=267)

图 11.6: 输入是序列 $\boldsymbol{x}_{1}, \cdots, \boldsymbol{x}_{n}$ 。图中的全连接层都共享参数。

2. 图 $11.6$ 中是一个循环神经网络。用这个公式更新 $\boldsymbol{h}$ :

$$
\boldsymbol{h}_{t}=\tanh \left(\boldsymbol{W}\left[\boldsymbol{h}_{t-1} ; \boldsymbol{x}_{t}\right]+\boldsymbol{b}\right) .
$$

设 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{h}_{i}$ 分别是 $d_{1} \times 1$ 和 $d_{2} \times 1$ 的向量。请问循环层中有多少需要学习的参数?

## 第 11 章相关文献

RNN 是一类很重要的神经网络。学术界认为最早的 RNN 是 Hopfield network ${ }^{[52]}$, 尽 管它跟我们今天用的 RNN 很不一样。现在最常用的 RNN 包括 LSTM ${ }^{[51]}$ 和 GRU ${ }^{[28]}$ 。 注意力机制 (attention) 由 2015 年的论文 ${ }^{[6]}$ 提出, 将注意力机制与 $R N N$ 结合, 可以大 幅提升 RNN 在机器翻译任务上的表现。注意力机制显然可以用于本章介绍的 RNN 策略 网络, 但是这样会大幅增加计算量。

2015 年的论文 ${ }^{[46]}$ 首先将 RNN 应用于深度强化学习, 把 RNN 与 DQN 相结合, 把 得到的方法叫做 DRQN。在此之后, RNN 成为解决不完全观测问题的一种标准技巧, 比 如论文 ${ }^{[74,38,86]}$ 。
