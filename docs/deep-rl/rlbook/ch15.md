

## 第 15 章 合作关系设定下的多智能体强化学习

本章只考虑最简单的设定一一完全合作关系一一并在这种设定下研究多智能体强化 学习 (MARL)。第 $15.1$ 节定义“完全合作关系”下的策略学习。第 $15.2$ 节介绍“完全合作关 系”下的多智能体 A2C 方法, 本书称之为 MAC-A2C。第 $15.3$ 节介绍 MARL 的三种常见 架构一一完全去中心化、完全中心化、中心化训练 + 去中心化决策一一并在三种框架下 实现 MAC-A2C。

本章与上一章对状态的定义有所区别。在多智能体系统中, 一个智能体末必能观测 到全局状态 $S$ 。设第 $i$ 号智能体有一个局部观测, 记作 $O^{i}$, 它是 $S$ 的一部分。不妨假设 所有的局部观测的总和构成全局状态：

$$
S=\left[O^{1}, O^{2}, \cdots, O^{m}\right],
$$

MARL 的文献大多采用这种假设。本章中采用的符号如图 $15.1$ 所示。

知能 体:

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-231.jpg?height=140&width=605&top_left_y=1181&top_left_x=563)

状态：

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-231.jpg?height=160&width=591&top_left_y=1359&top_left_x=573)

观测：

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-231.jpg?height=165&width=577&top_left_y=1528&top_left_x=585)

奖励：

$$
R^{1}=R^{2}=\cdots=R^{m}=R
$$

回报：

$U^{1}=U^{2}=\cdots=U^{m}=U$

等式仅在

动作价值：

$$
Q_{\pi}^{1}=Q_{\pi}^{2}=\cdots=Q_{\pi}^{m}=Q_{\pi}
$$
“完全合作”

状态价值：

$V_{\pi}^{1}=V_{\pi}^{2}=\cdots=V_{\pi}^{m}=V_{\pi}$

的设定下

目标函数:

$$
J^{1}=J^{2}=\cdots=J^{m}=J
$$

成立

图 15.1: 多智能体强化学习 (MARL) 在“完全合作关系”设定下的符号。

## 1 合作关系设定下的策略学习

MARL 中的完全合作关系 (fully-cooperative) 意思是所有智能体的利益是一致的, 它 们具有相同的奖励:

$$
R^{1}=R^{2}=\cdots=R^{m} \triangleq R
$$

因此, 所有的智能体都有相同的回报：

$$
U^{1}=U^{2}=\cdots=U^{m} \triangleq U .
$$

因为价值函数是回报的期望, 所以所有的智能体都有相同的价值函数。省略上标 $i$, 把动 作价值函数记作 $Q_{\pi}(S, A)$, 把状态价值函数记作 $V_{\pi}(S)$ 。

注意, 价值函数 $Q_{\pi}$ 和 $V_{\pi}$ 依赖于所有智能体的策略:

$$
\pi\left(A^{1} \mid S ; \boldsymbol{\theta}^{1}\right), \quad \pi\left(A^{2} \mid S ; \boldsymbol{\theta}^{2}\right), \quad \cdots, \quad \pi\left(A^{m} \mid S ; \boldsymbol{\theta}^{m}\right) .
$$

举个例子, 在某个竞技电游中, 玩家组队打任务; 每完成一个任务, 团队成员（即智能 体）获得相同的奖励。所以大家的 $R, U, Q_{\pi}, V_{\pi}$ 全都是一样的。回报的期望一一即价值 函数 $Q_{\pi}$ 与 $V_{\pi}$ 一显然与所有成员的策略相关：只要有一个猪队友（即策略差）拖后腿, 就有可能导致任务失败。通常来说, 团队成员有分工合作, 因此每个成员的策略是不同 的, 即 $\boldsymbol{\theta}^{i} \neq \boldsymbol{\theta}^{j}$ 。

如果做策略学习 (即学习策略网络参数 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ ), 那么所有智能体都有一个共同 目标函数：

$$
J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S}\left[V_{\pi}(S)\right] .
$$

所有智能体的目的是一致的, 即改进自己的策略网络参数 $\boldsymbol{\theta}^{i}$, 使得目标函数 $J$ 增大。那 么策略学习可以写作这样的优化问题:

$$
\max _{\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right) .
$$

注意, 只有 “完全合作关系”这种设定下, 所有智能体才会有共同的目标函数, 其原因在 于 $R^{1}=\cdots=R^{m}$ 。对于其它设定——“竞争关系”、混合关系”“利己主义”一智能体的 目标函数是各不相同的 (见下一章)。

合作关系设定下的策略学习的原理很简单, 即让智能体各自做策略梯度上升, 使得 目标函数 $J$ 增长。

$$
\begin{array}{ll}
\text { 第 } 1 \text { 号智能体执行: } & \boldsymbol{\theta}^{1} \leftarrow \boldsymbol{\theta}^{1}+\alpha^{1} \cdot \nabla_{\boldsymbol{\theta}^{1}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right), \\
\text { 第 } 2 \text { 号智能体执行 : } & \boldsymbol{\theta}^{2} \leftarrow \boldsymbol{\theta}^{2}+\alpha^{2} \cdot \nabla_{\boldsymbol{\theta}^{2}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right),
\end{array}
$$

第 $m$ 号智能体执行 : $\quad \boldsymbol{\theta}^{m} \leftarrow \boldsymbol{\theta}^{m}+\alpha^{m} \cdot \nabla_{\boldsymbol{\theta}^{m}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)$.

公式中的 $\alpha^{1}, \alpha^{2}, \cdots, \alpha^{m}$ 是学习率。判断策略学习收敛的标准是目标函数 $J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)$ 不再增长。在实践中, 当平均回报不再增长, 即可终止算法。由于无法直接计算策略梯 度 $\nabla_{\boldsymbol{\theta}^{i}} J$, 我们需要对其做近似。下一节用价值网络近似策略梯度, 进而推导出一种实际 可行的策略梯度方法。

## $15.2$ 合作设定下的多智能体 $\mathbf{A 2 C}$

第 8 章介绍过 advantage actor-critic (A2C) 方法。本节介绍“完全合作关系”设定下的 多智能体 $\mathrm{A} 2 \mathrm{C}$ 方法 (multi-agent cooperative $\mathrm{A} 2 \mathrm{C}$ ), 缩写 MAC-A2C。注意, 本节介绍的 方法仅适用于 “完全合作关系”, 也就是要求所有智能体有相同的奖励： $R^{1}=\cdots=R^{m}$ 。 第 $15.2 .1$ 小节定义策略网络和价值网络。第 15.2.2 小节描述 MAC-A2C 训练和决策。第 15.2.3 小节讨论 MAC-A2C 实现中的难点。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-233.jpg?height=414&width=1197&top_left_y=804&top_left_x=378)

图 15.2: 图中是 MAC-A2C 中的价值网络 $v(s ; \boldsymbol{w})$ 。所有智能体共用这个价值网络。输入是所有智 能体的观测: $s=\left[o^{1}, \cdots, o^{m}\right]$ 。输出是价值网络给 $s$ 的评分。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-233.jpg?height=394&width=1425&top_left_y=1405&top_left_x=247)

图 15.3: 图中是 MAC-A2C 中第 $i$ 号智能体的策略网络 $\pi\left(\cdot \mid s ; \boldsymbol{\theta}^{i}\right)$ 。所有智能体的策略网络结构都 一样, 但是参数 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 可能不一样。输入是所有智能体的观测: $s=\left[o^{1}, \cdots, o^{m}\right]$ 。输出是在 离散动作空间 $\mathcal{A}^{i}$ 上的概率分布。

### 1 策略网络和价值网络

本章只考虑离散控制问题, 即动作空间 $\mathcal{A}^{1}, \cdots, \mathcal{A}^{m}$ 都是离散集合。MAC-A2C 使用 两类神经网络：价值网络 $v$ 与策略网络 $\pi$; 见图 15.2、图 15.3。

所有智能体共用一个价值网络, 记作 $v(s ; \boldsymbol{w})$, 它是对状态价值函数 $V_{\pi}(s)$ 的近似。 它把所有观测 $s=\left[o^{1}, \cdots, o^{m}\right]$ 作为输入, 并输出一个实数, 作为对状态 $s$ 的评分。

每个智能体有自己的策略网络。把第 $i$ 号策略网络记作 $\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$ 。它的输入是所 有智能体的观测 $s=\left[o^{1}, \cdots, o^{m}\right]$ 。它的输出是一个向量, 表示动作空间 $\mathcal{A}^{i}$ 上的概率分 布。比如, 第 $i$ 号智能体的动作空间是 $\mathcal{A}^{i}=\{$ 左, 右, 上 $\}$; 策略网络的输出是

$$
\pi\left(\text { 左 } \mid s ; \boldsymbol{\theta}^{i}\right)=0.2, \quad \pi\left(\text { 右 } \mid s ; \boldsymbol{\theta}^{i}\right)=0.1, \quad \pi\left(\text { 上 } s ; \boldsymbol{\theta}^{i}\right)=0.7 \text {. }
$$

第 $i$ 号智能体依据该概率分布抽样得到动作 $a^{i}$ 。

MAC-A2C 属于 actor-critic 方法: 策略网络 $\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$ 相当于第 $i$ 个运动员, 负责做 决策; 价值网络 $v(s ; \boldsymbol{w})$ 相当于评委, 对运动员团队的整体表现予以评价, 反馈给整个团 队一个分数。

训练价值网络：我们用 TD 算法训练价值网络 $v(s ; \boldsymbol{w})$ 。观测到状态 $s_{t}, s_{t+1}$ 和奖励 $r_{t}$, 计算 TD 目标:

$$
\widehat{y}_{t}=r_{t}+\gamma \cdot v\left(s_{t+1} ; \boldsymbol{w}\right) .
$$

把 $\widehat{y}_{t}$ 视作常数, 更新 $\boldsymbol{w}$ 使得 $v\left(s_{t} ; \boldsymbol{w}\right)$ 接近 $\widehat{y}_{t}$ 。定义损失函数：

$$
L(\boldsymbol{w})=\frac{1}{2}\left[v\left(s_{t} ; \boldsymbol{w}\right)-\widehat{y}_{t}\right]^{2} .
$$

损失函数的梯度等于:

$$
\nabla_{\boldsymbol{w}} L(\boldsymbol{w})=\delta_{t} \cdot \nabla_{\boldsymbol{w}} v\left(s_{t} ; \boldsymbol{w}\right),
$$

其中 $\delta_{t}=v\left(s_{t} ; \boldsymbol{w}\right)-\widehat{y}_{t}$ 是 $\mathrm{TD}$ 误差。做一次梯度下降更新 $\boldsymbol{w}$ :

$$
\boldsymbol{w} \leftarrow \boldsymbol{w}-\alpha \cdot \delta_{t} \cdot \nabla_{\boldsymbol{w}} v\left(s_{t} ; \boldsymbol{w}\right) .
$$

这样可以减小损失函数, 也就是让 $v\left(s_{t} ; \boldsymbol{w}\right)$ 接近 $\widehat{y}_{t}$ 。上述 $\mathrm{TD}$ 算法与单智能体 $\mathrm{A} 2 \mathrm{C}$ 的 $\mathrm{TD}$ 算法完全一样。

训练策略网络 : 完全合作关系设定下的动作价值函数记作 $Q_{\pi}(s, a)$, 第 $i$ 号智能体 的策略网络为 $\pi\left(A^{i} \mid S ; \boldsymbol{\theta}^{i}\right)$ 。不难证明下面的策略梯度定理（见习题 1）：

## 定理 15.1. 合作关系 MARL 的策略梯度定理

设基线 $b$ 为不依赖于 $A=\left[A^{1}, \cdots, A^{m}\right]$ 的函数。那么有

$$
\nabla_{\boldsymbol{\theta}^{i}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S, A}\left[\left(Q_{\pi}(S, A)-b\right) \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(A^{i} \mid S ; \boldsymbol{\theta}^{i}\right)\right] .
$$

期望中的动作 $A$ 的概率质量函数为

$$
\pi\left(A \mid S ; \boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right) \triangleq \pi\left(A^{1} \mid S ; \boldsymbol{\theta}^{1}\right) \times \cdots \times \pi\left(A^{m} \mid S ; \boldsymbol{\theta}^{m}\right) .
$$

把基线设置为状态价值： $b=V_{\pi}(s)$ 。定义

$$
\boldsymbol{g}^{i}\left(s, a ; \boldsymbol{\theta}^{i}\right) \triangleq\left(Q_{\pi}(s, a)-V_{\pi}(s)\right) \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right) .
$$

定理 $15.1$ 说明 $\boldsymbol{g}^{i}\left(s, a^{i} ; \boldsymbol{\theta}^{i}\right)$ 是策略梯度的无偏估计, 即

$$
\nabla_{\boldsymbol{\theta}^{i}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S, A}\left[\boldsymbol{g}^{i}\left(S, A ; \boldsymbol{\theta}^{i}\right)\right] .
$$

因此 $\boldsymbol{g}^{i}\left(s, a ; \boldsymbol{\theta}^{i}\right)$ 可以作为策略梯度的近似。但是我们不知道公式中的 $Q_{\pi} 、 V_{\pi}$, 还需要 进一步做近似。根据第 $8.3$ 节 $\mathrm{A} 2 \mathrm{C}$ 的推导, 我们把 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 近似成 $r_{t}+\gamma \cdot v\left(s_{t+1} ; \boldsymbol{w}\right)$, 把 $V_{\pi}\left(s_{t}\right)$ 近似成 $v\left(s_{t} ; \boldsymbol{w}\right)$ 。那么近似策略梯度 $\boldsymbol{g}^{i}\left(s_{t}, a_{t} ; \boldsymbol{\theta}^{i}\right)$ 可以进一步近似成:

$$
\tilde{\boldsymbol{g}}^{i}\left(s_{t}, a_{t}^{i} ; \boldsymbol{\theta}^{i}\right) \triangleq(\underbrace{r_{t}+\gamma \cdot v\left(s_{t+1} ; \boldsymbol{w}\right)-v\left(s_{t} ; \boldsymbol{w}\right)}_{\text {对 } Q_{\pi}\left(s_{t}, a_{t}\right)-V_{\pi}\left(s_{t}\right)}) \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid s_{t} ; \boldsymbol{\theta}^{i}\right) .
$$

观测到状态 $s_{t} 、 s_{t+1}$ 、动作 $a_{t}^{i}$ 、奖励 $r_{t}$, 这样更新策略网络参数：

$$
\boldsymbol{\theta}^{i} \leftarrow \boldsymbol{\theta}^{i}+\beta \cdot \tilde{\boldsymbol{g}}^{i}\left(s_{t}, a_{t}^{i} ; \boldsymbol{\theta}^{i}\right) .
$$

根据 TD 误差 $\delta_{t}$ 的定义, 不难看出 $\tilde{\boldsymbol{g}}^{i}\left(s_{t}, a_{t}^{i} ; \boldsymbol{\theta}^{i}\right)=-\delta_{t} \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid s_{t} ; \boldsymbol{\theta}^{i}\right)$ 。因此, 上面 更新策略网络参数的公式可以写作:

$$
\boldsymbol{\theta}^{i} \leftarrow \boldsymbol{\theta}^{i}-\beta \cdot \delta_{t} \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid s_{t} ; \boldsymbol{\theta}^{i}\right) .
$$

### 2 训练和决策

训练: 实际实现的时候, 应当使用目标网络缓解自举造成的偏差。目标网络记作 $v\left(s ; \boldsymbol{w}^{-}\right)$, 它的结构与 $v$ 相同, 但是参数不同。设当前价值网络和目标网络的参数分别是 $\boldsymbol{w}_{\text {now }}$ 和 $\boldsymbol{w}_{\text {now }}^{-}$。设当前 $m$ 个策略网络的参数分别是 $\boldsymbol{\theta}_{\text {now }}^{1}, \cdots, \boldsymbol{\theta}_{\text {now }}^{m}$ 。MAC-A2C 重复下面 的步骤更新参数:

1. 观测到当前状态 $s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$, 让每一个智能体独立做随机抽样:

$$
a_{t}^{i} \sim \pi\left(\cdot \mid s_{t} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m,
$$

并执行选中的动作。

2. 从环境中观测到奖励 $r_{t}$ 与下一时刻状态 $s_{t+1}=\left[o_{t+1}^{1}, \cdots, o_{t+1}^{m}\right]$ 。

3. 让价值网络做预测： $\widehat{v}_{t}=v\left(s_{t} ; \boldsymbol{w}_{\text {now }}\right)$ 。

4. 让目标网络做预测： $\widehat{v}_{t+1}^{-}=v\left(s_{t+1} ; \boldsymbol{w}_{\text {now }}^{-}\right)$。

5. 计算 TD 目标与 TD 误差:

$$
\widehat{y}_{t}^{-}=r_{t}+\gamma \cdot \widehat{v}_{t+1}^{-}, \quad \delta_{t}=\widehat{v}_{t}-\widehat{y}_{t}^{-} .
$$

6. 更新价值网络参数：

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\alpha \cdot \delta_{t} \cdot \nabla_{\boldsymbol{w}} v\left(s_{t} ; \boldsymbol{w}_{\text {now }}\right)
$$

7. 更新目标网络参数：

$$
\boldsymbol{w}_{\text {new }}^{-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{-} .
$$

8. 更新策略网络参数：

$$
\boldsymbol{\theta}_{\text {new }}^{i} \leftarrow \boldsymbol{\theta}_{\text {now }}^{i}-\beta \cdot \delta_{t} \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid s_{t} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

MAC-A2C 属于同策略 (on-policy), 不能使用经验回放。

决策 : 在完成训练之后, 不再需要价值网络 $v(s ; \boldsymbol{w})$ 。每个智能体可以用它自己的策 略网络做决策。在时刻 $t$ 观测到全局状态 $s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$, 然后做随机抽样得到动作:

$$
a_{t}^{i} \sim \pi\left(\cdot \mid s_{t} ; \boldsymbol{\theta}^{i}\right),
$$

并执行动作。注意, 智能体并不能独立做决策, 因为一个智能体的策略网络需要知道其 他所有智能体的观测。

### 3 实现中的难点

上述 MAC-A2C 的训练和决策貌似简单, 然而实现起来却不容易。在 MARL 的常见 设定下, 第 $i$ 号智能体只知道 $o^{i}$, 而观测不到全局状态:

$$
s=\left[o^{1}, \cdots, o^{m}\right] .
$$

这会给决策和训练造成如下的困难：

- 每个智能体有自己的策略网络 $\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$, 可以依靠它做决策。但是它的决策需要 全局状态 $s$ 。

- 在训练价值网络时, 为了计算 $\mathrm{TD}$ 误差 $\delta$ 与梯度 $\nabla_{\boldsymbol{w}} v(s ; \boldsymbol{w})$, 价值网络 $v(s ; \boldsymbol{w})$ 需 要知道全局状态 $s$ 。

- 在训练策略网络时, 为了计算梯度 $\nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$, 每个策略网络都需要知道全 局状态 $s$ 。

综上所述, 如果智能体之间不交换信息, 那么智能体既无法做训练, 也无法做决策。想 要做训练和决策, 有两种可行的途径:

- 一种办法是让智能体共享观测。这需要做通信, 每个智能体把自己的 $o^{i}$ 传输给其 他智能体。这样每个智能体都有全局的状态 $s=\left[o^{1}, \cdots, o^{m}\right]$ 。

- 另一种办法是对策略网络和价值函数做近似。通常使用 $\pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right)$ 替代 $\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$ 。 甚至可以进一步用 $v\left(o^{i} ; \boldsymbol{w}^{i}\right)$ 代替 $v(s ; \boldsymbol{w})$ 。

共享观测的缺点在于通信会让训练和决策的速度变慢。而做近似的缺点在于不完全信息 造成训练不收玫、做出错误决策。我们不得不在两种办法之间做出取舍, 承受其造成的 不良影响。

下一节介绍中心化 (centralized) 与去中心化 (decentralized) 的实现方法。中心化让 智能体共享信息; 优点是训练和决策的效果好, 缺点是需要通信, 造成延时, 影响速度。 去中心化需要做近似, 避免通信; 其优点在于速度快, 而缺点则是影响训练和决策的质 量。

## 3 三种架构

本节介绍 MAC-A2C 的三种实现方法。第 $15.3 .1$ 节介绍“中心化训练 + 中心化决 策” (centralized training with centralized execution), 它是对 MAC-A2C 的忠实实现, 训练和 决策都需要通信。第 $15.3 .2$ 节介绍“去中心化训练 + 去中心化决策” (decentralized training with decentralized execution), 它对策略网络和价值网络都做近似, 以避免训练和决策的 通信。第 $15.3 .3$ 节介绍“中心化训练 + 去中心化决策” (centralized training with decentralized execution), 它只近似策略网络以避免决策的通信, 它的训练需要通信。

图 $15.4$ 对比了三种架构的策略网络和价值网络。用“完全中心化”作出的决策最好, 但是速度最慢, 在很多问题中不适用。 “中心化训练 $+$ 去中心化决策” 虽然在训练中需要通 信, 但是决策的时候不需要通信, 可以做到实时决策。 “中心化训练 + 去中心化决策”是三 种架构中最实用的。

$\begin{array}{ccccc} & \text { 价值网络 } & \text { 策略网络 } & \text { 训练 } & \text { 决策 } \\ \text { 中心化训练 + 中心化决策 } & v(s ; \mathbf{w}) & \pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right) & \text { 需要通信 } & \text { 需要通信 } \\ \text { 去中心化训练 + 去中心化决策 } & v\left(o^{i} ; \mathbf{w}^{i}\right) & \pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right) & \text { 无需通信 } & \text { 无需通信 } \\ \text { 中心化训练 + 去中心化决策 } & v(s ; \mathbf{w}) & \pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right) & \text { 需要通信 } & \text { 无需通信 }\end{array}$

图 15.4: 三种架构的对比。

### 1 中心化训练 + 中心化决策

本节用完全中心化 (fully centralized) 的方式实现 MAC-A2C, 没有做任何近似。这 种实现的缺点在于通信造成延时，使得训练和决策速度变慢。图 $15.5$ 描述了系统的架构。 最上面是中央控制器 (central controller), 里面部署了价值网络 $v(s ; \boldsymbol{w})$ 与所有 $m$ 个策略 网络

$$
\pi\left(a^{1} \mid s, \boldsymbol{\theta}^{1}\right), \quad \pi\left(a^{2} \mid s, \boldsymbol{\theta}^{2}\right), \quad \cdots, \quad \pi\left(a^{m} \mid s, \boldsymbol{\theta}^{m}\right) .
$$

训练和决策全部由中央控制器完成。智能体负责与环境交互, 执行中央控制器的决策 $a^{i}$, 并把观测到的 $o^{i}$ 汇报给中央控制器。如果智能体观测到奖励 $r^{i}$,也发给中央控制器。

中心化训练 : 在时刻 $t$ 和 $t+1$, 中央控制器收集到所有智能体的观测值

$$
s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right] \quad \text { 和 } \quad s_{t+1}=\left[o_{t+1}^{1}, \cdots, o_{t+1}^{m}\right] .
$$

在“完全合作关系”的设定下, 所有智能体有相同的奖励：

$$
r_{t}^{1}=r_{t}^{2}=\cdots=r_{t}^{m} \triangleq r_{t} .
$$

$r_{t}$ 可以是中央控制器直接从环境中观测到的, 也可能是所有智能体本地的奖励 $\tilde{r}_{t}^{i}$ 的加和:

$$
r_{t}=\tilde{r}_{t}^{1}+\tilde{r}_{t}^{2}+\cdots+\tilde{r}_{t}^{m} .
$$



## 中央控制器

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-238.jpg?height=846&width=1237&top_left_y=328&top_left_x=478)

环境

图 15.5: 中心化训练 + 中心化决策的系统架构。

决策是中央控制器上的策略网络做出的, 中央控制器因此知道所有的动作:

$$
a_{t}=\left[a_{t}^{1}, \cdots, a_{t}^{m}\right] .
$$

综上所述，中央控制器知道如下信息：

$$
s_{t}, s_{t+1}, a_{t}, r_{t}
$$

因此, 中央控制器有足够的信息按照第 15.2.2 小节中的算法训练 MAC-A2C, 更新价值网 络的参数 $\boldsymbol{w}$ 和策略网络的参数 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 。

中心化决策 : 在 $t$ 时刻, 中央控制器收集到所有智能体的观测值 $s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$, 然后用中央控制器上部署的策略网络做决策：

$$
a_{t}^{i} \sim \pi\left(\cdot \mid s_{t} ; \boldsymbol{\theta}^{i}\right), \quad \forall i=1, \cdots, m .
$$

中央控制器把决策 $a_{t}^{i}$ 传达给第 $i$ 号智能体, 该智能体执行 $a_{t}^{i}$ 。综上所述, 智能体只需要 执行中央下达的决策, 而不需要自己 “思考”。其原因在于策略函数 $\pi$ 需要全局的状态 $s_{t}$ 作为输入, 而单个智能体不知道全局状态, 没有能力单独做决策。

优缺点：中心化训练 + 中心化决策的优点在于完全按照 MAC-A2C 的算法实现, 没 有做任何改动, 因此可以确保正确性。基于全局的观测 $s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$ 做中心化的决策, 利用完整的信息, 因此作出的决策可以更好。中心化训练和决策的缺点在于延迟 (latency) 很大, 影响训练和决策的速度。在中心化执行的框架下, 智能体与中央控制器要做通信。 第 $i$ 号智能体要把 $o_{t}^{i}$ 传输给中央控制器, 而控制器要在收集到所有观测 $\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$ 之 后才会做决策, 做出的决策 $a_{t}^{i}$ 还得传输给第 $i$ 号智能体。这个过程通常比较慢, 使得实 时决策不可能做到。机器人、无人车、无人机等应用都需要实时决策, 比如在几十毫秒 内做出决策; 如果出现几百毫秒、甚至几秒的延迟, 可能会造成灾难性的后果。

### 2 去中心化训练 $+$ 去中心化决策

上一小节的 “中化训练 + 中心化决策”严格按照 MAC-A2C 的算法实现, 其缺点在 于训练和决策都需要智能体与中央控制器之间通信, 造成训练和决策的速度慢。想要避 免通信代价, 就不得不对策略网络和价值网络做近似。MAC-A2C 中的策略网络

$$
\pi\left(a^{1} \mid s ; \boldsymbol{\theta}^{1}\right), \quad \pi\left(a^{2} \mid s ; \boldsymbol{\theta}^{2}\right), \quad \cdots, \quad \pi\left(a^{m} \mid s ; \boldsymbol{\theta}^{m}\right),
$$

和价值网络 $v(s ; \boldsymbol{w})$ 都需要全局的观测 $s=\left[o^{1}, \cdots, o^{m}\right]$ 。去中心化训练 + 去中心化决策” 的基本思想是用局部观测 $o^{i}$ 代替 $s$, 把策略网络和价值网络近似成为:

$$
\pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right) \quad \text { 和 } v\left(o^{i} ; \boldsymbol{w}^{i}\right) .
$$

在每个智能体上部署一个策略网络和一个价值网络, 它们的参数记作 $\boldsymbol{\theta}^{i}$ 和 $\boldsymbol{w}^{i}$ 。智能体 之间不共享参数, 即 $\boldsymbol{\theta}^{i} \neq \boldsymbol{\theta}^{j}, \boldsymbol{w}^{i} \neq \boldsymbol{w}^{j}$ 。这样一来, 训练就可以在智能体本地完成, 无 需中央控制器的参与, 无需任何通信。见图 $15.5$ 中的系统架构。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-239.jpg?height=528&width=1408&top_left_y=1335&top_left_x=264)

图 15.6: 去中心化训练 + 去中心化决策的系统架构。这种方法也叫做 independent actor-critic。

去中心化训练 : 假设所有智能体的奖励都是相同的, 而且每个智能体都能观测到奖 励 $r$ 。每个智能体独立做训练, 智能体之间不做通信, 不共享观测、动作、参数。这样一 来, MAC-A2C 就变成了标准的 $\mathrm{A} 2 \mathrm{C}$, 每个智能体独立学习自己的参数 $\boldsymbol{\theta}^{i}$ 与 $\boldsymbol{w}^{i}$ 。

实际实现的时候, 每个智能体还需要一个目标网络, 记作 $v\left(s ; \boldsymbol{w}^{i-}\right)$, 它的结构与 $v\left(s ; \boldsymbol{w}^{i}\right)$ 相同, 但是参数不同。设第 $i$ 号智能体的策略网络、价值网络、目标网络当前参 数分别为 $\boldsymbol{\theta}_{\text {now }}^{i} 、 \boldsymbol{w}_{\text {now }}^{i} \boldsymbol{w}_{\text {now }}^{i-}$ 。该智能体重复以下步骤更新参数：

1. 在 $t$ 时刻, 智能体 $i$ 观测到 $o_{t}^{i}$, 然后做随机抽样 $a_{t}^{i} \sim \pi\left(\cdot \mid o_{t}^{i} ; \boldsymbol{\theta}^{i}\right)$, 并执行选中的动 作 $a_{t}^{i}$ 。

2. 环境反馈给智能体奖励 $r_{t}$ 与新的观测 $o_{t+1}^{i}$ 。

3. 让价值网络做预测: $\widehat{v}_{t}^{i}=v\left(o_{t}^{i} ; \boldsymbol{w}_{\text {now }}^{i}\right)$ 。

4. 让目标网络做预测: $\widehat{v}_{t+1}^{i}=v\left(o_{t+1}^{i} ; \boldsymbol{w}_{\text {now }}^{i-}\right)$ 。 5. 计算 TD 目标与 TD 误差：

$$
\widehat{y}_{t}^{i}=r_{t}+\gamma \cdot \widehat{v}_{t+1}^{i}, \quad \delta_{t}^{i}=\widehat{v}_{t}^{i}-\widehat{y}_{t}^{i} .
$$

6. 更新价值网络参数:

$$
\boldsymbol{w}_{\text {new }}^{i} \leftarrow \boldsymbol{w}_{\text {now }}^{i}-\alpha \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{w}^{i}} v\left(o_{t}^{i} ; \boldsymbol{w}_{\text {now }}^{i}\right) .
$$

7. 更新目标网络参数：

$$
\boldsymbol{w}_{\text {new }}^{i-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}^{i}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{i-} .
$$

8. 更新策略网络参数：

$$
\boldsymbol{\theta}_{\text {new }}^{i} \leftarrow \boldsymbol{\theta}_{\text {now }}^{i}-\beta \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid o_{t}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

注 上述算法不是真正的 MAC-A2C, 而是单智能体的 A2C。去中心化训练的本质就是单 智能体强化学习 (SARL), 而非多智能体强化学习 (MARL)。在 MARL 中, 智能体之间 会相互影响, 而本节中的 “去中心化训练”把智能体视为独立个体, 忽视它们之间的关联, 直接用 SARL 方法独立训练每个智能体。用上述 SARL 的方法解决 MARL 问题, 在实践 中效果往往不佳。

去中心化决策 : 在完成训练之后, 智能体 $i$ 不再需要其价值网络 $v\left(o^{i} ; \boldsymbol{w}^{i}\right)$ 。智能体 只需要用其本地部署的策略网络 $\pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right)$ 做决策即可, 决策过程无需通信。去中心化 执行的速度很快, 可以做到实时决策。

### 3 中心化训练 $+$ 去中心化决策

前面两节讨论了完全中心化与完全去中心化, 两种实现各有优缺点。当前更流行的 MARL 架构是“中心化训练 + 去中心化决策”。训练的时候使用中央控制器, 辅助智能体 做训练; 见图 15.7。训练结束之后, 不再需要中央控制器, 每个智能体独立根据本地观 测 $o^{i}$ 做决策；见图 15.8。

本小节与 “完全中心化”使用相同的价值网络 $v(s ; \boldsymbol{w})$ 及其目标网络 $v\left(s ; \boldsymbol{w}^{-}\right)$; 本节与 “完全去中心化”使用相同的策略网络：

$$
\pi\left(a^{1} \mid o^{1} ; \boldsymbol{\theta}^{1}\right), \quad \cdots, \quad \pi\left(a^{m} \mid o^{m} ; \boldsymbol{\theta}^{m}\right) .
$$

第 $i$ 号策略网络的输入是局部观测 $o^{i}$, 因此可以将其部署到第 $i$ 号智能体上。价值网络 $v(s ; \boldsymbol{w})$ 的输入是全局状态 $s=\left[o^{1}, \cdots, o^{m}\right]$, 因此需要将其部署到中央控制器上。

中心化训练: 训练的过程需要所有 $m$ 个智能体共同参与, 共同改进策略网络参数 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 与价值网络参数 $\boldsymbol{w}$ 。设当前 $m$ 个策略网络的参数为 $\boldsymbol{\theta}_{\mathrm{now}}^{1}, \cdots, \boldsymbol{\theta}_{\mathrm{now}}^{m}$. 设当前价 值网络和目标网络的参数分别是 $\boldsymbol{w}_{\text {now }}$ 和 $\boldsymbol{w}_{\text {now }}^{-}$。训练的流程如下:

1. 每个智能体 $i$ 与环境交互, 获取当前观测 $o_{t}^{i}$, 独立做随机抽样:

$$
a_{t}^{i} \sim \pi\left(\cdot \mid o_{t}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m,
$$

并执行选中的动作。

2. 下一时刻, 每个智能体 $i$ 都观测到 $o_{t+1}^{i}$ 。假设中央控制器可以从环境获取奖励 $r_{t}$, 或者向智能体询问奖励 $r_{t}$ 。 中央控制器

价值网络 $v(s ; \mathbf{w})$

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-241.jpg?height=483&width=1333&top_left_y=655&top_left_x=250)

环境

图 15.7: 中心化训练的系统架构。价值网络（以及没画出的目标网络）部署到中央控制器上, 策 略网络部署到每个智能体上。训练的时候, 智能体 $i$ 将观测 $o^{i}$ 传输到控制器上, 控制器将 TD 误 差 $\delta$ 传回智能体。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-241.jpg?height=365&width=1348&top_left_y=1882&top_left_x=251)

环境

图 15.8: 去中心化决策的系统架构。在完成训练之后, 智能体不再做通信, 智能体用本地部署的 策略网络做决策。 3. 每个智能体 $i$ 向中央控制器传输观测 $o_{t}^{i}$ 和 $o_{t+1}^{i}$; 中央控制器得到状态

$$
s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right] \quad \text { 和 } \quad s_{t+1}=\left[o_{t+1}^{1}, \cdots, o_{t+1}^{m}\right] .
$$

4. 中央控制器让价值网络做预测： $\widehat{v}_{t}=v\left(s_{t} ; \boldsymbol{w}_{\text {now }}\right)$ 。

5. 中央控制器让目标网络做预测： $\widehat{v}_{t+1}^{-}=v\left(s_{t+1} ; \boldsymbol{w}_{\text {now }}^{-}\right)$。

6. 中央控制器计算 TD 目标和 TD 误差:

$$
\widehat{y}_{t}^{-}=r_{t}+\gamma \cdot \widehat{v}_{t+1}^{-}, \quad \delta_{t}=\widehat{v}_{t}-\widehat{y}_{t}^{-},
$$

并将 $\delta_{t}$ 广播到所有智能体。

7. 中央控制器更新价值网络参数:

$$
\boldsymbol{w}_{\text {new }} \leftarrow \boldsymbol{w}_{\text {now }}-\alpha \cdot \delta_{t} \cdot \nabla_{\boldsymbol{w}} v\left(s_{t} ; \boldsymbol{w}_{\text {now }}\right) .
$$

8. 中央控制器更新目标网络参数:

$$
\boldsymbol{w}_{\text {new }}^{-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{-} .
$$

9. 每个智能体 $i$ 更新策略网络参数：

$$
\boldsymbol{\theta}_{\text {new }}^{i} \leftarrow \boldsymbol{\theta}_{\text {now }}^{i}-\beta \cdot \delta_{t} \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid o_{t}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i}\right) .
$$

注此处的算法并不等价于第 $15.2$ 节的 MAC-A2C。区别在于此处用 $\pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right)$ 代替 MAC$\mathrm{A} 2 \mathrm{C}$ 中的 $\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$.

去中心化决策 : 在完成训练之后, 不再需要价值网络 $v(s ; \boldsymbol{w})$ 。智能体只需要用其 本地部署的策略网络 $\pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right)$ 做决策, 决策过程无需通信。去中心化执行的速度很快, 可以做到实时决策。

## 第 15 章 知识点

- 在完全合作关系的设定下, 所有智能体的奖励是相同的。因此所有的智能体都有相 同的回报和价值函数。如果做策略学习, 那么所有智能体都有相同的目标函数。

- 有两种方式实现多智能体强化学习：中心化、去中心化。中心化需要智能体与中央 控制器通信, 速度较慢。去中心化无需通信, 速度快, 但是基于不完全信息的决策 效果较差。

- 最流行的架构是“中心化训练 + 去中心化决策”。中心化训练有利于学到更好的策略, 而去中心化决策无需通信, 可以做到实时决策。

## 第 15 章习题

1. 设动作 $A=\left[A^{1}, \cdots, A^{m}\right]$ 的概率质量函数为

$$
\pi\left(A \mid S ; \boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right) \triangleq \pi\left(A^{1} \mid S ; \boldsymbol{\theta}^{1}\right) \times \cdots \times \pi\left(A^{m} \mid S ; \boldsymbol{\theta}^{m}\right) .
$$

由第 8 章中带基线的策略梯度定理可得:

$$
\nabla_{\boldsymbol{\theta}^{i}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S, A}\left[\left(Q_{\pi}(S, A)-b\right) \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(A \mid S ; \boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)\right] .
$$

公式中动作 $A$ 的概率质量函数为 $\pi\left(A \mid S ; \boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)$, 公式中的 $b$ 是任意不依赖 于 $A$ 的函数。请用上面两个公式证明下面的公式:

$$
\nabla_{\boldsymbol{\theta}^{i}} J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S, A}\left[\left(Q_{\pi}(S, A)-V_{\pi}(S)\right) \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(A^{i} \mid S ; \boldsymbol{\theta}^{i}\right)\right] .
$$



## 第 15 章 相关文献

完全去中心化的架构早在 1993 年就被提出 $^{[108]}$, 在 2017 年被用在多智能体 DQN 上 $^{[39,107]}$ 。中心化训练 + 去中心化执行 (centralized training with decentralized execution) 在近年来很流行 $[84,43,38,72,56\rfloor 。$

MAC-A2C 是本书设计出来的简单方法, 用于讲解 MARL 的三种架构; MAC-A2C 这 个名字并没有出现在任何文献中。MAC-A2C 本质是带基线的 actor-critic, 其中的基线是 状态价值

$$
V_{\pi}(s) \triangleq \mathbb{E}_{A}\left[Q_{\pi}(s, A)\right],
$$

期望是关于动作 $A=\left[A^{1}, \cdots, A^{m}\right]$ 求的。可以把基线换成

$$
Q_{\pi}^{-i}\left(s, a^{-i}\right) \triangleq \mathbb{E}_{A^{i}}\left[Q_{\pi}\left(s, A^{i}, a^{-i}\right)\right],
$$

公式中 $a^{-i}=\left[a^{1}, \cdots, a^{i-1}, a^{i+1}, \cdots, a^{m}\right]$ 。公式中的期望是关于第 $i$ 号智能体的动作 $A^{i} \sim$ $\pi\left(\cdot \mid o^{i}, \boldsymbol{\theta}^{i}\right)$ 求的。用 $Q_{\pi}^{-i}\left(s, a^{-i}\right)$ 作为基线, 代替 $V_{\pi}(s)$, 得到的方法叫做 counterfactual multi-agent, 缩写 COMA ${ }^{[38]}$ 。此外, COMA 还在策略网络中使用 RNN; 其原理见第 11 章的解释。COMA 的表现略好于 MAC-A2C, 但是 COMA 的实现很复杂, 不建议读者自 己实现。
