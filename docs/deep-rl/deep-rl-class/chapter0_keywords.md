# 词汇表

## 马尔科夫性质

智能体采取的行动**仅以当前状态为条件，而与过去的状态和行动无关**。

## 观察/状态

- **状态**：对世界状态的完整描述。
- **观察**：对环境/世界状态的部分描述。

## 动作

- **离散动作**：有限数量的动作，例如向左、向右、向上和向下。
- **连续动作**：动作的无限可能性；例如，在自动驾驶汽车的情况下，驾驶场景有无限可能的动作发生。

## 奖励和折扣

- **奖励**：RL 中的基本因素。告诉智能体所采取的行动是好是坏。
- RL 算法专注于最大化**累积奖励**。
- **奖励假设**：RL 问题可以表述为（累积）回报的最大化。
- **执行折扣**是因为在开始时获得的奖励更有可能发生，因为它们比长期奖励更可预测。

## 探索与开发权衡

- **探索**：通过尝试随机行动并从环境中接收反馈/回报/奖励来探索环境。
- **利用**：利用我们对环境的了解以获得最大回报。
- **Exploration-Exploitation Trade-Off**：它平衡了我们想要**探索**环境的程度和我们想要**利用**我们对环境的了解程度。

## Policy

- **策略**：它被称为智能体的大脑。它告诉我们在给定状态下采取什么行动。
- **Optimal Policy**：当智能体按照它行事时，**最大化**预期回报*的Policy。它是通过训练学会的。

## 基于策略的方法：

- 一种解决 RL 问题的方法。
- 在这种方法中，Policy 是直接学习的。
- 将每个状态映射到该状态下的最佳对应动作。或者在该状态下一组可能的动作的概率分布。

## 基于价值的方法：

- 解决 RL 问题的另一种方法。
- 在这里，我们没有训练策略，而是训练了一个**价值函数**，将每个状态映射到处于该状态的期望值。