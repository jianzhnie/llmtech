# Decision Transformers

## ä»€ä¹ˆæ˜¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Ÿ

æ·±åº¦å¼ºåŒ–å­¦ä¹  (RL) æ˜¯æ„å»ºå†³ç­–$Agents$çš„æ¡†æ¶ã€‚è¿™äº›$Agents$æ—¨åœ¨é€šè¿‡åå¤è¯•éªŒä¸ç¯å¢ƒäº¤äº’å¹¶æ¥æ”¶å¥–åŠ±ä½œä¸ºç‹¬ç‰¹çš„åé¦ˆæ¥å­¦ä¹ æœ€ä½³è¡Œä¸ºï¼ˆç­–ç•¥ï¼‰ã€‚

$Agents$çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–**å…¶ç´¯ç§¯å¥–åŠ±ï¼Œç§°ä¸ºå›æŠ¥ã€‚**å› ä¸º RL åŸºäºå¥–åŠ±å‡è®¾ï¼š**æ‰€æœ‰ç›®æ ‡éƒ½å¯ä»¥æè¿°ä¸ºæœŸæœ›ç´¯ç§¯å¥–åŠ±çš„æœ€å¤§åŒ–ã€‚**

æ·±åº¦å¼ºåŒ–å­¦ä¹ $Agents$**é€šè¿‡æ‰¹é‡ç»éªŒè¿›è¡Œå­¦ä¹ ã€‚**é—®é¢˜æ˜¯ï¼Œä»–ä»¬å¦‚ä½•æ”¶é›†æ•°æ®ï¼Ÿï¼š

[![ç¦»çº¿ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ ](https://huggingface.co/blog/assets/58_decision-transformers/offlinevsonlinerl.gif)](https://huggingface.co/blog/assets/58_decision-transformers/offlinevsonlinerl.gif)

åœ¨çº¿å’Œç¦»çº¿è®¾ç½®ä¸­å¼ºåŒ–å­¦ä¹ çš„æ¯”è¾ƒï¼Œå›¾ç‰‡æ¥è‡ª[è¿™ç¯‡æ–‡ç« ](https://offline-rl.github.io/)

åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œ**$Agents$ç›´æ¥æ”¶é›†æ•°æ®**ï¼šå®ƒé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥æ”¶é›†ä¸€æ‰¹ç»éªŒã€‚ç„¶åï¼Œå®ƒä¼šç«‹å³ï¼ˆæˆ–é€šè¿‡ä¸€äº›é‡æ’­ç¼“å†²åŒºï¼‰ä½¿ç”¨æ­¤ç»éªŒæ¥ä»ä¸­å­¦ä¹ ï¼ˆæ›´æ–°å…¶ç­–ç•¥ï¼‰ã€‚

ä½†è¿™æ„å‘³ç€ä½ è¦ä¹ˆç›´æ¥åœ¨ç°å®ä¸–ç•Œä¸­è®­ç»ƒä½ çš„$Agents$ï¼Œè¦ä¹ˆæœ‰ä¸€ä¸ªæ¨¡æ‹Ÿå™¨ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™éœ€è¦æ„å»ºå®ƒï¼Œè¿™å¯èƒ½éå¸¸å¤æ‚ï¼ˆå¦‚ä½•åœ¨ç¯å¢ƒä¸­åæ˜ ç°å®ä¸–ç•Œçš„å¤æ‚ç°å®ï¼Ÿï¼‰ã€æ˜‚è´µä¸”ä¸å®‰å…¨ï¼Œå› ä¸ºå¦‚æœæ¨¡æ‹Ÿå™¨æœ‰ç¼ºé™·ï¼Œå¦‚æœå®ƒä»¬æä¾›ç«äº‰ä¼˜åŠ¿ï¼Œ$Agents$å°±åˆ©ç”¨å®ƒä»¬ã€‚

å¦ä¸€æ–¹é¢ï¼Œåœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œ$Agents$ä»…ä½¿ç”¨ä»å…¶ä»–$Agents$æˆ–äººç±»æ¼”ç¤ºä¸­æ”¶é›†çš„æ•°æ®ã€‚**å®ƒä¸ä¸ç¯å¢ƒç›¸äº’ä½œç”¨**ã€‚

è¿‡ç¨‹å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªç­–ç•¥å’Œ/æˆ–äººå·¥äº¤äº’åˆ›å»ºæ•°æ®é›†ã€‚
2. åœ¨æ­¤æ•°æ®é›†ä¸Šè¿è¡Œç¦»çº¿ RL ä»¥å­¦ä¹ ç­–ç•¥

è¿™ç§æ–¹æ³•æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼šåäº‹å®æŸ¥è¯¢é—®é¢˜ã€‚å¦‚æœæˆ‘ä»¬çš„$Agents$äººå†³å®šåšä¸€äº›æˆ‘ä»¬æ²¡æœ‰æ•°æ®çš„äº‹æƒ…ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿä¾‹å¦‚ï¼Œåœ¨åå­—è·¯å£å³è½¬ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰è¿™ä¸ªè½¨è¿¹ã€‚

å·²ç»æœ‰ä¸€äº›å…³äºè¿™ä¸ªä¸»é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„ä¿¡æ¯ï¼Œä½ å¯ä»¥è§‚çœ‹[è¿™ä¸ªè§†é¢‘](https://www.youtube.com/watch?v=k08N5a0gG0A)

## å¼•å…¥ Decision Transformers

Decision Transformer æ¨¡å‹[ç”± Chen L. ç­‰äººçš„â€œDecision Transformerï¼šReinforcement Learning via Sequence Modelingâ€](https://arxiv.org/abs/2106.01345)ä»‹ç»ã€‚å®ƒå°†å¼ºåŒ–å­¦ä¹ æŠ½è±¡ä¸º**æ¡ä»¶åºåˆ—å»ºæ¨¡é—®é¢˜**ã€‚

ä¸»è¦æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬ä¸æ˜¯ä½¿ç”¨ RL æ–¹æ³•è®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚æ‹Ÿåˆå€¼å‡½æ•°ï¼Œå®ƒä¼šå‘Šè¯‰æˆ‘ä»¬é‡‡å–ä»€ä¹ˆåŠ¨ä½œæ¥æœ€å¤§åŒ–å›æŠ¥ï¼ˆç´¯ç§¯å¥–åŠ±ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨åºåˆ—å»ºæ¨¡ç®—æ³•ï¼ˆTransformerï¼‰ï¼Œç»™å®šæœŸæœ›çš„å›æŠ¥ã€è¿‡å»çš„çŠ¶æ€å’ŒåŠ¨ä½œå°†äº§ç”Ÿæœªæ¥çš„åŠ¨ä½œä»¥å®ç°è¿™ä¸€æœŸæœ›çš„å›æŠ¥ã€‚å®ƒæ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œä»¥æœŸæœ›å›æŠ¥ã€è¿‡å»çŠ¶æ€å’ŒåŠ¨ä½œä¸ºæ¡ä»¶ï¼Œä»¥ç”Ÿæˆå®ç°æœŸæœ›å›æŠ¥çš„æœªæ¥åŠ¨ä½œã€‚

è¿™æ˜¯å¼ºåŒ–å­¦ä¹ èŒƒå¼çš„å½»åº•è½¬å˜ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆè½¨è¿¹å»ºæ¨¡ï¼ˆå¯¹çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±åºåˆ—çš„è”åˆåˆ†å¸ƒå»ºæ¨¡ï¼‰æ¥å–ä»£ä¼ ç»Ÿçš„ RL ç®—æ³•ã€‚è¿™æ„å‘³ç€åœ¨ Decision Transformers ä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šæœ€å¤§åŒ–å›æŠ¥ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ç³»åˆ—æœªæ¥çš„åŠ¨ä½œæ¥å®ç°é¢„æœŸçš„å›æŠ¥ã€‚

è¿™ä¸ªè¿‡ç¨‹æ˜¯è¿™æ ·çš„ï¼š

1. æˆ‘ä»¬å°†æœ€å K ä¸ªæ—¶é—´æ­¥è¾“å…¥åˆ°å…·æœ‰ 3 ä¸ªè¾“å…¥çš„Decision Transformerä¸­ï¼š
   - Return-to-go
   - çŠ¶æ€
   - åŠ¨ä½œ
2. å¦‚æœçŠ¶æ€æ˜¯å‘é‡ï¼Œåˆ™åµŒå…¥çº¿æ€§å±‚ï¼›å¦‚æœçŠ¶æ€æ˜¯å¸§ï¼Œåˆ™åµŒå…¥ CNN ç¼–ç å™¨ï¼Œ å¯¹ Token è¿›è¡Œç¼–ç ã€‚
3. è¾“å…¥ç”± GPT-2 æ¨¡å‹å¤„ç†ï¼Œè¯¥æ¨¡å‹é€šè¿‡è‡ªå›å½’å»ºæ¨¡é¢„æµ‹æœªæ¥çš„è¡Œä¸ºã€‚

![Decision Transformeræ¶æ„](https://huggingface.co/blog/assets/58_decision-transformers/dt-architecture.gif)

Decision Transformers æ¶æ„ã€‚çŠ¶æ€ã€åŠ¨ä½œå’Œå›æŠ¥è¢«é€åˆ°æ¨¡æ€ç‰¹å®šçš„çº¿æ€§åµŒå…¥ä¸­ï¼Œå¹¶æ·»åŠ äº†ä½ç½®æƒ…æ™¯æ—¶é—´æ­¥é•¿ç¼–ç ã€‚Token è¢«é€å…¥ GPT æ¶æ„ï¼Œè¯¥æ¶æ„ä½¿ç”¨å› æœè‡ªæ³¨æ„æ©ç è‡ªå›å½’åœ°é¢„æµ‹åŠ¨ä½œã€‚å›¾æ¥è‡ª[1]ã€‚

## åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨Decision Transformer

Decision Transformer æ¨¡å‹ç°åœ¨ä½œä¸º ğŸ¤— transformers åº“çš„ä¸€éƒ¨åˆ†æä¾›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†äº«[äº† Gym ç¯å¢ƒä¸­è¿ç»­æ§åˆ¶ä»»åŠ¡çš„ä¹ä¸ªé¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/models?other=gym-continous-control)ã€‚

<video alt="WalkerEd-ä¸“å®¶" autoplay="" loop="" autobuffer="" muted="" playsinline="" style="border: 0px solid rgb(229, 231, 235); box-sizing: border-box; --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(59,130,246,0.5); --tw-ring-offset-shadow:0 0 #0000; --tw-ring-shadow:0 0 #0000; --tw-shadow:0 0 #0000; --tw-shadow-colored:0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; height: auto; max-width: 70%; margin: auto;"></video>

*â€œä¸“å®¶â€Decision Transformeræ¨¡å‹ï¼Œåœ¨ Gym Walker2d ç¯å¢ƒä¸­ä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ å­¦ä¹ ã€‚*

### å®‰è£…åŒ…

```python
pip install git+https://github.com/huggingface/transformers
```

### åŠ è½½æ¨¡å‹

ä½¿ç”¨ Decision Transformer ç›¸å¯¹å®¹æ˜“ï¼Œä½†ç”±äºå®ƒæ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œå› æ­¤å¿…é¡»å°å¿ƒè°¨æ…ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ—¶é—´æ­¥å‡†å¤‡æ¨¡å‹çš„è¾“å…¥ã€‚æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ª[Python è„šæœ¬](https://github.com/huggingface/transformers/blob/main/examples/research_projects/decision_transformer/run_decision_transformer.py)å’Œä¸€ä¸ª[Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1K3UuajwoPY1MzRKNkONNRS3gS5DxZ-qF?usp=sharing)æ¥æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è¯¥æ¨¡å‹ã€‚

åœ¨ ğŸ¤— transformers åº“ä¸­åŠ è½½é¢„è®­ç»ƒçš„ Decision Transformer å¾ˆç®€å•ï¼š

```python
from transformers import DecisionTransformerModel

model_name = "edbeeching/decision-transformer-gym-hopper-expert"
model = DecisionTransformerModel.from_pretrained(model_name)
```

### åˆ›é€ ç¯å¢ƒ

æˆ‘ä»¬ä¸º Gym Hopperã€Walker2D å’Œ Halfcheetah æä¾›é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚Atari ç¯å¢ƒçš„æ£€æŸ¥ç‚¹å°†å¾ˆå¿«å¯ç”¨ã€‚

```python
import gym
env = gym.make("Hopper-v3")
state_dim = env.observation_space.shape[0] # state size
act_dim = env.action_space.shape[0] # action size
```

### è‡ªå›å½’é¢„æµ‹å‡½æ•°

è¯¥æ¨¡å‹æ‰§è¡Œ[è‡ªå›å½’é¢„æµ‹](https://en.wikipedia.org/wiki/Autoregressive_model)ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å½“å‰æ—¶é—´æ­¥**t**åšå‡ºçš„é¢„æµ‹é¡ºåºåœ°å–å†³äºå…ˆå‰æ—¶é—´æ­¥é•¿çš„è¾“å‡ºã€‚è¿™ä¸ªåŠŸèƒ½å¾ˆä¸°å¯Œï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨è¯„è®ºä¸­è§£é‡Šå®ƒã€‚

```python
# Function that gets an action from the model using autoregressive prediction 
# with a window of the previous 20 timesteps.
def get_action(model, states, actions, rewards, returns_to_go, timesteps):
    # This implementation does not condition on past rewards
    
    states = states.reshape(1, -1, model.config.state_dim)
    actions = actions.reshape(1, -1, model.config.act_dim)
    returns_to_go = returns_to_go.reshape(1, -1, 1)
    timesteps = timesteps.reshape(1, -1)
    
    # The prediction is conditioned on up to 20 previous time-steps
    states = states[:, -model.config.max_length :]
    actions = actions[:, -model.config.max_length :]
    returns_to_go = returns_to_go[:, -model.config.max_length :]
    timesteps = timesteps[:, -model.config.max_length :]
    
    # pad all tokens to sequence length, this is required if we process batches
    padding = model.config.max_length - states.shape[1]
    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])
    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)
    states = torch.cat([torch.zeros((1, padding, state_dim)), states], dim=1).float()
    actions = torch.cat([torch.zeros((1, padding, act_dim)), actions], dim=1).float()
    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()
    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)
    
    # perform the prediction
    state_preds, action_preds, return_preds = model(
            states=states,
            actions=actions,
            rewards=rewards,
            returns_to_go=returns_to_go,
            timesteps=timesteps,
            attention_mask=attention_mask,
            return_dict=False,)
    return action_preds[0, -1]
```

### è¯„ä¼°æ¨¡å‹

ä¸ºäº†è¯„ä¼°æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›é¢å¤–çš„ä¿¡æ¯ï¼›è®­ç»ƒæœŸé—´ä½¿ç”¨çš„çŠ¶æ€çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚å¹¸è¿çš„æ˜¯ï¼Œ Hugging Face Hub ä¸Šçš„æ¯ä¸ª[æ¨¡å‹å¡éƒ½å¯ä»¥ä½¿ç”¨è¿™äº›ï¼](https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert)

æˆ‘ä»¬è¿˜éœ€è¦æ¨¡å‹çš„ç›®æ ‡å›æŠ¥ã€‚è¿™å°±æ˜¯ä»¥å›æŠ¥ä¸ºæ¡ä»¶çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„åŠ›é‡ï¼šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›®æ ‡å›æŠ¥æ¥æ§åˆ¶æ”¿ç­–çš„è¡¨ç°ã€‚è¿™åœ¨å¤šäººæ¸¸æˆè®¾ç½®ä¸­å¯èƒ½éå¸¸å¼ºå¤§ï¼Œæˆ‘ä»¬å¸Œæœ›è°ƒæ•´å¯¹æ‰‹æœºå™¨äººçš„æ€§èƒ½ï¼Œä½¿å…¶å¤„äºé€‚åˆç©å®¶çš„éš¾åº¦ã€‚ä½œè€…åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­å±•ç¤ºäº†ä¸€ä¸ªå¾ˆå¥½çš„æƒ…èŠ‚ï¼

[![ç»“æœDecision Transformer](https://huggingface.co/blog/assets/58_decision-transformers/results-dt.png)](https://huggingface.co/blog/assets/58_decision-transformers/results-dt.png)*åœ¨ä»¥æŒ‡å®šç›®æ ‡ï¼ˆæœŸæœ›ï¼‰å›æŠ¥ä¸ºæ¡ä»¶æ—¶ï¼Œç”± Decision Transformer ç´¯ç§¯çš„é‡‡æ ·ï¼ˆè¯„ä¼°ï¼‰å›æŠ¥ã€‚ä¸Šï¼šé›…è¾¾åˆ©ã€‚åº•éƒ¨ï¼šD4RL ä¸­é‡æ”¾æ•°æ®é›†ã€‚å›¾æ¥è‡ª[1]ã€‚*

```python
TARGET_RETURN = 3.6 # This was normalized during training
MAX_EPISODE_LENGTH = 1000 

state_mean = np.array(
    [1.3490015,  -0.11208222, -0.5506444,  -0.13188992, -0.00378754,  2.6071432,
     0.02322114, -0.01626922, -0.06840388, -0.05183131,  0.04272673,])

state_std = np.array(
    [0.15980862, 0.0446214,  0.14307782, 0.17629202, 0.5912333,  0.5899924,
         1.5405099,  0.8152689,  2.0173461,  2.4107876,  5.8440027,])

state_mean = torch.from_numpy(state_mean)
state_std = torch.from_numpy(state_std)

state = env.reset()
target_return = torch.tensor(TARGET_RETURN).float().reshape(1, 1)
states = torch.from_numpy(state).reshape(1, state_dim).float()
actions = torch.zeros((0, act_dim)).float()
rewards = torch.zeros(0).float()
timesteps = torch.tensor(0).reshape(1, 1).long()

# take steps in the environment
for t in range(max_ep_len):
    # add zeros for actions as input for the current time-step
    actions = torch.cat([actions, torch.zeros((1, act_dim))], dim=0)
    rewards = torch.cat([rewards, torch.zeros(1)])

    # predicting the action to take
    action = get_action(model,
                        (states - state_mean) / state_std,
                        actions,
                        rewards,
                        target_return,
                        timesteps)
    actions[-1] = action
    action = action.detach().numpy()

    # interact with the environment based on this action
    state, reward, done, _ = env.step(action)
    
    cur_state = torch.from_numpy(state).reshape(1, state_dim)
    states = torch.cat([states, cur_state], dim=0)
    rewards[-1] = reward
    
    pred_return = target_return[0, -1] - (reward / scale)
    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)
    timesteps = torch.cat([timesteps, torch.ones((1, 1)).long() * (t + 1)], dim=1)
    
    if done:
        break
```

[æ‚¨ä¼šåœ¨æˆ‘ä»¬çš„Colab notebook](https://colab.research.google.com/drive/1K3UuajwoPY1MzRKNkONNRS3gS5DxZ-qF?usp=sharing)ä¸­æ‰¾åˆ°æ›´è¯¦ç»†çš„ç¤ºä¾‹ï¼Œä»¥åŠ$Agents$è§†é¢‘çš„åˆ›å»ºã€‚



## Training Decision Transformers

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— Trainer å’Œè‡ªå®šä¹‰æ•°æ®æ”¶é›†å™¨ä»å¤´å¼€å§‹è®­ç»ƒDecision Transformersæ¨¡å‹ï¼Œä½¿ç”¨ ğŸ¤— é›†çº¿å™¨ä¸Šæ‰˜ç®¡çš„ç¦»çº¿ RL æ•°æ®é›†ã€‚[æ‚¨å¯ä»¥åœ¨è¿™ä¸ª colab notebook](https://github.com/huggingface/blog/blob/main/notebooks/101_train-decision-transformers.ipynb)ä¸­æ‰¾åˆ°æœ¬æ•™ç¨‹çš„ä»£ç 

æˆ‘ä»¬å°†æ‰§è¡Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ä»¥åœ¨[mujoco halfcheetah ç¯å¢ƒ](https://www.gymlibrary.dev/environments/mujoco/half_cheetah/)ä¸­å­¦ä¹ ä»¥ä¸‹è¡Œä¸ºã€‚

<video alt="CheetahEd-ä¸“å®¶" autoplay="" loop="" autobuffer="" muted="" playsinline="" style="border: 0px solid rgb(229, 231, 235); box-sizing: border-box; --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(59,130,246,0.5); --tw-ring-offset-shadow:0 0 #0000; --tw-ring-shadow:0 0 #0000; --tw-shadow:0 0 #0000; --tw-shadow-colored:0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; height: auto; max-width: 70%; margin: auto;"></video>

*â€œä¸“å®¶â€Decision Transformers æ¨¡å‹ï¼Œåœ¨ Gym HalfCheetah ç¯å¢ƒä¸­ä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ å­¦ä¹ ã€‚*

### åŠ è½½æ•°æ®é›†å¹¶æ„å»ºè‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨

æˆ‘ä»¬åœ¨hubä¸Šæ‰˜ç®¡äº†è®¸å¤šç¦»çº¿ RL æ•°æ®é›†ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ hub ä¸Šæ‰˜ç®¡çš„ halfcheetahâ€œä¸“å®¶â€æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦`load_dataset`ä» ğŸ¤— æ•°æ®é›†åŒ…ä¸­å¯¼å…¥å‡½æ•°ï¼Œå¹¶å°†æ•°æ®é›†ä¸‹è½½åˆ°æˆ‘ä»¬çš„æœºå™¨ä¸Šã€‚

```python
from datasets import load_dataset
dataset = load_dataset("edbeeching/decision_transformer_gym_replay", "halfcheetah-expert-v2")
```

è™½ç„¶é›†çº¿å™¨ä¸Šçš„å¤§å¤šæ•°æ•°æ®é›†éƒ½å¯ä»¥å¼€ç®±å³ç”¨ï¼Œä½†æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›å¯¹æ•°æ®é›†æ‰§è¡Œä¸€äº›é¢å¤–çš„å¤„ç†æˆ–ä¿®æ”¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹[æˆ‘ä»¬å¸Œæœ›åŒ¹é…ä½œè€…çš„å®ç°](https://github.com/kzl/decision-transformer)ï¼Œå³æˆ‘ä»¬éœ€è¦ï¼š

- é€šè¿‡å‡å»å¹³å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®æ¥å½’ä¸€åŒ–æ¯ä¸ªç‰¹å¾ã€‚
- é¢„å…ˆè®¡ç®—æ¯ä¸ªè½¨è¿¹çš„æŠ˜æ‰£å›æŠ¥ã€‚
- å°†å¥–åŠ±å’Œå›æŠ¥ä¹˜ä»¥ 1000 å€ã€‚
- å¢åŠ æ•°æ®é›†é‡‡æ ·åˆ†å¸ƒï¼Œä»¥ä¾¿å°†ä¸“å®¶ä»£ç†è½¨è¿¹çš„é•¿åº¦è€ƒè™‘åœ¨å†…ã€‚

ä¸ºäº†æ‰§è¡Œæ­¤æ•°æ®é›†é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‡ªå®šä¹‰ ğŸ¤— [Data](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator) Collator ã€‚

ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹ä½¿ç”¨ç”¨äºç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ã€‚

```python
@dataclass
class DecisionTransformerGymDataCollator:
    return_tensors: str = "pt"
    max_len: int = 20 #subsets of the episode we use for training
    state_dim: int = 17  # size of state space
    act_dim: int = 6  # size of action space
    max_ep_len: int = 1000 # max episode length in the dataset
    scale: float = 1000.0  # normalization of rewards/returns
    state_mean: np.array = None  # to store state means
    state_std: np.array = None  # to store state stds
    p_sample: np.array = None  # a distribution to take account trajectory lengths
    n_traj: int = 0 # to store the number of trajectories in the dataset

    def __init__(self, dataset) -> None:
        self.act_dim = len(dataset[0]["actions"][0])
        self.state_dim = len(dataset[0]["observations"][0])
        self.dataset = dataset
        # calculate dataset stats for normalization of states
        states = []
        traj_lens = []
        for obs in dataset["observations"]:
            states.extend(obs)
            traj_lens.append(len(obs))
        self.n_traj = len(traj_lens)
        states = np.vstack(states)
        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6
        
        traj_lens = np.array(traj_lens)
        self.p_sample = traj_lens / sum(traj_lens)

    def _discount_cumsum(self, x, gamma):
        discount_cumsum = np.zeros_like(x)
        discount_cumsum[-1] = x[-1]
        for t in reversed(range(x.shape[0] - 1)):
            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]
        return discount_cumsum

    def __call__(self, features):
        batch_size = len(features)
        # this is a bit of a hack to be able to sample of a non-uniform distribution
        batch_inds = np.random.choice(
            np.arange(self.n_traj),
            size=batch_size,
            replace=True,
            p=self.p_sample,  # reweights so we sample according to timesteps
        )
        # a batch of dataset features
        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []
        
        for ind in batch_inds:
            # for feature in features:
            feature = self.dataset[int(ind)]
            si = random.randint(0, len(feature["rewards"]) - 1)

            # get sequences from dataset
            s.append(np.array(feature["observations"][si : si + self.max_len]).reshape(1, -1, self.state_dim))
            a.append(np.array(feature["actions"][si : si + self.max_len]).reshape(1, -1, self.act_dim))
            r.append(np.array(feature["rewards"][si : si + self.max_len]).reshape(1, -1, 1))

            d.append(np.array(feature["dones"][si : si + self.max_len]).reshape(1, -1))
            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))
            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff
            rtg.append(
                self._discount_cumsum(np.array(feature["rewards"][si:]), gamma=1.0)[
                    : s[-1].shape[1]   # TODO check the +1 removed here
                ].reshape(1, -1, 1)
            )
            if rtg[-1].shape[1] < s[-1].shape[1]:
                print("if true")
                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)

            # padding and state + reward normalization
            tlen = s[-1].shape[1]
            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)
            s[-1] = (s[-1] - self.state_mean) / self.state_std
            a[-1] = np.concatenate(
                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],
                axis=1,
            )
            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)
            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)
            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale
            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)
            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))

        s = torch.from_numpy(np.concatenate(s, axis=0)).float()
        a = torch.from_numpy(np.concatenate(a, axis=0)).float()
        r = torch.from_numpy(np.concatenate(r, axis=0)).float()
        d = torch.from_numpy(np.concatenate(d, axis=0))
        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()
        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()
        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()

        return {
            "states": s,
            "actions": a,
            "rewards": r,
            "returns_to_go": rtg,
            "timesteps": timesteps,
            "attention_mask": mask,
        }
```

å¾ˆå¤šä»£ç ï¼ŒTLDR æ˜¯æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç±»ï¼Œå®ƒæ¥å—æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œæ‰§è¡Œæ‰€éœ€çš„é¢„å¤„ç†ï¼Œå¹¶å°†è¿”å›æˆ‘ä»¬æ‰¹æ¬¡çš„**states**ã€**actions**ã€**rewards**ã€**returns**ã€**timesteps**å’Œ**masks ã€‚**è¿™äº›æ‰¹æ¬¡å¯ä»¥ç›´æ¥ç”¨äºä½¿ç”¨ ğŸ¤— transformers Trainer è®­ç»ƒ Decision Transformer æ¨¡å‹ã€‚

### ä½¿ç”¨ ğŸ¤— transformers Trainer è®­ç»ƒ Decision Transformer æ¨¡å‹ã€‚

ä¸ºäº†ç”¨ ğŸ¤— [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer)ç±»è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç¡®ä¿å®ƒè¿”å›çš„å­—å…¸åŒ…å«æŸå¤±ï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯æ¨¡å‹åŠ¨ä½œé¢„æµ‹å’Œç›®æ ‡çš„[L-2 èŒƒæ•°ã€‚](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm)æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ªç»§æ‰¿è‡ª Decision Transformer æ¨¡å‹çš„ TrainableDT ç±»æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```python
class TrainableDT(DecisionTransformerModel):
    def __init__(self, config):
        super().__init__(config)

    def forward(self, **kwargs):
        output = super().forward(**kwargs)
        # add the DT loss
        action_preds = output[1]
        action_targets = kwargs["actions"]
        attention_mask = kwargs["attention_mask"]
        act_dim = action_preds.shape[2]
        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]
        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]
        
        loss = torch.mean((action_preds - action_targets) ** 2)

        return {"loss": loss}

    def original_forward(self, **kwargs):
        return super().forward(**kwargs)
```

Transformers Trainer ç±»éœ€è¦ä¸€äº›å‚æ•°ï¼Œå®šä¹‰åœ¨ TrainingArguments ç±»ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸ä½œè€…åŸå§‹å®ç°ä¸­ç›¸åŒçš„è¶…å‚æ•°ï¼Œä½†è®­ç»ƒè¿­ä»£æ¬¡æ•°æ›´å°‘ã€‚è¿™éœ€è¦å¤§çº¦ 40 åˆ†é’Ÿæ‰èƒ½åœ¨ colab notebook ä¸­è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥åœ¨ç­‰å¾…çš„æ—¶å€™å–æ¯å’–å•¡æˆ–é˜…è¯» ğŸ¤— [Annotated Diffusion](https://huggingface.co/blog/annotated-diffusion)åšæ–‡ã€‚ä½œè€…è®­ç»ƒäº†å¤§çº¦ 3 ä¸ªå°æ—¶ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾—åˆ°çš„ç»“æœä¸ä¼šåƒä»–ä»¬çš„é‚£ä¹ˆå¥½ã€‚

```python
training_args = TrainingArguments(
    output_dir="output/",
    remove_unused_columns=False,
    num_train_epochs=120,
    per_device_train_batch_size=64,
    learning_rate=1e-4,
    weight_decay=1e-4,
    warmup_ratio=0.1,
    optim="adamw_torch",
    max_grad_norm=0.25,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    data_collator=collator,
)

trainer.train()
```

ç°åœ¨æˆ‘ä»¬è§£é‡Šäº† Decision Transformerã€Trainer èƒŒåçš„ç†è®ºï¼Œä»¥åŠå¦‚ä½•è®­ç»ƒå®ƒã€‚

## ç»“è®º

è¿™ç¯‡æ–‡ç« æ¼”ç¤ºäº†å¦‚ä½•åœ¨[ğŸ¤— æ•°æ®é›†](https://huggingface.co/docs/datasets/index)ä¸Šæ‰˜ç®¡çš„ç¦»çº¿ RL æ•°æ®é›†ä¸Šè®­ç»ƒ Decision Transformer ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªğŸ¤— transformers [Trainer](https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/decision_transformer#overview)å’Œä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ã€‚