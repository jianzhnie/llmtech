# QMIX

## 多智能体强化学习的单调值函数因子分解

在许多现实世界环境中，一组智能体必须互相协调其行为，同时以非中心化方式动作。

通常情况下是在模拟或实验室环境中以中心化方式训练智能体，在那里可以获得全局状态信息并解除通信限制。学习以状态之外信息为条件的联合动作价值是利用中心化学习的一种有吸引力的方式，但提取分散策略最佳策略尚不清楚。

我们的解决方案是QMIX，这是一种基于价值的新方法，可以以中心化的端到端方式训练分散的策略。

QMIX采用了一个网络将联合动作价值估计为仅仅基于局部观测的每个智能体价值的复杂非线性组合。我们从结构上强调，联合动作价值在每个智能体价值中是单调的，这允许在非策略学习中实现联合动作价值的可控制最大化，并确保中心化和分散策略之间的一致性。

我们在一组具有挑战性的星际争霸II 任务上对QMIX进行了评估，并表明QMIX显著优于现有的基于价值的多智能体强化学习方法。

## Introduction

在许多这样的设置中，部分可观察性和/或通信约束要求学习分布式策略，这仅取决于每个智能体的局部动作观察历史。分散的策略也自然地削弱了联合动作空间， 随着智能体数量呈指数增长的问题，通常使传统的单智能体RL方法的应用变得不切实际。

幸运的是，分散的策略通常可以在模拟或实验室环境中以分散的方式学习。这通常会授予对其他状态信息的访问权限，而不是对智能体隐藏，并消除智能体间的通信约束。中心化训练与分散执行的范例最近在RL社区引起了关注。

然而，围绕如何最好地利用中心化训练的许多挑战仍然悬而未决。其中一个挑战是如何表示和使用大多数RL方法学习的动作价值函数。一方面，正确捕捉智能体行为的影响需要一个以全局状态和联合行为为条件的中心化动作价值函数Q。另一方面，当存在多个智能体时，很难学习，即使可以学习，也无法提供任何明显的方法来提取分散的策略，允许每个智能体基于单独的观察仅选择单独的动作。

最简单的选择是放弃中心化动作价值函数，让每个智能体独立地学习一个单独的动作价值功能Qa，就像独立的Q-Learning（IQL）一样。然而，这种方法不能明确地表示智能体之间的互动，也可能无法转化，因为每个智能体的学习都会被其他人的学习和探索所混淆。

另一个极端，我们可以学习一个完全中心化状态动作价值函数Qt，然后使用它来指导 Actor-critic 框架中分散策略的选择，这是（COMA）策略梯度（Foerster et al.，2018）采取的一种方法，以及Gupta et al.（2017）的工作。然而，这需要策略学习，可能是低效的，并且当智能体数量超过一定数量时，训练完全中心化 Critic 变得不切实际.

在这两个极端之间，我们可以学习到中心化但因子化的Qt，这是一种在价值分解网络（VDN）采用的方法（Sunehag等人，2017）。通过将Qt表示为仅基于单个智能体观察和动作的单个智能体价值函数Q的总和，中心化的策略只因每个智能体对其Qa的贪婪选择而产生。然而，VDN严重限制了可以表示的中心化动作价值函数的复杂性，并忽略了训练期间可用的任何额外状态信息。

在本文中，我们提出了一种称为QMIX的新方法，它与VDN一样，位于IQL和COMA的两种极端之间，但可以表示更丰富的一类动作值函数。我们方法的关键是洞察到，VDN的完全因子化对于提取分散的策略是不必要的。相反，我们只需要确保在Qtot上进行 argmax 操作与在每个Qa上单独执行的一组的argmax 操作得到相同的结果。为此，对Qtot和每个Qa之间的关系施加单调约束就足够了：

​                            $∂ Qtot/ ∂Q ≥0,∀a.$（1）

QMIX由呈现每个Qa的 agent 网络和将它们组合成Qtot的 mixing 网络组成，不是像VDN那样简单的加和，而是以一种复杂的非线性方式确保中心化和非中心化策略之间的一致性。同时，它通过限制mixing 网络具有正权重来强制执行（1）的约束。因此，QMIX可以用因子表示法来表示复杂的中心化式动作价值函数，该因子表示法可以很好地扩展智能体的数量，并允许通过线性时间的单个 argmax 操作轻松提取中心化式策略。

我们对星际争霸II1中构建的一系列任务进行了QMIX评估（Vinyals等人，2017）。我们的实验表明，QMIX在绝对性能和学习速度方面都优于IQL和VDN。特别是，我们的方法在具有异构智能体的任务中显示出相当高的性能。此外，我们的实验表明，为了在任务中实现一致的性能，必须对状态信息进行调节，并对智能体Q值进行非线性混合。

## 相关工作

最近在多智能体RL中的工作已经开始从表格方法（Yang&amp;Gu，2004；Busoniu等人，2008）转向可以处理高维状态和动作空间的深度学习方法（Tampuu等人，2017；Foerster等人，2018；Peng等人，2017）。在本文中，我们重点讨论合作设定。

一方面，为多智能体系统寻找策略的自然方法是直接学习分散的价值函数或策略。独立Q-Learning（Tan，1993）使用Q-Learning（Watkins，1989）为每个智能体训练独立的动作价值函数。（Tampuu等人，2017）使用DQN将这种方法扩展到深度神经网络（Mnih等人，2015）。虽然这些方法很容易实现去中心化，但由于同时学习和探索智能体所导致的环境的非平稳性，这些方法容易产生不稳定性。Omidshafiei等人（2017年）和Foerster等人（2017）在一定程度上解决了学习稳定性问题，但仍然学习分散的价值函数，不允许在训练期间包含额外的状态信息。

另一方面，联合动作的中心化学习可以自然地处理协调问题并避免非平稳性，但难以扩展，因为联合动作空间在智能体数量上呈指数增长。可扩展中心化学习的经典方法包括顺序图（Guestrin等人，2002），通过将全局奖励函数分解为智能体局部项的总和，利用智能体之间的条件独立性。稀疏协作Q-Learning（Kok&amp;Vlasis，2006）是一种简单的Q-Learning算法，它学习仅在需要协调的状态下协调一组协作智能体的行为，并在协调图中对这些依赖进行编码。这些方法需要预先提供智能体之间的依赖性，而我们不需要这些先验知识。相反，我们假设每个智能体总是对全局回报做出贡献，并了解其在每个状态的贡献程度。

最近的中心化学习方法需要在执行过程中进行更多的通信：（Sukhbaatar等人，2016）使用中心化网络架构在智能体之间交换信息。BicNet（Peng等人，2017）使用双向RNN在 actor-critic 环境中的智能体之间交换信息。这种方法还需要估计单个智能体的回报。

一些工作开发了混合方法，利用中心化学习和完全分散执行的设置。COMA（Foerster等人，2018）使用一个中心化的critic来训练分散的参与者，估计每个智能体的优势函数，以解决多智能体信用分配问题。类似地，Gupta等人（2017）提出了一种带有每个智能体critic的中心化式角色critic算法，该算法随着智能体数量的增加而扩展得更好，但限制了中心化的优势。Lowe等人（2017）为每个智能体学习一个中心化的critic，并将其应用于具有连续动作空间的竞争游戏。这些方法用于策略策略梯度学习，这可能具有较差的样本效率，并且容易陷入次优局部最小值。

Sunehag等人（2017）提出了价值分解网络（VDN），该网络允许分散执行的中心化价值函数学习。他们的算法将中心状态作用值函数分解为单个智能体项的总和。这对应于使用退化断开的坐标图。VDN在训练期间不使用额外的状态信息，只能代表有限的一类中心化行动价值函数。Usenier等人（2017）提出了一种使用中心化贪婪MDP一阶优化的算法。彭等人（2017）还评估了他们在星际争霸上的方法。然而，两者都不需要分散执行。与我们的设置类似的是Foerster等人（2017）的工作，他们在多达五名特工的作战场景中评估IQL的回放稳定方法。Foerster等人（2018）也使用了这种设置。在本文中，我们在星际争霸II学习环境（SC2LE）（Vinyalseet等人，2017）中构建任务，因为它得到了游戏开发者的积极支持，SC2LE提供了一个更稳定的测试环境。QMIX依靠神经网络将中心状态转换为另一个神经网络的权重，以一种让人联想到神经网络的方式（Ha等人，2017）。通过保持权重为正，第二个神经网络被约束为相对于其输入单调。Dugas等人（2009）研究了神经网络的这种功能限制。



## 算法流程

QMIX 框架图如下

![img](https://liushunyu.github.io/img/in-post/2020-06-18-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%EF%BC%889%EF%BC%89QMIX.assets/image-20200618104937306.png)

### 主要思路

1、VDN 满足下述等式约束，而 QMIX 将该等式约束推广到了更大的单调函数约束上，因此该等式约束对 QMIX 来说是充分不必要的。

![img](https://liushunyu.github.io/img/in-post/2020-06-18-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%EF%BC%889%EF%BC%89QMIX.assets/image-20200618111341687.png)

2、QMIX 通过提出单调性假设放松了 VDN 中对单智能体的价值函数直接求和等于联合价值函数的约束限制，其中 QaQa 为单智能体的价值函数，QtotQtot 为联合价值函数，a∈Aa∈A 是智能体。

![img](https://liushunyu.github.io/img/in-post/2020-06-18-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%EF%BC%889%EF%BC%89QMIX.assets/image-20200618105713386.png)

3、其主要结构与 VDN 类似，重点修改在于引入将额外状态信息加入到单智能体的价值函数到联合价值函数的映射过程，并将其称为 mixing network。

4、mixing network 的权重和 bias 来自于 hypernetwork 的输出。

- hypernetwork 的输入为额外状态信息。
- hypernetwork 的权重生成部分主要包括一个线性与一个绝对值激活函数，以生成非负的权重，从而保证映射的单调性。
- hypernetwork 的 bias 生成部分与权重生成部分类似但不需要绝对值激活函数，而且最后的 bias 使用了用 ReLU 作激活函数的两层 hypernetwork。

5、在训练阶段生成 target 值或执行阶段生成联合动作，只需要线性的计算时间复杂度分别计算单智能体的动作再进行联合即可。

## 实验

1、在 Two-Step Game 上进行实验表明 QMIX 的逼近能力比 VDN 更强，QMIX 算法的效果更好。

2、在 Decentralised StarCraft Micromanagement 环境中进行实验

- 将环境修改为部分可观察环境

3、在消融实验中发现在同构智能体智能体中不一定需要非线形值函数分解，而在异构智能体中需要使用额外状态信息及非线性值函数分解才能实现更好的性能。

## 其他补充

1、论文中提出当任何一个智能体的最佳行动依赖于其他智能体在同一时间采取的行动的价值函数无法进行成功的分解，因此不能由 QMIX 进行表示。

2、论文将团队价值函数的等式约束推广到了单调性约束上，而且利用额外状态信息使用了 hypernetwork 来学习网络权重，这种学习权重的方式感觉比较新颖。