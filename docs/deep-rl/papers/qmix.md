# QMIX

## 多智能体强化学习的单调值函数因子分解

在许多现实世界环境中，一组智能体必须互相协调其动作，同时以非中心化方式动作。

通常情况下是在模拟或实验室环境中以中心化方式训练智能体，在那里可以获得全局状态信息并解除通信限制。学习以状态之外信息为条件的联合动作价值是利用中心化学习的一种有吸引力的方式，但提取分散策略最佳策略尚不清楚。

我们的解决方案是QMIX，这是一种基于价值的新方法，可以以中心化的端到端方式训练分散的策略。

QMIX采用了一个网络将联合动作价值估计为仅仅基于局部观测的每个智能体价值的复杂非线性组合。我们从结构上强调，联合动作价值在每个智能体价值中是单调的，这允许在非策略学习中实现联合动作价值的可控制最大化，并确保中心化和分散策略之间的一致性。

我们在一组具有挑战性的星际争霸II 任务上对QMIX进行了评估，并表明QMIX显著优于现有的基于价值的多智能体强化学习方法。

## Introduction

在许多这样的设置中，部分可观察性和/或通信约束要求学习分布式策略，这仅取决于每个智能体的局部动作观察历史。分散的策略也自然地削弱了联合动作空间， 随着智能体数量呈指数增长的问题，通常使传统的单智能体RL方法的应用变得不切实际。

幸运的是，分散的策略通常可以在模拟或实验室环境中以分散的方式学习。这通常会授予对其他状态信息的访问权限，而不是对智能体隐藏，并消除智能体间的通信约束。中心化训练与分散执行的范例最近在RL社区引起了关注。

然而，围绕如何最好地利用中心化训练的许多挑战仍然悬而未决。其中一个挑战是如何表示和使用大多数RL方法学习的动作价值函数。一方面，正确捕捉智能体动作的影响需要一个以全局状态和联合动作为条件的中心化动作价值函数Q。另一方面，当存在多个智能体时，很难学习，即使可以学习，也无法提供任何明显的方法来提取分散的策略，允许每个智能体基于单独的观察仅选择单独的动作。

最简单的选择是放弃中心化动作价值函数，让每个智能体独立地学习一个单独的动作价值功能Qa，就像独立的Q-Learning（IQL）一样。然而，这种方法不能明确地表示智能体之间的互动，也可能无法转化，因为每个智能体的学习都会被其他人的学习和探索所混淆。

另一个极端，我们可以学习一个完全中心化状态动作价值函数Qt，然后使用它来指导 Actor-critic 框架中分散策略的选择，这是（COMA）策略梯度（Foerster et al.，2018）采取的一种方法，以及Gupta et al.（2017）的工作。然而，这需要策略学习，可能是低效的，并且当智能体数量超过一定数量时，训练完全中心化 Critic 变得不切实际.

在这两个极端之间，我们可以学习到中心化但因子化的Qt，这是一种在价值分解网络（VDN）采用的方法（Sunehag等人，2017）。通过将Qt表示为仅基于单个智能体观察和动作的单个智能体价值函数Q的总和，中心化的策略只因每个智能体对其Qa的贪婪选择而产生。然而，VDN严重限制了可以表示的中心化动作价值函数的复杂性，并忽略了训练期间可用的任何额外状态信息。

在本文中，我们提出了一种称为QMIX的新方法，它与VDN一样，位于IQL和COMA的两种极端之间，但可以表示更丰富的一类动作价值函数。我们方法的关键是洞察到，VDN的完全因子化对于提取分散的策略是不必要的。相反，我们只需要确保在Qtot上进行 argmax 操作与在每个Qa上单独执行的一组的argmax 操作得到相同的结果。为此，对Qtot和每个Qa之间的关系施加单调约束就足够了：

​                            $∂ Qtot/ ∂Q ≥0,∀a.$（1）

QMIX由呈现每个Qa的 agent 网络和将它们组合成Qtot的 mixing 网络组成，不是像VDN那样简单的加和，而是以一种复杂的非线性方式确保中心化和非中心化策略之间的一致性。同时，它通过限制mixing 网络具有正权重来强制执行（1）的约束。因此，QMIX可以用因子表示法来表示复杂的中心化式动作价值函数，该因子表示法可以很好地扩展智能体的数量，并允许通过线性时间的单个 argmax 操作轻松提取中心化式策略。

我们对星际争霸II1中构建的一系列任务进行了QMIX评估（Vinyals等人，2017）。我们的实验表明，QMIX在绝对性能和学习速度方面都优于IQL和VDN。特别是，我们的方法在具有异构智能体的任务中显示出相当高的性能。此外，我们的实验表明，为了在任务中实现一致的性能，必须对状态信息进行调节，并对智能体Q值进行非线性混合。

## 相关工作

最近在多智能体RL中的工作已经开始从表格方法（Yang&amp;Gu，2004；Busoniu等人，2008）转向可以处理高维状态和动作空间的深度学习方法（Tampuu等人，2017；Foerster等人，2018；Peng等人，2017）。在本文中，我们重点讨论合作设定。

一方面，为多智能体系统寻找策略的自然方法是直接学习分散的价值函数或策略。独立Q-Learning（Tan，1993）使用Q-Learning（Watkins，1989）为每个智能体训练独立的动作价值函数。（Tampuu等人，2017）使用DQN将这种方法扩展到深度神经网络（Mnih等人，2015）。虽然这些方法很容易实现去中心化，但由于同时学习和探索智能体所导致的环境的非平稳性，这些方法容易产生不稳定性。Omidshafiei等人（2017年）和Foerster等人（2017）在一定程度上解决了学习稳定性问题，但仍然学习分散的价值函数，不允许在训练期间包含额外的状态信息。

另一方面，联合动作的中心化学习可以自然地处理协调问题并避免非平稳性，但难以扩展，因为联合动作空间在智能体数量上呈指数增长。可扩展中心化学习的经典方法包括顺序图（Guestrin等人，2002），通过将全局奖励函数分解为智能体局部项的总和，利用智能体之间的条件独立性。稀疏协作Q-Learning（Kok&amp;Vlasis，2006）是一种简单的Q-Learning算法，它学习仅在需要协调的状态下协调一组协作智能体的动作，并在协调图中对这些独立进行编码。这些方法需要预先提供智能体之间的独立性，而我们不需要这些先验知识。相反，我们假设每个智能体总是对全局回报做出贡献，并了解其在每个状态的贡献程度。

最近的中心化学习方法需要在执行过程中进行更多的通信：（Sukhbaatar等人，2016）使用中心化网络架构在智能体之间交换信息。BicNet（Peng等人，2017）使用双向RNN在 actor-critic 环境中的智能体之间交换信息。这种方法还需要估计单个智能体的回报。

一些工作开发了混合方法，利用中心化学习和完全分散执行的设置。COMA（Foerster等人，2018）使用一个中心化的critic来训练分散的参与者，估计每个智能体的优势函数，以解决多智能体信用分配问题。类似地，Gupta等人（2017）提出了一种带有每个智能体critic的中心化式角色critic算法，该算法随着智能体数量的增加而扩展得更好，但限制了中心化的优势。Lowe等人（2017）为每个智能体学习一个中心化的critic，并将其应用于具有连续动作空间的竞争游戏。这些方法用于策略策略梯度学习，这可能具有较差的样本效率，并且容易陷入次优局部最小值。

Sunehag等人（2017）提出了价值分解网络（VDN），该网络允许分散执行的中心化价值函数学习。他们的算法将中心状态作用值函数分解为单个智能体项的总和。这对应于使用退化断开的坐标图。VDN在训练期间不使用额外的状态信息，只能代表有限的一类中心化动作价值函数。Usenier等人（2017）提出了一种使用中心化贪婪MDP一阶优化的算法。彭等人（2017）还评估了他们在星际争霸上的方法。然而，两者都不需要分散执行。与我们的设置类似的是Foerster等人（2017）的工作，他们在多达五名特工的作战场景中评估IQL的回放稳定方法。Foerster等人（2018）也使用了这种设置。在本文中，我们在星际争霸II学习环境（SC2LE）（Vinyalseet等人，2017）中构建任务，因为它得到了游戏开发者的积极支持，SC2LE提供了一个更稳定的测试环境。QMIX依靠神经网络将中心状态转换为另一个神经网络的权重，以一种让人联想到神经网络的方式（Ha等人，2017）。通过保持权重为正，第二个神经网络被约束为相对于其输入单调。Dugas等人（2009）研究了神经网络的这种功能限制。

## 背景

### Deep Recurrent Q-Learning

在部分可观察的环境中，智能体可以受益于对其整个动作观察历史的调节。Hausknecht&Stone（2015）提出了利用递归神经网络的深度递归Q网络（DRQN）。通常，诸如LSTM（Hochreiter&Schmidhuber，1997）或GRU（Chung等人，2014）的门控架构用于促进更长时间步的学习。

### 独立Q-Lerning

可能在多智能体学习中最常用的方法是独立Q-learning（IQL）（Tan，1993），它将多智能体问题分解为共享相同环境的同时存在的单智能体问题的集合。这种方法没有解决由于学习智能体的策略变化而引入的非平稳性，因此，与 Q-Learning 不同，即使在无限探索的极限下，也没有收敛的保证。然而，在实践中，无论在混合和竞争性游戏中，IQL通常也是一个非常强大的基准（Tampuu等人，2017；Leibo等人，2017）。

### 价值分解网络

相比之下，价值分解网络（VDN）旨在学习联合动作价值函数 $Q_{tot}（τ，u）$，其中τ∈T^n 这是一段联合行动观察记录，也是一次联合行动。它将Qtot表示为单个值函数 Qa（τa，ua；θa）的总和，仅在单个动作-观察历记录：

$Q_{tot}（τ，u）=n∑i=1Qi（τi，ui；θi）$。（3） 

严格来说，每个Qa  is a utility 函数, 都不是一个值函数，因为它本身不对预期回报做出估计。然而，为了简化术语逻辑，我们同时引用Qtot和Qa 作为值函数。

VDN的损失函数相当于（2），其中Qi被Qtot代替。这种表示法的一个优点是，去中心化的策略只产生于每个智能体对其Qa执行贪婪的动作选择。

### QMIX

在本节中，我们提出了一种新的方法，称为QMIX，与VDN一样，它位于IQL和中心化Q-learning的极端之间，但可以表示更丰富的一类动作价值函数。我们的方法的关键在于，不需要对VDN进行充分分解，以便能够提取出与其中心化策略完全一致的去中心化策略。相反，为了保持一致性，我们只需要确保对Qtot执行的全局argmax与对每个Qa:

$$argmax Q_{tot}（τ=argmaxu1Q1（τ1，u1）…argmaxunQn（τn，un）$$（4）

这允许每个智能体仅通过选择与 Qa相关的贪婪动作来参与分散执行。作为一个副作用，如果满足（4），那么获取非策略学习更新所需的Qtot的最大值是非常容易的。

VDN的表示足以满足（4）。然而，QMIX基于这样的观察，即该表示可以推广到更大的单调函数族，这些单调函数也足够但不必满足（4）。可通过限制Qtot和每个Qa之间的关系来强制执行单调性：

为了实施（5），QMIX使用由 agent 网络、mixing 网络和一组 *hypernetworks* 组成的架构表示 Qtot（Ha等人，2017）。图2说明了总体设置。对于每个智能体a，有一个智能体网络表示其各自的值函数Qa（τa，ua）。我们将智能体网络表示为接收当前个人观察和最后行动的DRQN−如图2c所示。混合网络是一个前馈神经网络，它将智能体网络的输出作为输入，并单调混合它们，产生Qtot的值，如图2a所示。为了加强（5）的单调性约束，混合网络的权重（但不是偏差）被重新限制为非负。这允许混合网络任意接近地逼近任何单调函数（Dugas等人，2009）。

混合网络的权重由独立的超网络产生。每个超级网络都将状态作为输入，并生成混合网络一层的权重。每个超网络由一个线性层组成，然后是一个绝对值激活函数，以确保混合网络权重为非负。然后，超网络的输出是一个向量，它被重组成一个适当大小的矩阵。bias 以相同的方式产生，但不限于否定。最终偏置由具有ReLU非线性的2层超网络产生。图2a说明了混合网络和超网络。

状态由超网络使用，而不是直接传递到混合网络中，因为Qtot被允许以非单调的方式依赖于额外的状态信息。所以，通过单调网络和每个智能体值传递某些函数将是过度约束。相反，超网络的使用使得可以以任意的方式调节声子网络的权重，从而将完整状态整合到联合作用值估计中，如下所示.

![img](https://liushunyu.github.io/img/in-post/2020-06-18-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%EF%BC%889%EF%BC%89QMIX.assets/image-20200618104937306.png)

### 主要思路

1、VDN 满足下述等式约束，而 QMIX 将该等式约束推广到了更大的单调函数约束上，因此该等式约束对 QMIX 来说是充分不必要的。

![img](https://liushunyu.github.io/img/in-post/2020-06-18-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%EF%BC%889%EF%BC%89QMIX.assets/image-20200618111341687.png)

2、QMIX 通过提出单调性假设放松了 VDN 中对单智能体的价值函数直接求和等于联合价值函数的约束限制，其中 Qa为单智能体的价值函数，Qtot 为联合价值函数，a∈A 是智能体。

![img](https://liushunyu.github.io/img/in-post/2020-06-18-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%EF%BC%889%EF%BC%89QMIX.assets/image-20200618105713386.png)

3、其主要结构与 VDN 类似，重点修改在于引入将额外状态信息加入到单智能体的价值函数到联合价值函数的映射过程，并将其称为 mixing network。

4、mixing network 的权重和 bias 来自于 hypernetwork 的输出。

- hypernetwork 的输入为额外状态信息。
- hypernetwork 的权重生成部分主要包括一个线性与一个绝对值激活函数，以生成非负的权重，从而保证映射的单调性。
- hypernetwork 的 bias 生成部分与权重生成部分类似但不需要绝对值激活函数，而且最后的 bias 使用了用 ReLU 作激活函数的两层 hypernetwork。

5、在训练阶段生成 target 值或执行阶段生成联合动作，只需要线性的计算时间复杂度分别计算单智能体的动作再进行联合即可。

## 实验

1、在 Two-Step Game 上进行实验表明 QMIX 的逼近能力比 VDN 更强，QMIX 算法的效果更好。

2、在 Decentralised StarCraft Micromanagement 环境中进行实验

- 将环境修改为部分可观察环境

3、在消融实验中发现在同构智能体智能体中不一定需要非线形值函数分解，而在异构智能体中需要使用额外状态信息及非线性值函数分解才能实现更好的性能。

## 其他补充

1、论文中提出当任何一个智能体的最佳动作独立于其他智能体在同一时间采取的动作的价值函数无法进行成功的分解，因此不能由 QMIX 进行表示。

2、论文将团队价值函数的等式约束推广到了单调性约束上，而且利用额外状态信息使用了 hypernetwork 来学习网络权重，这种学习权重的方式感觉比较新颖。