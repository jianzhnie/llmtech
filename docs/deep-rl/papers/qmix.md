# QMIX

## 多智能体强化学习的单调值函数因子分解

在许多现实世界环境中，一组智能体必须互相协调其行为，同时以非集中的方式动作。

通常情况下是在模拟或实验室环境中以集中的方式训练智能体，在那里可以获得全局状态信息并解除通信限制。学习以状态之外信息为条件的联合动作价值是利用集中学习的一种有吸引力的方式，但提取分散策略最佳策略尚不清楚。

我们的解决方案是QMIX，这是一种基于价值的新方法，可以以中心化的端到端方式训练分散的策略。

QMIX采用了一个网络将联合动作价值估计为仅仅基于局部观测的每个智能体价值的复杂非线性组合。我们从结构上强调，联合动作价值在每个智能体价值中是单调的，这允许在非策略学习中实现联合动作价值的可控制最大化，并确保集中和分散策略之间的一致性。

我们在一组具有挑战性的星际争霸II 任务上对QMIX进行了评估，并表明QMIX显著优于现有的基于价值的多智能体强化学习方法。