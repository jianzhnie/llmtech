##### ![img](https://miro.medium.com/max/700/1*y-FnMby1uCDxw6Br-BeZng.png)

# MuZeroï¼šï¼ˆç¬¬ 1/3 éƒ¨åˆ†ï¼‰

åœ¨è¿™ä¸ªç”±ä¸‰éƒ¨åˆ†ç»„æˆçš„ç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢ DeepMind MuZero æ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚

ğŸ‘‰[ç¬¬2éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-2-3-f99dad7a7ad)

ğŸ‘‰[ç¬¬ 3 éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-3-3-ccea6b03538b)

å¦è¯·æŸ¥çœ‹æˆ‘çš„æœ€æ–°å¸–å­ï¼Œå…³äºå¦‚ä½•ä½¿ç”¨è‡ªæˆ‘å¯¹å¼ˆä¸ºå¤šäººæ£‹ç›˜æ¸¸æˆè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼

ğŸ‘‰[åœ¨å¤šäººç¯å¢ƒä¸­è‡ªæˆ‘å¯¹æˆ˜](https://medium.com/applied-data-science/how-to-train-ai-agents-to-play-multiplayer-games-using-self-play-deep-reinforcement-learning-247d0b440717)

2019 å¹´ 11 æœˆ 19 æ—¥ï¼ŒDeepMind å‘ä¸–ç•Œå‘å¸ƒäº†ä»–ä»¬æœ€æ–°çš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•[â€”â€”MuZero](https://arxiv.org/abs/1911.08265)ã€‚è¿™æ˜¯ DeepMind å¼ºåŒ–å­¦ä¹ è®ºæ–‡ç³»åˆ—ä¸­çš„ç¬¬å››ç¯‡ï¼Œè¿™äº›è®ºæ–‡ä» 2016 å¹´çš„ AlphaGo å¼€å§‹ï¼Œä¸æ–­çªç ´æ”¹è¿›ã€‚AlphaZero è¢«èª‰ä¸ºä¸€ç§é€šç”¨ç®—æ³•ï¼Œå¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•äººç±»ä¸“å®¶ç­–ç•¥å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å¿«é€Ÿç²¾é€šæŸäº‹ã€‚

## MuZero

[é€šè¿‡ä½¿ç”¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè§„åˆ’æ¥æŒæ¡ Atariã€å›´æ£‹ã€å›½é™…è±¡æ£‹å’Œå°†æ£‹](https://arxiv.org/abs/1911.08265)

MuZero è¿ˆå‡ºäº†æœ€ç»ˆçš„ä¸‹ä¸€æ­¥ã€‚MuZero ä¸ä»…å¦è®¤è‡ªå·±å¯ä»¥å­¦ä¹ äººç±»ç­–ç•¥ã€‚å®ƒç”šè‡³æ²¡æœ‰æ˜¾ç¤ºæ¸¸æˆè§„åˆ™ã€‚

æ¢å¥è¯è¯´ï¼Œå¯¹äºå›½é™…è±¡æ£‹ï¼ŒAlphaZero è®¾ç½®äº†ä»¥ä¸‹æŒ‘æˆ˜ï¼š

> å­¦ä¹ å¦‚ä½•è‡ªå·±ç©è¿™ä¸ªæ¸¸æˆâ€”â€”è¿™é‡Œçš„è§„åˆ™æ‰‹å†Œè§£é‡Šäº†æ¯å—æ£‹å­å¦‚ä½•ç§»åŠ¨ä»¥åŠå“ªäº›ç§»åŠ¨æ˜¯åˆæ³•çš„ã€‚å®ƒè¿˜å‘Šè¯‰æ‚¨å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªä½ç½®æ˜¯å°†æ­»ï¼ˆæˆ–å¹³å±€ï¼‰ã€‚

å¦ä¸€æ–¹é¢ï¼ŒMuZero é¢ä¸´ç€è¿™æ ·çš„æŒ‘æˆ˜ï¼š

> å­¦ä¹ å¦‚ä½•è‡ªå·±ç©è¿™ä¸ªæ¸¸æˆâ€”â€”æˆ‘ä¼šå‘Šè¯‰ä½ åœ¨å½“å‰ä½ç½®å“ªäº›åŠ¨ä½œæ˜¯åˆæ³•çš„ï¼Œä»¥åŠä¸€æ–¹è·èƒœï¼ˆæˆ–å¹³å±€ï¼‰çš„æ—¶é—´ï¼Œä½†æˆ‘ä¸ä¼šå‘Šè¯‰ä½ æ¸¸æˆçš„æ€»ä½“è§„åˆ™ã€‚

å› æ­¤ï¼Œé™¤äº†åˆ¶å®šåˆ¶èƒœç­–ç•¥å¤–ï¼ŒMuZero è¿˜å¿…é¡»å¼€å‘è‡ªå·±çš„åŠ¨æ€ç¯å¢ƒæ¨¡å‹ï¼Œä»¥ä¾¿äº†è§£å…¶é€‰æ‹©çš„å½±å“å¹¶æå‰è§„åˆ’ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œåœ¨ä¸€åœºä½ ä»æœªè¢«å‘ŠçŸ¥è§„åˆ™çš„æ¯”èµ›ä¸­ï¼Œä½ è¯•å›¾æˆä¸ºæ¯”ä¸–ç•Œå† å†›æ›´å¥½çš„ç©å®¶ã€‚MuZero æ°æ°åšåˆ°äº†è¿™ä¸€ç‚¹ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡è¯¦ç»†æµè§ˆä»£ç åº“æ¥æ¢ç´¢ MuZero å¦‚ä½•å®ç°è¿™ä¸€æƒŠäººçš„å£®ä¸¾ã€‚

## MuZero ä¼ªä»£ç 

é™¤äº† MuZero [é¢„å°æœ¬å¤–](https://arxiv.org/abs/1911.08265) DeepMind è¿˜å‘å¸ƒäº† Python[ä¼ªä»£ç ](https://arxiv.org/src/1911.08265v1/anc/pseudocode.py)ï¼Œè¯¦ç»†è¯´æ˜äº†ç®—æ³•å„éƒ¨åˆ†ä¹‹é—´çš„äº¤äº’ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŒ‰é€»è¾‘é¡ºåºåŒºåˆ†æ¯ä¸ªå‡½æ•°å’Œç±»ï¼Œæˆ‘å°†è§£é‡Šæ¯ä¸ªéƒ¨åˆ†çš„ä½œç”¨å’ŒåŸå› ã€‚æˆ‘ä»¬å‡è®¾ MuZero æ­£åœ¨å­¦ä¹ ä¸‹å›½é™…è±¡æ£‹ï¼Œä½†ä»»ä½•æ¸¸æˆçš„è¿‡ç¨‹éƒ½æ˜¯ç›¸åŒçš„ï¼Œåªæ˜¯å‚æ•°ä¸åŒã€‚æ‰€æœ‰ä»£ç å‡æ¥è‡ªå¼€æºçš„ DeepMind[ä¼ªä»£ç ](https://arxiv.org/src/1911.08265v1/anc/pseudocode.py)ã€‚

è®©æˆ‘ä»¬ä»æ•´ä¸ªè¿‡ç¨‹çš„æ¦‚è¿°å¼€å§‹ï¼Œä»å…¥å£ç‚¹å‡½æ•°å¼€å§‹ï¼Œ`muzero`.

![img](https://miro.medium.com/max/700/1*ajFyjeF-1hVbmtlAsSoT2Q.png)

> MuZero è‡ªæˆ‘å¯¹å¼ˆå’Œè®­ç»ƒè¿‡ç¨‹æ¦‚è¿°.

```python
def muzero(config: MuZeroConfig):
  storage = SharedStorage()
  replay_buffer = ReplayBuffer(config)

  for _ in range(config.num_actors):
    launch_job(run_selfplay, config, storage, replay_buffer)

  train_network(config, storage, replay_buffer)

  return storage.latest_network()
```

å‘å‡½æ•°`muzero`ä¼ é€’ä¸€ä¸ª`MuZeroConfig`å¯¹è±¡ï¼Œè¯¥å¯¹è±¡å­˜å‚¨æœ‰å…³è¿è¡Œå‚æ•°åŒ–çš„é‡è¦ä¿¡æ¯ï¼Œä¾‹å¦‚`action_space_size`ï¼ˆå¯èƒ½çš„æ“ä½œæ•°ï¼‰å’Œ`num_actors`ï¼ˆè¦å¯åŠ¨çš„å¹¶è¡Œæ¸¸æˆæ¨¡æ‹Ÿæ•°ï¼‰ã€‚

åœ¨é«˜å±‚æ¬¡ä¸Šï¼ŒMuZero ç®—æ³•æœ‰ä¸¤ä¸ªç‹¬ç«‹çš„éƒ¨åˆ†â€”â€”è‡ªæˆ‘å¯¹å¼ˆï¼ˆåˆ›å»ºæ¸¸æˆæ•°æ®ï¼‰å’Œè®­ç»ƒï¼ˆç”Ÿæˆç¥ç»ç½‘ç»œçš„æ”¹è¿›ç‰ˆæœ¬ï¼‰ã€‚`SharedStorage`å’Œ`ReplayBuffer`å¯¹è±¡å¯ä»¥è¢«ç®—æ³•çš„ä¸¤éƒ¨åˆ†è®¿é—®å¹¶åˆ†åˆ«å­˜å‚¨ç¥ç»ç½‘ç»œç‰ˆæœ¬å’Œæ¸¸æˆæ•°æ®ã€‚

## Shared Storageå’ŒReplay Buffer

è¯¥`SharedStorage`å¯¹è±¡åŒ…å«ç”¨äºä¿å­˜ç¥ç»ç½‘ç»œç‰ˆæœ¬å’Œä»å­˜å‚¨ä¸­æ£€ç´¢æœ€æ–°ç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚

```python
class SharedStorage(object):

  def __init__(self):
    self._networks = {}

  def latest_network(self) -> Network:
    if self._networks:
      return self._networks[max(self._networks.keys())]
    else:
      # policy -> uniform, value -> 0, reward -> 0
      return make_uniform_network()

  def save_network(self, step: int, network: Network):
    self._networks[step] = network
```

æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ª`ReplayBuffer`æ¥å­˜å‚¨ä»¥å‰æ¸¸æˆçš„æ•°æ®ã€‚è¿™é‡‡ç”¨ä»¥ä¸‹å½¢å¼ï¼š

```python
class ReplayBuffer(object):

  def __init__(self, config: MuZeroConfig):
    self.window_size = config.window_size
    self.batch_size = config.batch_size
    self.buffer = []

  def save_game(self, game):
    if len(self.buffer) > self.window_size:
      self.buffer.pop(0)
    self.buffer.append(game)

  ...
```

è¯·æ³¨æ„è¯¥`window_size`å‚æ•°é™åˆ¶ç¼“å†²åŒºä¸­å­˜å‚¨çš„æœ€å¤§æ¸¸æˆæ•°ã€‚åœ¨ MuZero ä¸­ï¼Œè¿™è¢«è®¾ç½®ä¸ºæœ€æ–°çš„ 1,000,000 åœºæ¯”èµ›ã€‚

## è‡ªæˆ‘å¯¹å¼ˆ (run_selfplay)

åˆ›å»ºShared Storageå’Œreplay bufferåï¼ŒMuZero å¯åŠ¨`num_actors`ç‹¬ç«‹è¿è¡Œçš„å¹¶è¡Œæ¸¸æˆç¯å¢ƒã€‚å¯¹äºå›½é™…è±¡æ£‹ï¼Œ`num_actors`è®¾ç½®ä¸º 3000ã€‚æ¯ä¸ªéƒ½è¿è¡Œä¸€ä¸ªå‡½æ•°`run_selfplay`ï¼Œä»å­˜å‚¨ä¸­è·å–æœ€æ–°ç‰ˆæœ¬çš„ç½‘ç»œï¼Œç”¨å®ƒç©æ¸¸æˆ ( `play_game`) å¹¶å°†æ¸¸æˆæ•°æ®ä¿å­˜åˆ°shared bufferã€‚

```python
# Each self-play job is independent of all others; it takes the latest network
# snapshot, produces a game and makes it available to the training job by
# writing it to a shared replay buffer.
def run_selfplay(config: MuZeroConfig, storage: SharedStorage,
                 replay_buffer: ReplayBuffer):
  while True:
    network = storage.latest_network()
    game = play_game(config, network)
    replay_buffer.save_game(game)
```

å› æ­¤ï¼Œæ€»è€Œè¨€ä¹‹ï¼ŒMuZero æ­£åœ¨ä¸è‡ªå·±è¿›è¡Œæ•°åƒåœºæ¯”èµ›ï¼Œå°†è¿™äº›æ¯”èµ›ä¿å­˜åˆ°ç¼“å†²åŒºä¸­ï¼Œç„¶åæ ¹æ®è¿™äº›æ¯”èµ›çš„æ•°æ®è¿›è¡Œè‡ªæˆ‘è®­ç»ƒã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™ä¸ AlphaZero æ²¡æœ‰ä»€ä¹ˆä¸åŒã€‚

åœ¨ç¬¬ 1 éƒ¨åˆ†çš„ç»“å°¾ï¼Œæˆ‘ä»¬å°†ä»‹ç» AlphaZero å’Œ MuZero ä¹‹é—´çš„ä¸»è¦åŒºåˆ«ä¹‹ä¸€â€”â€”ä¸ºä»€ä¹ˆ MuZero æœ‰ä¸‰ä¸ªç¥ç»ç½‘ç»œï¼Œè€Œ AlphaZero åªæœ‰ä¸€ä¸ªï¼Ÿ

## MuZero çš„ 3 ä¸ªç¥ç»ç½‘ç»œ

AlphaZero å’Œ MuZero éƒ½ä½¿ç”¨ä¸€ç§ç§°ä¸º**è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS)**çš„æŠ€æœ¯æ¥é€‰æ‹©ä¸‹ä¸€ä¸ªæœ€ä½³ç€æ³•ã€‚

æƒ³æ³•æ˜¯ï¼Œä¸ºäº†é€‰æ‹©ä¸‹ä¸€ä¸ªæœ€ä½³åŠ¨ä½œï¼Œä»å½“å‰ä½ç½®â€œæ’­æ”¾â€å¯èƒ½çš„æœªæ¥åœºæ™¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¯„ä¼°å®ƒä»¬çš„ä»·å€¼å¹¶é€‰æ‹©æœ€å¤§åŒ–æœªæ¥é¢„æœŸå€¼çš„åŠ¨ä½œã€‚è¿™ä¼¼ä¹æ˜¯æˆ‘ä»¬äººç±»åœ¨ä¸‹æ£‹æ—¶è„‘å­é‡Œåœ¨åšçš„äº‹æƒ…ï¼Œè€Œäººå·¥æ™ºèƒ½ä¹Ÿæ˜¯ä¸ºäº†åˆ©ç”¨è¿™ç§æŠ€æœ¯è€Œè®¾è®¡çš„ã€‚

ä½†æ˜¯ï¼ŒMuZero æœ‰ä¸€ä¸ªé—®é¢˜ã€‚ç”±äºå®ƒä¸çŸ¥é“æ¸¸æˆè§„åˆ™ï¼Œå®ƒä¸çŸ¥é“ç»™å®šçš„åŠ¨ä½œå°†å¦‚ä½•å½±å“æ¸¸æˆçŠ¶æ€ï¼Œå› æ­¤å®ƒæ— æ³•æƒ³è±¡ MCTS ä¸­çš„æœªæ¥åœºæ™¯ã€‚å®ƒç”šè‡³ä¸çŸ¥é“å¦‚ä½•è®¡ç®—å‡ºåœ¨ç»™å®šä½ç½®ä¸Šå“ªäº›åŠ¨ä½œæ˜¯åˆæ³•çš„ï¼Œæˆ–è€…ä¸€æ–¹æ˜¯å¦è·èƒœã€‚

> MuZero è®ºæ–‡ä¸­æƒŠäººçš„è¿›å±•è¡¨æ˜è¿™æ— å…³ç´§è¦ã€‚MuZero é€šè¿‡åœ¨è‡ªå·±çš„æƒ³è±¡ä¸­åˆ›å»ºç¯å¢ƒçš„åŠ¨æ€æ¨¡å‹å¹¶åœ¨è¯¥æ¨¡å‹ä¸­è¿›è¡Œä¼˜åŒ–æ¥å­¦ä¹ å¦‚ä½•ç©æ¸¸æˆã€‚

ä¸‹å›¾æ˜¾ç¤ºäº† AlphaZero å’Œ MuZero ä¸­ MCTS æµç¨‹çš„æ¯”è¾ƒï¼š

![img](https://miro.medium.com/max/700/1*NowOwxV5SQ9aLKbjdz41lQ.png)

è€Œ AlphaZero åªæœ‰ä¸€ä¸ªç¥ç»ç½‘ç»œï¼ˆ**é¢„æµ‹**), MuZero éœ€è¦ä¸‰ä¸ª (**é¢„æµ‹**,**åŠ¨åŠ›å­¦**,**è¡¨ç¤º**)

AlphaZero **é¢„æµ‹**ç¥ç»ç½‘ç»œçš„å·¥ä½œ`f`æ˜¯é¢„æµ‹ç»™å®šæ¸¸æˆçŠ¶æ€çš„ç­–ç•¥`p`å’Œä»·å€¼`v`ã€‚è¯¥ç­–ç•¥æ˜¯æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå€¼åªæ˜¯ä¼°è®¡æœªæ¥å¥–åŠ±çš„å€¼ã€‚æ¯æ¬¡ MCTS å‘½ä¸­ä¸€ä¸ªæœªæ¢ç´¢çš„å¶èŠ‚ç‚¹æ—¶éƒ½ä¼šè¿›è¡Œæ­¤é¢„æµ‹ï¼Œä»¥ä¾¿å®ƒå¯ä»¥ç«‹å³ä¸ºæ–°ä½ç½®åˆ†é…ä¸€ä¸ªä¼°è®¡å€¼ï¼Œå¹¶ä¸ºæ¯ä¸ªåç»­åŠ¨ä½œåˆ†é…ä¸€ä¸ªæ¦‚ç‡ã€‚è¿™äº›å€¼è¢«å›å¡«åˆ°æ ‘ä¸Šï¼Œè¿”å›åˆ°æ ¹èŠ‚ç‚¹ï¼Œå› æ­¤ç»è¿‡å¤šæ¬¡æ¨¡æ‹Ÿåï¼Œæ ¹èŠ‚ç‚¹å¯¹å½“å‰çŠ¶æ€çš„æœªæ¥å€¼æœ‰äº†å¾ˆå¥½çš„äº†è§£ï¼Œæ¢ç´¢äº†è®¸å¤šä¸åŒçš„å¯èƒ½æœªæ¥ã€‚

MuZero ä¹Ÿæœ‰ä¸€ä¸ª**é¢„æµ‹**ç¥ç»ç½‘ç»œ`f`ï¼Œä½†ç°åœ¨å®ƒè¿è¡Œçš„â€œæ¸¸æˆçŠ¶æ€â€æ˜¯ä¸€ä¸ªéšè—çš„è¡¨ç¤ºï¼ŒMuZero å­¦ä¹ å¦‚ä½•é€šè¿‡**åŠ¨æ€**ç¥ç»ç½‘ç»œè¿›åŒ–`g`ã€‚åŠ¨æ€ç½‘ç»œé‡‡ç”¨å½“å‰éšè—çŠ¶æ€`s`å’Œé€‰æ‹©çš„åŠ¨ä½œ`a`å¹¶è¾“å‡ºå¥–åŠ±`r`å’Œæ–°çŠ¶æ€ã€‚æ³¨æ„åœ¨ AlphaZero ä¸­ï¼Œå¦‚ä½•åœ¨ MCTS æ ‘ä¸­çš„çŠ¶æ€ä¹‹é—´ç§»åŠ¨åªæ˜¯è¯¢é—®ç¯å¢ƒçš„æƒ…å†µã€‚MuZero æ²¡æœ‰è¿™ä¸ªå¥¢ä¾ˆï¼Œæ‰€ä»¥éœ€è¦å»ºç«‹è‡ªå·±çš„åŠ¨æ€æ¨¡å‹ï¼

æœ€åï¼Œä¸ºäº†å°†å½“å‰è§‚å¯Ÿåˆ°çš„æ¸¸æˆçŠ¶æ€æ˜ å°„åˆ°åˆå§‹è¡¨ç¤ºï¼ŒMuZero ä½¿ç”¨ç¬¬ä¸‰ä¸ª**è¡¨ç¤º**ç¥ç»ç½‘ç»œï¼Œ`h`ã€‚

å› æ­¤ï¼ŒMuZero éœ€è¦ä¸¤ä¸ªæ¨ç†å‡½æ•°ï¼Œä»¥ä¾¿é€šè¿‡ MCTS æ ‘è¿›è¡Œé¢„æµ‹ï¼š

- `initial_inference`å¯¹äºå½“å‰çŠ¶æ€ã€‚`h`å…¶æ¬¡æ˜¯`f`ï¼ˆè¡¨ç¤ºåè·Ÿé¢„æµ‹ï¼‰ã€‚
- `recurrent_inference`ç”¨äºåœ¨ MCTS æ ‘å†…çš„çŠ¶æ€ä¹‹é—´ç§»åŠ¨ã€‚`g`å…¶æ¬¡æ˜¯`f`ï¼ˆè¡¨ç¤ºå…¶æ¬¡æ˜¯åŠ¨æ€ï¼‰ã€‚

![img](https://miro.medium.com/max/700/1*GA72IpY7ZciGshmVvtl8kQ.png)

>  MuZero ä¸­çš„ä¸¤ç§æ¨ç†

ä¼ªä»£ç ä¸­æœªæä¾›ç¡®åˆ‡çš„æ¨¡å‹ï¼Œä½†éšé™„çš„è®ºæ–‡ä¸­æä¾›äº†è¯¦ç»†è¯´æ˜ã€‚

```python
class NetworkOutput(typing.NamedTuple):
  value: float
  reward: float
  policy_logits: Dict[Action, float]
  hidden_state: List[float]


class Network(object):

  def initial_inference(self, image) -> NetworkOutput:
    # representation + prediction function
    return NetworkOutput(0, 0, {}, [])

  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:
    # dynamics + prediction function
    return NetworkOutput(0, 0, {}, [])

  def get_weights(self):
    # Returns the weights of this network.
    return []

  def training_steps(self) -> int:
    # How many steps / batches the network has been trained for.
    return 0
```

ç»¼ä¸Šæ‰€è¿°ï¼Œåœ¨ç¼ºä¹å®é™…å›½é™…è±¡æ£‹è§„åˆ™çš„æƒ…å†µä¸‹ï¼ŒMuZero åœ¨å…¶è„‘æµ·ä¸­åˆ›é€ äº†ä¸€ä¸ªå®ƒå¯ä»¥æ§åˆ¶çš„æ–°æ¸¸æˆï¼Œå¹¶ä»¥æ­¤æ¥è§„åˆ’æœªæ¥ã€‚è¿™ä¸‰ä¸ªç½‘ç»œï¼ˆ**é¢„æµ‹**ã€**åŠ¨æ€**å’Œ**è¡¨ç¤º**ï¼‰ä¸€èµ·ä¼˜åŒ–ï¼Œå› æ­¤åœ¨æƒ³è±¡ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½çš„ç­–ç•¥åœ¨çœŸå®ç¯å¢ƒä¸­ä¹Ÿè¡¨ç°è‰¯å¥½ã€‚

è¿™æ˜¯ç¬¬ 1 éƒ¨åˆ†çš„ç»“å°¾ â€” åœ¨[ç¬¬ 2 éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-2-3-f99dad7a7ad)ä¸­ï¼Œæˆ‘ä»¬å°†ä»éå†è¯¥`play_game`å‡½æ•°å¼€å§‹ï¼Œçœ‹çœ‹ MuZero å¦‚ä½•åœ¨æ¯ä¸ªå›åˆä¸­åšå‡ºå…³äºä¸‹ä¸€ä¸ªæœ€ä½³ç§»åŠ¨çš„å†³å®šã€‚

# MuZeroï¼šï¼ˆç¬¬ 2/3 éƒ¨åˆ†ï¼‰

> å¦‚æœæ‚¨æƒ³äº†è§£æœ‰å²ä»¥æ¥æœ€å¤æ‚çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿä¹‹ä¸€æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œé‚£ä¹ˆæ‚¨æ¥å¯¹åœ°æ–¹äº†ã€‚

è¿™æ˜¯è¯¥ç³»åˆ—çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œä»‹ç»äº†DeepMind ä¸ºå…¶å¼€åˆ›æ€§çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹[MuZeroå‘å¸ƒçš„](https://arxiv.org/abs/1911.08265)[ä¼ªä»£ç ](https://arxiv.org/src/1911.08265v1/anc/pseudocode.py)ã€‚

ğŸ‘‰[ç¬¬ 1 éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a)

ğŸ‘‰[ç¬¬ 3 éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-3-3-ccea6b03538b)

ä¸Šæ¬¡æˆ‘ä»¬ä»‹ç»äº† MuZero å¹¶äº†è§£äº†å®ƒä¸å®ƒçš„å“¥å“¥ AlphaZero æœ‰ä½•ä¸åŒã€‚

åœ¨æ²¡æœ‰å®é™…å›½é™…è±¡æ£‹è§„åˆ™çš„æƒ…å†µä¸‹ï¼ŒMuZero åœ¨å…¶è„‘æµ·ä¸­åˆ›é€ äº†ä¸€ä¸ªå®ƒå¯ä»¥æ§åˆ¶çš„æ–°æ¸¸æˆï¼Œå¹¶ä»¥æ­¤æ¥è§„åˆ’æœªæ¥ã€‚è¿™ä¸‰ä¸ªç½‘ç»œï¼ˆ**é¢„æµ‹**ã€**åŠ¨æ€**å’Œ**è¡¨ç¤º**ï¼‰ä¸€èµ·ä¼˜åŒ–ï¼Œå› æ­¤åœ¨æƒ³è±¡ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½çš„ç­–ç•¥åœ¨çœŸå®ç¯å¢ƒä¸­ä¹Ÿè¡¨ç°è‰¯å¥½ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥äº†è§£è¯¥`play_game`å‡½æ•°ï¼Œçœ‹çœ‹ MuZero å¦‚ä½•åœ¨æ¯ä¸ªå›åˆä¸­åšå‡ºå…³äºä¸‹ä¸€ä¸ªæœ€ä½³ç§»åŠ¨çš„å†³å®šã€‚

## ä½¿ç”¨ MuZero ç©æ¸¸æˆ (play_game)

æˆ‘ä»¬ç°åœ¨å°†é€æ­¥æ‰§è¡Œè¯¥`play_game`å‡½æ•°ï¼š

```python
# Each game is produced by starting at the initial board position, then
# repeatedly executing a Monte Carlo Tree Search to generate moves until the end
# of the game is reached.
def play_game(config: MuZeroConfig, network: Network) -> Game:
  game = config.new_game()

  while not game.terminal() and len(game.history) < config.max_moves:
    # At the root of the search tree we use the representation function to
    # obtain a hidden state given the current observation.
    root = Node(0)
    current_observation = game.make_image(-1)
    expand_node(root, game.to_play(), game.legal_actions(),
                network.initial_inference(current_observation))
    add_exploration_noise(config, root)

    # We then run a Monte Carlo Tree Search using only action sequences and the
    # model learned by the network.
    run_mcts(config, root, game.action_history(), network)
    action = select_action(config, len(game.history), root, network)
    game.apply(action)
    game.store_search_statistics(root)
  return game
```

é¦–å…ˆï¼Œ`Game`åˆ›å»ºä¸€ä¸ªæ–°å¯¹è±¡å¹¶å¯åŠ¨ä¸»æ¸¸æˆå¾ªç¯ã€‚å½“æ»¡è¶³ç»ˆæ­¢æ¡ä»¶æˆ–ç§»åŠ¨æ¬¡æ•°è¶…è¿‡å…è®¸çš„æœ€å¤§å€¼æ—¶ï¼Œæ¸¸æˆç»“æŸã€‚

æˆ‘ä»¬ä»æ ¹èŠ‚ç‚¹å¼€å§‹ MCTS æ ‘ã€‚

```
root = Node(0)
```

æ¯ä¸ªèŠ‚ç‚¹å­˜å‚¨ä¸å…¶è¢«è®¿é—®æ¬¡æ•°ç›¸å…³çš„å…³é”®ç»Ÿè®¡ä¿¡æ¯`visit_count`ï¼Œè½®åˆ°å®ƒ`to_play`ï¼Œé€‰æ‹©å¯¼è‡´è¯¥èŠ‚ç‚¹çš„åŠ¨ä½œçš„é¢„æµ‹å…ˆéªŒæ¦‚ç‡ï¼ŒèŠ‚ç‚¹`prior`çš„å›å¡«å€¼æ€»å’Œ`node_sum`ï¼Œå…¶å­èŠ‚ç‚¹`children`ï¼Œéšè—çŠ¶æ€å®ƒå¯¹åº”äº`hidden_state`é€šè¿‡ç§»åŠ¨åˆ°è¯¥èŠ‚ç‚¹è€Œè·å¾—çš„é¢„æµ‹å¥–åŠ±`reward`ã€‚

```python
class Node(object):

  def __init__(self, prior: float):
    self.visit_count = 0
    self.to_play = -1
    self.prior = prior
    self.value_sum = 0
    self.children = {}
    self.hidden_state = None
    self.reward = 0

  def expanded(self) -> bool:
    return len(self.children) > 0

  def value(self) -> float:
    if self.visit_count == 0:
      return 0
    return self.value_sum / self.visit_count
```

æ¥ä¸‹æ¥æˆ‘ä»¬è¦æ±‚æ¸¸æˆè¿”å›å½“å‰è§‚å¯Ÿï¼ˆå¯¹åº”`o`äºä¸Šå›¾ä¸­ï¼‰......

```
current_observation = game.make_image(-1)
```

â€¦å¹¶ä½¿ç”¨æ¸¸æˆæä¾›çš„å·²çŸ¥åˆæ³•è¡Œä¸ºå’Œå‡½æ•°æä¾›çš„å…³äºå½“å‰è§‚å¯Ÿçš„æ¨æ–­æ¥æ‰©å±•æ ¹èŠ‚ç‚¹`initial_inference`ã€‚

```
expand_node(root, game.to_play(), game.legal_actions(),network.initial_inference(current_observation))
```

```python
# We expand a node using the value, reward and policy prediction obtained from
# the neural network.
def expand_node(node: Node, to_play: Player, actions: List[Action],
                network_output: NetworkOutput):
  node.to_play = to_play
  node.hidden_state = network_output.hidden_state
  node.reward = network_output.reward
  policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}
  policy_sum = sum(policy.values())
  for action, p in policy.items():
    node.children[action] = Node(p / policy_sum)
```

æˆ‘ä»¬è¿˜éœ€è¦å‘æ ¹èŠ‚ç‚¹åŠ¨ä½œæ·»åŠ æ¢ç´¢å™ªéŸ³â€”â€”è¿™å¯¹äºç¡®ä¿ MCTS æ¢ç´¢ä¸€ç³»åˆ—å¯èƒ½çš„åŠ¨ä½œè€Œä¸æ˜¯ä»…ä»…æ¢ç´¢å®ƒå½“å‰è®¤ä¸ºæœ€ä½³çš„åŠ¨ä½œå¾ˆé‡è¦ã€‚å¯¹äºå›½é™…è±¡æ£‹ï¼Œ`root_dirichlet_alpha`= 0.3ã€‚

```
add_exploration_noise(config, root)
```

````python
# At the start of each search, we add dirichlet noise to the prior of the root
# to encourage the search to explore new actions.
def add_exploration_noise(config: MuZeroConfig, node: Node):
  actions = list(node.children.keys())
  noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))
  frac = config.root_exploration_fraction
  for a, n in zip(actions, noise):
    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac
````

æˆ‘ä»¬ç°åœ¨è¿›å…¥ä¸»è¦çš„ MCTS è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä»‹ç»ã€‚

```
run_mcts(config, root, game.action_history(), network)
```

## MuZero ä¸­çš„è’™ç‰¹å¡ç½—æœç´¢æ ‘ (run_mcts)

```python
# Core Monte Carlo Tree Search algorithm.
# To decide on an action, we run N simulations, always starting at the root of
# the search tree and traversing the tree according to the UCB formula until we
# reach a leaf node.
def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,
             network: Network):
  min_max_stats = MinMaxStats(config.known_bounds)

  for _ in range(config.num_simulations):
    history = action_history.clone()
    node = root
    search_path = [node]

    while node.expanded():
      action, node = select_child(config, node, min_max_stats)
      history.add_action(action)
      search_path.append(node)

    # Inside the search tree we use the dynamics function to obtain the next
    # hidden state given an action and the previous hidden state.
    parent = search_path[-2]
    network_output = network.recurrent_inference(parent.hidden_state,
                                                 history.last_action())
    expand_node(node, history.to_play(), history.action_space(), network_output)

    backpropagate(search_path, network_output.value, history.to_play(),
                  config.discount, min_max_stats)
```



ç”±äº MuZero ä¸äº†è§£ç¯å¢ƒè§„åˆ™ï¼Œå› æ­¤å®ƒä¹Ÿä¸çŸ¥é“åœ¨æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­å¯èƒ½è·å¾—çš„å¥–åŠ±ç•Œé™ã€‚åˆ›å»ºè¯¥`MinMaxStats`å¯¹è±¡æ˜¯ä¸ºäº†å­˜å‚¨æœ‰å…³å½“å‰é‡åˆ°çš„æœ€å°å’Œæœ€å¤§å¥–åŠ±çš„ä¿¡æ¯ï¼Œä»¥ä¾¿ MuZero å¯ä»¥ç›¸åº”åœ°è§„èŒƒåŒ–å…¶ä»·å€¼è¾“å‡ºã€‚æˆ–è€…ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å›½é™…è±¡æ£‹ (-1, 1) ç­‰æ¸¸æˆçš„å·²çŸ¥è¾¹ç•Œå¯¹å…¶è¿›è¡Œåˆå§‹åŒ–ã€‚

ä¸» MCTS å¾ªç¯è¿­ä»£`num_simulations`ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡æ‹Ÿæ˜¯é€šè¿‡ MCTS æ ‘ç›´åˆ°åˆ°è¾¾å¶èŠ‚ç‚¹ï¼ˆå³æœªæ¢ç´¢çš„èŠ‚ç‚¹ï¼‰å’Œéšåçš„åå‘ä¼ æ’­ã€‚ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæ¨¡æ‹Ÿã€‚

é¦–å…ˆï¼Œ`history`ä½¿ç”¨ä»æ¸¸æˆå¼€å§‹è‡³ä»Šé‡‡å–çš„è¡ŒåŠ¨åˆ—è¡¨è¿›è¡Œåˆå§‹åŒ–ã€‚å½“å‰`node`æ˜¯`root`èŠ‚ç‚¹å¹¶ä¸”`search_path`ä»…åŒ…å«å½“å‰èŠ‚ç‚¹ã€‚

ç„¶åæ¨¡æ‹Ÿå¦‚ä¸‹å›¾æ‰€ç¤ºè¿›è¡Œï¼š

![img](https://miro.medium.com/max/700/1*Qyy9JuAoJXqPPs2ILM9mEw.png)

MuZero é¦–å…ˆå‘ä¸‹éå† MCTS æ ‘ï¼Œå§‹ç»ˆé€‰æ‹© UCBï¼ˆç½®ä¿¡ä¸Šé™ï¼‰å¾—åˆ†æœ€é«˜çš„åŠ¨ä½œï¼š

```python
# Select the child with the highest UCB score.
def select_child(config: MuZeroConfig, node: Node,
                 min_max_stats: MinMaxStats):
  _, action, child = max(
      (ucb_score(config, node, child, min_max_stats), action,
       child) for action, child in node.children.items())
  return action, child
```

`Q(s,a)`UCB åˆ†æ•°æ˜¯æ ¹æ®é€‰æ‹©åŠ¨ä½œçš„å…ˆéªŒæ¦‚ç‡`P(s,a)`å’Œå·²ç»é€‰æ‹©åŠ¨ä½œçš„æ¬¡æ•°æ¥å¹³è¡¡åŠ¨ä½œçš„ä¼°è®¡å€¼å’Œæ¢ç´¢å¥–åŠ±çš„åº¦é‡`N(s,a)`ã€‚

![img](https://miro.medium.com/max/700/1*6IdoEqOEWPY_RztCuuVA8Q.png)

åœ¨ MCTS æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹é€‰æ‹©å…·æœ‰æœ€é«˜ UCB åˆ†æ•°çš„åŠ¨ä½œã€‚

åœ¨æ¨¡æ‹Ÿçš„æ—©æœŸï¼Œæ¢ç´¢å¥–åŠ±å ä¸»å¯¼åœ°ä½ï¼Œä½†éšç€æ¨¡æ‹Ÿæ€»æ•°çš„å¢åŠ ï¼Œä»·å€¼é¡¹å˜å¾—æ›´åŠ é‡è¦ã€‚

æœ€ç»ˆï¼Œè¯¥è¿‡ç¨‹å°†åˆ°è¾¾å¶èŠ‚ç‚¹ï¼ˆå°šæœªæ‰©å±•çš„èŠ‚ç‚¹ï¼Œå› æ­¤æ²¡æœ‰å­èŠ‚ç‚¹ï¼‰ã€‚

æ­¤æ—¶ï¼Œ`recurrent_inference`åœ¨å¶èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ä¸Šè°ƒç”¨è¯¥å‡½æ•°ï¼Œä»¥è·å¾—é¢„æµ‹çš„å¥–åŠ±å’Œæ–°çš„éšè—çŠ¶æ€ï¼ˆæ¥è‡ª**åŠ¨æ€**ç½‘ç»œï¼‰ä»¥åŠæ–°éšè—çŠ¶æ€çš„ç­–ç•¥å’Œå€¼ï¼ˆæ¥è‡ª**é¢„æµ‹**ç½‘ç»œï¼‰ã€‚

![img](https://miro.medium.com/max/700/1*GMCWZDlwiD-IJMH2sTdovg.png)

>  MCTSè¿‡ç¨‹ï¼ˆå¶å­æ‰©å±•å’Œåå‘ä¼ æ’­ï¼‰

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¶èŠ‚ç‚¹ç°åœ¨é€šè¿‡åˆ›å»ºæ–°çš„å­èŠ‚ç‚¹ï¼ˆä¸€ä¸ªç”¨äºæ¸¸æˆä¸­çš„æ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œï¼‰å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…å…¶å„è‡ªçš„å…ˆéªŒç­–ç•¥æ¥æ‰©å±•ã€‚è¯·æ³¨æ„ï¼ŒMuZero ä¸ä¼šæ£€æŸ¥è¿™äº›åŠ¨ä½œä¸­çš„å“ªäº›æ˜¯åˆæ³•çš„ï¼Œæˆ–è€…è¯¥åŠ¨ä½œæ˜¯å¦å¯¼è‡´æ¸¸æˆç»“æŸï¼ˆå®ƒä¸èƒ½ï¼‰ï¼Œå› æ­¤ä¸ºæ¯ä¸ªåŠ¨ä½œåˆ›å»ºä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ— è®ºå®ƒæ˜¯å¦åˆæ³•ã€‚

æœ€åï¼Œç½‘ç»œé¢„æµ‹çš„å€¼æ²¿ç€æœç´¢è·¯å¾„åå‘ä¼ æ’­åˆ°æ ‘ä¸Šã€‚

```python
# At the end of a simulation, we propagate the evaluation all the way up the
# tree to the root.
def backpropagate(search_path: List[Node], value: float, to_play: Player,
                  discount: float, min_max_stats: MinMaxStats):
  for node in search_path:
    node.value_sum += value if node.to_play == to_play else -value
    node.visit_count += 1
    min_max_stats.update(node.value())

    value = node.reward + discount * value
```

è¯·æ³¨æ„å€¼æ˜¯å¦‚ä½•æ ¹æ®è½®åˆ°è°æ¥ç¿»è½¬çš„ï¼ˆå¦‚æœå¶èŠ‚ç‚¹å¯¹äºåº”è¯¥ç©çš„ç©å®¶æ˜¯æ­£æ•°ï¼Œé‚£ä¹ˆå¯¹äºå¦ä¸€ä¸ªç©å®¶æ¥è¯´å®ƒå°†æ˜¯è´Ÿæ•°ï¼‰ã€‚æ­¤å¤–ï¼Œç”±äºé¢„æµ‹ç½‘ç»œé¢„æµ‹*æœªæ¥*å€¼ï¼Œåœ¨æœç´¢è·¯å¾„ä¸Šæ”¶é›†çš„å¥–åŠ±è¢«æ”¶é›†èµ·æ¥å¹¶æ·»åŠ åˆ°æŠ˜æ‰£å¶èŠ‚ç‚¹å€¼ä¸­ï¼Œå› ä¸ºå®ƒè¢«ä¼ æ’­å›æ ‘ã€‚

è¯·è®°ä½ï¼Œç”±äºè¿™äº›æ˜¯*é¢„æµ‹*çš„å¥–åŠ±ï¼Œè€Œä¸æ˜¯æ¥è‡ªç¯å¢ƒçš„å®é™…å¥–åŠ±ï¼Œå› æ­¤å³ä½¿å¯¹äºåƒå›½é™…è±¡æ£‹è¿™æ ·çš„æ¸¸æˆæ¥è¯´ï¼Œå¥–åŠ±çš„æ”¶é›†ä¹Ÿæ˜¯ç›¸å…³çš„ï¼Œåœ¨è¿™ç§æ¸¸æˆä¸­ï¼ŒçœŸæ­£çš„å¥–åŠ±åªåœ¨æ¸¸æˆç»“æŸæ—¶æ‰ä¼šé¢å‘ã€‚MuZero æ­£åœ¨ç©è‡ªå·±æƒ³è±¡çš„æ¸¸æˆï¼Œå…¶ä¸­å¯èƒ½åŒ…æ‹¬ä¸´æ—¶å¥–åŠ±ï¼Œå³ä½¿å®ƒæ‰€æ¨¡ä»¿çš„æ¸¸æˆæ²¡æœ‰ã€‚

è¿™æ ·å°±å®Œæˆäº†ä¸€æ¬¡MCTSè¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚

é€šè¿‡æ ‘å`num_simulations`ï¼Œè¿›ç¨‹åœæ­¢ï¼Œå¹¶æ ¹æ®è®¿é—®æ ¹çš„æ¯ä¸ªå­èŠ‚ç‚¹çš„æ¬¡æ•°é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚

```python
def select_action(config: MuZeroConfig, num_moves: int, node: Node,
                  network: Network):
  visit_counts = [
      (child.visit_count, action) for action, child in node.children.items()
  ]
  t = config.visit_softmax_temperature_fn(
      num_moves=num_moves, training_steps=network.training_steps())
  _, action = softmax_sample(visit_counts, t)
  return action

def visit_softmax_temperature(num_moves, training_steps):
  if num_moves < 30:
    return 1.0
  else:
    return 0.0  # Play according to the max.
```

å¯¹äºå‰ 30 ä¸ªåŠ¨ä½œï¼Œsoftmax çš„æ¸©åº¦è®¾ç½®ä¸º 1ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªåŠ¨ä½œçš„é€‰æ‹©æ¦‚ç‡ä¸å®ƒè¢«è®¿é—®çš„æ¬¡æ•°æˆæ­£æ¯”ã€‚ä»ç¬¬ 30 æ­¥å¼€å§‹ï¼Œé€‰æ‹©è®¿é—®æ¬¡æ•°æœ€å¤šçš„åŠ¨ä½œã€‚

![img](https://miro.medium.com/max/217/1*Gavp5A6AgiyzgEF43QBWfQ.png)

softmax_sampleï¼šä»æ ¹èŠ‚ç‚¹é€‰æ‹©åŠ¨ä½œ'alpha'çš„æ¦‚ç‡ï¼ˆNä¸ºè®¿é—®æ¬¡æ•°ï¼‰

è™½ç„¶è®¿é—®æ¬¡æ•°å¯èƒ½æ„Ÿè§‰æ˜¯ä¸€ä¸ªå¥‡æ€ªçš„æŒ‡æ ‡æ¥é€‰æ‹©æœ€ç»ˆè¡ŒåŠ¨ï¼Œä½†äº‹å®å¹¶éå¦‚æ­¤ï¼Œå› ä¸º MCTS æµç¨‹ä¸­çš„ UCB é€‰æ‹©æ ‡å‡†æ—¨åœ¨æœ€ç»ˆèŠ±æ›´å¤šæ—¶é—´æ¢ç´¢å®ƒè®¤ä¸ºçœŸæ­£é«˜ä»·å€¼æœºä¼šçš„è¡ŒåŠ¨ï¼Œä¸€æ—¦å®ƒåœ¨æ­¤è¿‡ç¨‹çš„æ—©æœŸå……åˆ†æ¢ç´¢äº†æ›¿ä»£æ–¹æ¡ˆã€‚

ç„¶åå°†æ‰€é€‰æ“ä½œåº”ç”¨äºçœŸå®ç¯å¢ƒï¼Œå¹¶å°†ç›¸å…³å€¼é™„åŠ åˆ°ä»¥ä¸‹åˆ—è¡¨ä¸­`game`ç›®çš„ã€‚

- `game.rewards`â€” æ¸¸æˆæ¯å›åˆæ”¶åˆ°çš„çœŸå®å¥–åŠ±åˆ—è¡¨
- `game.history`â€” æ¸¸æˆæ¯å›åˆé‡‡å–çš„è¡ŒåŠ¨åˆ—è¡¨
- `game.child_visits`â€” åœ¨æ¸¸æˆçš„æ¯ä¸€è½®ä»æ ¹èŠ‚ç‚¹å¼€å§‹çš„è¡ŒåŠ¨æ¦‚ç‡åˆ†å¸ƒåˆ—è¡¨
- `game.root_values`â€” æ¸¸æˆæ¯ä¸€è½®æ ¹èŠ‚ç‚¹çš„å€¼åˆ—è¡¨

è¿™äº›åˆ—è¡¨å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä»¬æœ€ç»ˆå°†ç”¨äºæ„å»ºç¥ç»ç½‘ç»œçš„è®­ç»ƒæ•°æ®ï¼

è¿™ä¸ªè¿‡ç¨‹ç»§ç»­è¿›è¡Œï¼Œæ¯å›åˆä»å¤´å¼€å§‹åˆ›å»ºä¸€ä¸ªæ–°çš„ MCTS æ ‘ï¼Œå¹¶ç”¨å®ƒæ¥é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œç›´åˆ°æ¸¸æˆç»“æŸã€‚

æ‰€æœ‰æ¸¸æˆæ•°æ® ( `rewards`, `history`, `child_visits`, `root_values`) éƒ½ä¿å­˜åˆ°é‡æ’­ç¼“å†²åŒºï¼Œç„¶åæ¼”å‘˜å¯ä»¥è‡ªç”±åœ°å¼€å§‹æ–°æ¸¸æˆã€‚

å‘¸ã€‚

è¿™æ˜¯ç¬¬ 2 éƒ¨åˆ†çš„ç»“å°¾ï¼Œå®Œæ•´ä»‹ç»äº† MuZero å¦‚ä½•å¯¹è‡ªå·±ç©æ¸¸æˆå¹¶å°†æ¸¸æˆæ•°æ®ä¿å­˜åˆ°ç¼“å†²åŒºã€‚

åœ¨[ç¬¬ 3 éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-3-3-ccea6b03538b)ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£ MuZero å¦‚ä½•æ ¹æ®ä»¥å‰æ¸¸æˆä¸­ä¿å­˜çš„æ•°æ®æ¥è®­ç»ƒè‡ªå·±è¿›è¡Œæ”¹è¿›ã€‚æˆ‘è¿˜å°†æ€»ç»“ä¸ºä»€ä¹ˆæˆ‘è®¤ä¸º MuZero æ˜¯ AI çš„é‡å¤§è¿›æ­¥ä»¥åŠå¯¹è¯¥é¢†åŸŸæœªæ¥çš„å½±å“ã€‚



# MuZeroï¼šç¬¬ 3/3 éƒ¨åˆ†ï¼‰

è¿™æ˜¯DeepMind ä¸ºå…¶çªç ´æ€§çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹[MuZeroå‘å¸ƒçš„](https://arxiv.org/abs/1911.08265)[ä¼ªä»£ç ](https://arxiv.org/src/1911.08265v1/anc/pseudocode.py)ç³»åˆ—çš„ç¬¬ä¸‰éƒ¨åˆ†ä¹Ÿæ˜¯æœ€åä¸€éƒ¨åˆ†ã€‚

ğŸ‘‰[ç¬¬ 1 éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a)

ğŸ‘‰[ç¬¬2éƒ¨åˆ†](https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-2-3-f99dad7a7ad)

ä¸Šæ¬¡ï¼Œæˆ‘ä»¬æµè§ˆäº†è¿™ä¸ª`play_game`å‡½æ•°ï¼Œçœ‹åˆ°äº† MuZero å¦‚ä½•åœ¨æ¯ä¸ªå›åˆä¸­åšå‡ºä¸‹ä¸€ä¸ªæœ€ä½³åŠ¨ä½œçš„å†³å®šã€‚æˆ‘ä»¬è¿˜æ›´è¯¦ç»†åœ°æ¢è®¨äº† MCTS è¿‡ç¨‹ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£ MuZero çš„è®­ç»ƒè¿‡ç¨‹å¹¶äº†è§£å®ƒè¯•å›¾æœ€å°åŒ–çš„æŸå¤±å‡½æ•°ã€‚

æˆ‘å°†æ€»ç»“ä¸ºä»€ä¹ˆæˆ‘è®¤ä¸º MuZero æ˜¯ AI çš„é‡å¤§è¿›æ­¥ä»¥åŠå¯¹è¯¥é¢†åŸŸæœªæ¥çš„å½±å“ã€‚

## è®­ç»ƒï¼ˆtrain_networkï¼‰

åŸå§‹å…¥å£å‡½æ•°çš„æœ€åä¸€è¡Œï¼ˆè¿˜è®°å¾—ç¬¬ 1 éƒ¨åˆ†ä¸­çš„é‚£ä¸€è¡Œå—ï¼Ÿï¼‰å¯åŠ¨äº†`train_network`ä½¿ç”¨é‡æ”¾ç¼“å†²åŒºä¸­çš„æ•°æ®æŒç»­è®­ç»ƒç¥ç»ç½‘ç»œçš„è¿‡ç¨‹ã€‚

```python
def train_network(config: MuZeroConfig, storage: SharedStorage,
                  replay_buffer: ReplayBuffer):
  network = Network()
  learning_rate = config.lr_init * config.lr_decay_rate**(
      tf.train.get_global_step() / config.lr_decay_steps)
  optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)

  for i in range(config.training_steps):
    if i % config.checkpoint_interval == 0:
      storage.save_network(i, network)
    batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)
    update_weights(optimizer, network, batch, config.weight_decay)
  storage.save_network(config.training_steps, network)
```

å®ƒé¦–å…ˆåˆ›å»ºä¸€ä¸ªæ–°`Network`å¯¹è±¡ï¼ˆå­˜å‚¨ MuZero çš„ä¸‰ä¸ªç¥ç»ç½‘ç»œçš„éšæœºåˆå§‹åŒ–å®ä¾‹ï¼‰å¹¶æ ¹æ®å·²å®Œæˆçš„è®­ç»ƒæ­¥éª¤æ•°å°†å­¦ä¹ ç‡è®¾ç½®ä¸ºè¡°å‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œå®ƒå°†è®¡ç®—æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æƒé‡æ›´æ–°çš„å¹…åº¦å’Œæ–¹å‘ã€‚

è¯¥å‡½æ•°çš„æœ€åä¸€éƒ¨åˆ†åªæ˜¯å¾ªç¯`training_steps`ï¼ˆåœ¨è®ºæ–‡ä¸­ä¸º 1,000,000ï¼Œå¯¹äºå›½é™…è±¡æ£‹ï¼‰ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œå®ƒéƒ½ä¼šä»é‡æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·ä¸€æ‰¹ä½ç½®ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥æ›´æ–°ç½‘ç»œï¼Œ`checkpoint_interval`æ¯æ‰¹ï¼ˆ=1000ï¼‰å°†å…¶ä¿å­˜åˆ°å­˜å‚¨ä¸­ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®¨è®ºä¸¤ä¸ªæœ€åçš„éƒ¨åˆ†â€”â€”MuZero å¦‚ä½•åˆ›å»ºä¸€æ‰¹è®­ç»ƒæ•°æ®ï¼Œä»¥åŠå®ƒå¦‚ä½•ä½¿ç”¨å®ƒæ¥æ›´æ–°ä¸‰ä¸ªç¥ç»ç½‘ç»œçš„æƒé‡ã€‚

## åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡ (replay_buffer.sample_batch)

ReplayBuffer ç±»åŒ…å«ä¸€ä¸ª`sample_batch`ä»ç¼“å†²åŒºä¸­æŠ½å–ä¸€æ‰¹è§‚å¯Ÿå€¼çš„æ–¹æ³•ï¼š

```python
class ReplayBuffer(object):
    def __init__(self, config: MuZeroConfig):
      self.window_size = config.window_size
      self.batch_size = config.batch_size
      self.buffer = []
    
    def sample_batch(self, num_unroll_steps: int, td_steps: int):
      games = [self.sample_game() for _ in range(self.batch_size)]
      game_pos = [(g, self.sample_position(g)) for g in games]
      return [(g.make_image(i), g.history[i:i + num_unroll_steps],
               g.make_target(i, num_unroll_steps, td_steps, g.to_play()))
              for (g, i) in game_pos]
    
    ...
```

å›½é™…è±¡æ£‹çš„ MuZeroé»˜è®¤`batch_size`ä¸º 2048ã€‚æ­¤å±€æ•°æ˜¯ä»ç¼“å†²åŒºä¸­é€‰æ‹©çš„ï¼Œå¹¶ä»æ¯ä¸ªå±€ä¸­é€‰æ‹©ä¸€ä¸ªä½ç½®ã€‚

å•ä¸ª`batch`æ˜¯å…ƒç»„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç»„ç”±ä¸‰ä¸ªå…ƒç´ ç»„æˆï¼š

- `g.make_image(i)`â€” æ‰€é€‰ä½ç½®çš„è§‚å¯Ÿ
- `g.history[i:i + num_unroll_steps]`â€” æ‰€é€‰ä½ç½®ä¹‹åé‡‡å–çš„ä¸‹ä¸€æ­¥`num_unroll_steps`è¡ŒåŠ¨çš„åˆ—è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
- `g.make_target(i, num_unroll_steps, td_steps, g.to_play()`â€” å°†ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ç›®æ ‡åˆ—è¡¨ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼š`target_value`,`target_reward`å’Œ`target_policy`ã€‚

ä¸‹é¢æ˜¾ç¤ºäº†ç¤ºä¾‹æ‰¹å¤„ç†çš„å›¾è¡¨ï¼Œå…¶ä¸­`num_unroll_steps`= 5ï¼ˆMuZero ä½¿ç”¨çš„é»˜è®¤å€¼ï¼‰ï¼š

![img](https://miro.medium.com/max/1400/1*49FI1Uw0p7B_64xvEThveA.png)

ç¤ºä¾‹æ‰¹æ¬¡

æ‚¨å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæ¯æ¬¡è§‚å¯Ÿéƒ½éœ€è¦å¤šä¸ªæœªæ¥è¡ŒåŠ¨ã€‚åŸå› æ˜¯æˆ‘ä»¬éœ€è¦è®­ç»ƒæˆ‘ä»¬çš„åŠ¨æ€ç½‘ç»œï¼Œè€Œå”¯ä¸€çš„æ–¹æ³•æ˜¯è®­ç»ƒå°çš„é¡ºåºæ•°æ®æµã€‚

å¯¹äºæ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªè§‚å¯Ÿï¼Œæˆ‘ä»¬å°†`num_unroll_steps`ä½¿ç”¨æä¾›çš„æ“ä½œå°†å¤´å¯¸â€œå±•å¼€â€åˆ°æœªæ¥ã€‚å¯¹äºåˆå§‹ä½ç½®ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¯¥`initial_inference`å‡½æ•°æ¥é¢„æµ‹ä»·å€¼ã€å¥–åŠ±å’Œæ”¿ç­–ï¼Œå¹¶å°†è¿™äº›ä¸ç›®æ ‡ä»·å€¼ã€ç›®æ ‡å¥–åŠ±å’Œç›®æ ‡æ”¿ç­–è¿›è¡Œæ¯”è¾ƒã€‚å¯¹äºåç»­åŠ¨ä½œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¯¥`recurrent_inference`å‡½æ•°æ¥é¢„æµ‹ä»·å€¼ã€å¥–åŠ±å’Œæ”¿ç­–ï¼Œå¹¶ä¸ç›®æ ‡ä»·å€¼ã€ç›®æ ‡å¥–åŠ±å’Œç›®æ ‡æ”¿ç­–è¿›è¡Œæ¯”è¾ƒã€‚è¿™æ ·ï¼Œæ‰€æœ‰ä¸‰ä¸ªç½‘ç»œéƒ½ç”¨äºé¢„æµ‹è¿‡ç¨‹ï¼Œå› æ­¤æ‰€æœ‰ä¸‰ä¸ªç½‘ç»œä¸­çš„æƒé‡éƒ½å°†æ›´æ–°ã€‚

ç°åœ¨è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°äº†è§£å¦‚ä½•è®¡ç®—ç›®æ ‡ã€‚

```python
class Game(object):
  """A single episode of interaction with the environment."""

  def __init__(self, action_space_size: int, discount: float):
    self.environment = Environment()  # Game specific environment.
    self.history = []
    self.rewards = []
    self.child_visits = []
    self.root_values = []
    self.action_space_size = action_space_size
    self.discount = discount

  def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,
                  to_play: Player):
    # The value target is the discounted root value of the search tree N steps
    # into the future, plus the discounted sum of all rewards until then.
    targets = []
    for current_index in range(state_index, state_index + num_unroll_steps + 1):
      bootstrap_index = current_index + td_steps
      if bootstrap_index < len(self.root_values):
        value = self.root_values[bootstrap_index] * self.discount**td_steps
      else:
        value = 0

      for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):
        value += reward * self.discount**i  # pytype: disable=unsupported-operands

      if current_index < len(self.root_values):
        targets.append((value, self.rewards[current_index],
                        self.child_visits[current_index]))
      else:
        # States past the end of games are treated as absorbing states.
        targets.append((0, 0, []))
    return targets
  
  ...
view rawpseudocode.py hosted with â¤ by GitHub
```

è¯¥å‡½æ•°ä½¿ç”¨ TD-learning çš„æ€æƒ³æ¥è®¡ç®—ä½ç½®ä»åˆ°`make_target`çš„æ¯ä¸ªçŠ¶æ€çš„ç›®æ ‡å€¼ã€‚å˜é‡`state_index` `state_index + num_unroll_steps``current_index.`

TD-learning æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­å¸¸ç”¨çš„æŠ€æœ¯â€”â€”å…¶æ€æƒ³æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªä½ç½®çš„ä¼°è®¡æŠ˜æ‰£å€¼æ›´æ–°çŠ¶æ€å€¼`td_steps`åˆ°ä¸ä¹…çš„å°†æ¥åŠ ä¸Šåˆ°é‚£æ—¶ä¸ºæ­¢çš„æŠ˜æ‰£å¥–åŠ±ï¼Œè€Œä¸æ˜¯ä»…ä»…ä½¿ç”¨æƒ…èŠ‚ç»“æŸæ—¶ç´¯ç§¯çš„æ€»æŠ˜æ‰£å¥–åŠ±ã€‚

å½“æˆ‘ä»¬æ ¹æ®ä¼°è®¡å€¼æ›´æ–°ä¼°è®¡å€¼æ—¶ï¼Œæˆ‘ä»¬è¯´æˆ‘ä»¬åœ¨**è‡ªä¸¾**ã€‚æ˜¯æœªæ¥å¤´å¯¸çš„`bootstrap_index`æŒ‡æ•°`td_steps`ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä¼°è®¡çœŸå®çš„æœªæ¥å›æŠ¥ã€‚

è¯¥å‡½æ•°é¦–å…ˆæ£€æŸ¥æ˜¯å¦`bootstrap_index`åœ¨å‰§é›†ç»“æŸä¹‹åã€‚å¦‚æœæ˜¯ï¼Œ`value`åˆ™è®¾ç½®ä¸º 0ï¼Œå¦åˆ™`value`è®¾ç½®ä¸ºä½ç½®çš„æŠ˜æ‰£é¢„æµ‹å€¼`bootstrap_index`ã€‚

`current_index`ç„¶åï¼Œå°†å’Œä¹‹é—´ç´¯ç§¯çš„æŠ˜æ‰£å¥–åŠ±`bootstrap_index`æ·»åŠ åˆ°`value`ã€‚

æœ€åï¼Œæœ‰ä¸€ä¸ªæ£€æŸ¥ä»¥ç¡®ä¿`current_index`ä¸æ˜¯åœ¨å‰§é›†ç»“æŸä¹‹åã€‚å¦‚æœæ˜¯ï¼Œåˆ™é™„åŠ ç©ºç›®æ ‡å€¼ã€‚å¦åˆ™ï¼Œå°†è®¡ç®—å‡ºçš„ TD ç›®æ ‡å€¼ã€æ¥è‡ª MCTS çš„çœŸå®å¥–åŠ±å’Œç­–ç•¥é™„åŠ åˆ°ç›®æ ‡åˆ—è¡¨ä¸­ã€‚

å¯¹äºå›½é™…è±¡æ£‹ï¼Œ`td_steps`å®é™…ä¸Šè®¾ç½®ä¸º`max_moves`æ€»æ˜¯`bootstrap_index`åœ¨å‰§é›†ç»“æŸåè½ä¸‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨ä½¿ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡ç›®æ ‡å€¼ï¼ˆå³æ‰€æœ‰æœªæ¥å¥–åŠ±çš„è´´ç°æ€»å’Œåˆ° episode ç»“æŸï¼‰ã€‚è¿™æ˜¯å› ä¸ºå›½é™…è±¡æ£‹çš„å¥–åŠ±åªåœ¨å‰§é›†ç»“æŸæ—¶é¢å‘ã€‚TD-Learningå’ŒMonte Carloä¼°è®¡çš„åŒºåˆ«å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![img](https://miro.medium.com/max/1400/1*KgwA-oRPwZt04fFpYTst9g.png)

TD-Learningæ–¹æ³•å’ŒMonte Carloæ–¹æ³•åœ¨ç›®æ ‡å€¼è®¾å®šä¸Šçš„åŒºåˆ«

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†ç›®æ ‡æ˜¯å¦‚ä½•æ„å»ºçš„ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£å®ƒä»¬å¦‚ä½•é€‚åº” MuZero æŸå¤±å‡½æ•°ï¼Œæœ€åï¼Œäº†è§£å®ƒä»¬å¦‚ä½•åœ¨`update_weights`å‡½æ•°ä¸­ç”¨äºè®­ç»ƒç½‘ç»œã€‚

## MuZero æŸå¤±å‡½æ•°

Muzero çš„æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š

![img](https://miro.medium.com/max/1400/1*HQx-R0TkAiQMtVGlPIyBpg.png)

è¿™é‡Œï¼Œ`K`æ˜¯`num_unroll_steps`å˜é‡ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æ­£åœ¨åŠªåŠ›å‡å°‘ä¸‰ç§æŸå¤±ï¼š

1. æå‰é¢„æµ‹çš„**å¥–åŠ±** æ­¥éª¤ä¸å®é™…å¥–åŠ±ä¹‹é—´çš„å·®å¼‚`k``t` `(r)``(u)`
2. æå‰é¢„æµ‹**å€¼** `k`æ­¥æ•°`t` `(v)`ä¸TDç›®æ ‡å€¼çš„å·®å€¼`(z)`
3. æå‰é¢„æµ‹çš„**æ”¿ç­–** æ­¥éª¤ä¸ MCTS æ”¿ç­–ä¹‹é—´çš„å·®å¼‚`k``t` `(p)``(pi)`

è¿™äº›æŸå¤±åœ¨æ¨å‡ºè¿‡ç¨‹ä¸­ç›¸åŠ ï¼Œä»¥ç”Ÿæˆæ‰¹æ¬¡ä¸­ç»™å®šä½ç½®çš„æŸå¤±ã€‚è¿˜æœ‰ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹æ¥æƒ©ç½šç½‘ç»œä¸­çš„å¤§æƒé‡ã€‚

## æ›´æ–°ä¸‰ä¸ª MuZero ç½‘ç»œï¼ˆ`update_weights)`

```python
def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,
                   weight_decay: float):
  loss = 0
  for image, actions, targets in batch:
    # Initial step, from the real observation.
    value, reward, policy_logits, hidden_state = network.initial_inference(
        image)
    predictions = [(1.0, value, reward, policy_logits)]

    # Recurrent steps, from action and previous hidden state.
    for action in actions:
      value, reward, policy_logits, hidden_state = network.recurrent_inference(
          hidden_state, action)
      predictions.append((1.0 / len(actions), value, reward, policy_logits))

      hidden_state = tf.scale_gradient(hidden_state, 0.5)

    for prediction, target in zip(predictions, targets):
      gradient_scale, value, reward, policy_logits = prediction
      target_value, target_reward, target_policy = target

      l = (
          scalar_loss(value, target_value) +
          scalar_loss(reward, target_reward) +
          tf.nn.softmax_cross_entropy_with_logits(
              logits=policy_logits, labels=target_policy))

      loss += tf.scale_gradient(l, gradient_scale)

  for weights in network.get_weights():
    loss += weight_decay * tf.nn.l2_loss(weights)

  optimizer.minimize(loss)
view rawpseudocode.py hosted with â¤ by GitHub
```



è¯¥`update_weights`å‡½æ•°ä¸ºæ‰¹æ¬¡ä¸­çš„ 2048 ä¸ªä½ç½®ä¸­çš„æ¯ä¸€ä¸ªé€ä¸ªæ„å»ºæŸå¤±ã€‚

é¦–å…ˆï¼Œåˆå§‹è§‚å¯Ÿé€šè¿‡`initial_inference`ç½‘ç»œè¿›è¡Œé¢„æµ‹`value`ï¼Œ`reward`å¹¶`policy`ä»å½“å‰ä½ç½®å¼€å§‹ã€‚è¿™äº›ç”¨äºåˆ›å»º`predictions`åˆ—è¡¨ï¼Œä»¥åŠç»™å®šçš„æƒé‡ 1.0ã€‚

ç„¶åï¼Œä¾æ¬¡å¾ªç¯æ¯ä¸ªåŠ¨ä½œï¼Œå¹¶`recurrent_inference`è¦æ±‚å‡½æ•°é¢„æµ‹ä¸‹ä¸€ä¸ª`value`å’Œ`reward`å½“å‰`policy`çš„`hidden_state`ã€‚è¿™äº›é™„åŠ åˆ°`predictions`åˆ—è¡¨çš„æƒé‡ä¸º`1/num_rollout_steps`ï¼ˆä»¥ä¾¿å‡½æ•°çš„æ€»æƒé‡`recurrent_inference`ç­‰äºå‡½æ•°çš„æƒé‡`initial_inference`ï¼‰ã€‚

ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—å°†`predictions`ä¸å…¶å¯¹åº”çš„ç›®æ ‡å€¼è¿›è¡Œæ¯”è¾ƒçš„æŸå¤±â€”â€”è¿™æ˜¯`scalar_loss`å’Œ`reward`çš„`value`ç»„åˆ`softmax_crossentropy_loss_with_logits`ã€‚`policy`

ä¼˜åŒ–ç„¶åä½¿ç”¨æ­¤æŸå¤±å‡½æ•°åŒæ—¶è®­ç»ƒæ‰€æœ‰ä¸‰ä¸ª MuZero ç½‘ç»œã€‚

æ‰€ä»¥â€¦â€¦è¿™å°±æ˜¯æ‚¨ä½¿ç”¨ Python è®­ç»ƒ MuZero çš„æ–¹å¼ã€‚

# æ€»ç»“

æ€»ä¹‹ï¼ŒAlphaZero å¤©ç”Ÿå°±çŸ¥é“ä¸‰ä»¶äº‹ï¼š

- å½“å®ƒåšå‡ºç»™å®šçš„åŠ¨ä½œæ—¶ï¼Œæ£‹ç›˜ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœå®ƒæ‰§è¡Œâ€œå°†æ£‹å­ä» e2 ç§»åŠ¨åˆ° e4â€çš„æ“ä½œï¼Œå®ƒçŸ¥é“ä¸‹ä¸€ä¸ªæ£‹ç›˜ä½ç½®æ˜¯ç›¸åŒçš„ï¼Œåªæ˜¯æ£‹å­å·²ç»ç§»åŠ¨äº†ã€‚
- ç»™å®šä½ç½®çš„åˆæ³•ç§»åŠ¨æ˜¯ä»€ä¹ˆã€‚ä¾‹å¦‚ï¼ŒAlphaZero çŸ¥é“å¦‚æœæ‚¨çš„çš‡åä¸åœ¨æ£‹ç›˜ä¸Šã€æœ‰æ£‹å­é˜»æŒ¡ç§»åŠ¨æˆ–è€…æ‚¨åœ¨ c3 ä¸Šå·²æœ‰æ£‹å­ï¼Œåˆ™æ‚¨ä¸èƒ½å°†â€œçš‡åç§»è‡³ c3â€ã€‚
- æ¯”èµ›ç»“æŸæ—¶ï¼Œè°èµ¢äº†ã€‚ä¾‹å¦‚ï¼Œå®ƒçŸ¥é“å¦‚æœå¯¹æ‰‹çš„ç‹å¤„äºè¢«æ§åˆ¶çŠ¶æ€å¹¶ä¸”ä¸èƒ½è„±ç¦»è¢«æ§åˆ¶çŠ¶æ€ï¼Œå®ƒå°±èµ¢äº†ã€‚

æ¢å¥è¯è¯´ï¼ŒAlphaZero å¯ä»¥æƒ³è±¡å¯èƒ½çš„æœªæ¥ï¼Œå› ä¸ºå®ƒçŸ¥é“æ¸¸æˆè§„åˆ™ã€‚

åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒMuZero æ— æ³•è®¿é—®è¿™äº›åŸºæœ¬æ¸¸æˆæœºåˆ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡æ·»åŠ å‡ ä¸ªé¢å¤–çš„ç¥ç»ç½‘ç»œï¼Œå®ƒèƒ½å¤Ÿåº”å¯¹ä¸çŸ¥é“è§„åˆ™çš„æƒ…å†µã€‚

äº‹å®ä¸Šï¼Œå®ƒè“¬å‹ƒå‘å±•ã€‚

ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ˜¯ï¼ŒMuZero å®é™…ä¸Šæ”¹è¿›äº† AlphaZero åœ¨å›´æ£‹ä¸­çš„è¡¨ç°ã€‚è¿™å¯èƒ½è¡¨æ˜å®ƒæ­£åœ¨å¯»æ‰¾æ¯” AlphaZero åœ¨ä½¿ç”¨å®é™…æ£‹ç›˜ä½ç½®æ—¶æ‰¾åˆ°çš„æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥é€šè¿‡å…¶éšè—è¡¨ç¤ºæ¥è¡¨ç¤ºä½ç½®ã€‚MuZero å°†æ¸¸æˆåµŒå…¥è‡ªå·±å¤§è„‘çš„ç¥ç§˜æ–¹å¼è‚¯å®šä¼šæˆä¸º DeepMind åœ¨ä¸ä¹…çš„å°†æ¥çš„ä¸€ä¸ªæ´»è·ƒç ”ç©¶é¢†åŸŸã€‚

![img](https://miro.medium.com/max/1400/1*P0FWTk4xF2iMCtl9nxR4bQ.png)

MuZero åœ¨å›½é™…è±¡æ£‹ã€å°†æ£‹ã€å›´æ£‹å’Œ Atari æ¸¸æˆä¸­çš„è¡¨ç°æ€»ç»“ã€‚

æœ€åï¼Œæˆ‘æƒ³ç®€è¦æ€»ç»“ä¸€ä¸‹ä¸ºä»€ä¹ˆæˆ‘è®¤ä¸ºè¿™ç§å‘å±•å¯¹ AI éå¸¸é‡è¦ã€‚

## ä¸ºä»€ä¹ˆè¿™æ˜¯ä¸€ä»¶å¤§äº‹

AlphaZero å·²ç»è¢«è®¤ä¸ºæ˜¯è¿„ä»Šä¸ºæ­¢ AI æœ€ä¼Ÿå¤§çš„æˆå°±ä¹‹ä¸€ï¼Œå®ƒåœ¨ä¸€ç³»åˆ—æ¸¸æˆä¸­å®ç°äº†è¶…äººçš„å®åŠ›ï¼Œè€Œæ— éœ€äººç±»ä¸“ä¸šçŸ¥è¯†ä½œä¸ºè¾“å…¥ã€‚

ä»è¡¨é¢ä¸Šçœ‹ï¼ŒèŠ±è´¹å¦‚æ­¤å¤šçš„é¢å¤–åŠªåŠ›æ¥è¯æ˜ç®—æ³•ä¸ä¼šå› æ‹’ç»è®¿é—®è§„åˆ™è€Œå—åˆ°é˜»ç¢ï¼Œè¿™ä¼¼ä¹å¾ˆå¥‡æ€ªã€‚è¿™æœ‰ç‚¹åƒæˆä¸ºå›½é™…è±¡æ£‹ä¸–ç•Œå† å†›ï¼Œç„¶åé—­ç€çœ¼ç›å‚åŠ æ‰€æœ‰æœªæ¥çš„æ¯”èµ›ã€‚è¿™åªæ˜¯èšä¼šçš„æŠŠæˆå—ï¼Ÿ

ç­”æ¡ˆæ˜¯ï¼Œè¿™ä»æ¥éƒ½ä¸æ˜¯å…³äº DeepMind çš„å›´æ£‹ã€å›½é™…è±¡æ£‹æˆ–ä»»ä½•å…¶ä»–æ£‹ç›˜æ¸¸æˆã€‚è¿™æ˜¯å…³äºæ™ºèƒ½æœ¬èº«ã€‚

å½“æ‚¨å­¦ä¹ æ¸¸æ³³æ—¶ï¼Œæ‚¨å¹¶æ²¡æœ‰é¦–å…ˆè·å¾—æµä½“åŠ¨åŠ›å­¦çš„è§„åˆ™æ‰‹å†Œã€‚å½“æ‚¨å­¦ä¹ ç”¨ç§¯æœ¨å»ºé€ å¡”æ¥¼æ—¶ï¼Œæ‚¨å¹¶æ²¡æœ‰å‡†å¤‡å¥½ç‰›é¡¿ä¸‡æœ‰å¼•åŠ›å®šå¾‹ã€‚å½“ä½ å­¦ä¼šè¯´è¯æ—¶ï¼Œä½ æ˜¯åœ¨ä¸æ‡‚ä»»ä½•è¯­æ³•çš„æƒ…å†µä¸‹å­¦ä¹ çš„ï¼Œå³ä½¿åœ¨ä»Šå¤©ï¼Œä½ å¯èƒ½ä»ç„¶å¾ˆéš¾å‘éæ¯è¯­äººå£«è§£é‡Šè¯­è¨€çš„æ‰€æœ‰è§„åˆ™å’Œæ€ªç™–ã€‚

å…³é”®æ˜¯ï¼Œç”Ÿæ´»åœ¨æ²¡æœ‰è§„åˆ™æ‰‹å†Œçš„æƒ…å†µä¸‹å­¦ä¹ ã€‚

è¿™æ˜¯å¦‚ä½•è¿ä½œçš„ä»ç„¶æ˜¯å®‡å®™æœ€å¤§çš„ç§˜å¯†ä¹‹ä¸€ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ç»§ç»­æ¢ç´¢ä¸éœ€è¦ç›´æ¥äº†è§£ç¯å¢ƒåŠ›å­¦æ¥æå‰è®¡åˆ’çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¦‚æ­¤é‡è¦çš„åŸå› ã€‚

[MuZero è®ºæ–‡å’ŒåŒæ ·ä»¤äººå°è±¡æ·±åˆ»çš„WorldModels](https://worldmodels.github.io/)è®ºæ–‡ï¼ˆHaï¼ŒSchmidhuberï¼‰ä¹‹é—´å­˜åœ¨ç›¸ä¼¼ä¹‹å¤„ã€‚ä¸¤è€…éƒ½åˆ›å»ºä»…å­˜åœ¨äºä»£ç†å†…éƒ¨çš„ç¯å¢ƒå†…éƒ¨è¡¨ç¤ºï¼Œå¹¶ç”¨äºæƒ³è±¡å¯èƒ½çš„æœªæ¥ä»¥è®­ç»ƒæ¨¡å‹ä»¥å®ç°ç›®æ ‡ã€‚ä¸¤ç¯‡è®ºæ–‡å®ç°è¿™ä¸€ç›®æ ‡çš„æ–¹å¼ä¸åŒï¼Œä½†æœ‰ä¸€äº›ç›¸ä¼¼ä¹‹å¤„ï¼š

- MuZero ä½¿ç”¨è¡¨ç¤ºç½‘ç»œåµŒå…¥å½“å‰è§‚å¯Ÿï¼ŒWorldModels ä½¿ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ã€‚
- MuZero ä½¿ç”¨åŠ¨æ€ç½‘ç»œå¯¹æƒ³è±¡ç¯å¢ƒå»ºæ¨¡ï¼ŒWorldModel ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œã€‚
- MuZero ä½¿ç”¨ MCTS å’Œé¢„æµ‹ç½‘ç»œæ¥é€‰æ‹©åŠ¨ä½œï¼ŒWorld Models ä½¿ç”¨è¿›åŒ–è¿‡ç¨‹æ¥è¿›åŒ–æœ€ä½³åŠ¨ä½œæ§åˆ¶å™¨ã€‚

å½“ä¸¤ä¸ªä»¥è‡ªå·±çš„æ–¹å¼å¼€åˆ›æ€§çš„æƒ³æ³•å®ç°ç›¸ä¼¼çš„ç›®æ ‡æ—¶ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ªå¥½å…†å¤´ã€‚è¿™é€šå¸¸æ„å‘³ç€åŒæ–¹éƒ½å‘ç°äº†ä¸€äº›æ›´æ·±å±‚æ¬¡çš„æ½œåœ¨çœŸç›¸â€”â€”ä¹Ÿè®¸è¿™ä¸¤æŠŠé“²å­åªæ˜¯å‡»ä¸­äº†å®ç®±çš„ä¸åŒéƒ¨åˆ†ã€‚