

# CyberBattleSim： 网络攻防模拟工具

## CyberBattleSim 介绍

CyberBattleSim是一款微软365 Defender团队开源的人工智能攻防对抗模拟工具，来源于微软的一个实验性研究项目。该项目专注于对网络攻击入侵后横向移动阶段进行威胁建模，用于研究在模拟抽象企业网络环境中运行的自动化Agent的交互。

网络拓扑和一组预定义的漏洞定义了进行模拟的环境。攻击者利用现有漏洞，通过横向移动在网络中进化，目标是通过利用计算机节点中植入的参数化漏洞来获取网络的所有权。防御者试图遏制攻击者并将其从网络中驱逐。 CyberBattleSim 为其模拟提供了 OpenAI Gym 接口，以促进强化学习算法的实验。

为了比较Agent的性能，主要查看了两个指标：为实现其目标而采取的模拟步骤数，以及跨训练时期模拟步骤的累积奖励。通过步骤梳理与累计的分数，评估最先进的强化算法，以研究自主Agent如何与它们交互并从中学习。

## CyberBattleSim 框架介绍

CyberBattleSim 仿真运行时环境提供高保真度和控制，框架中使用的是参数化的虚拟环境，模拟环境性能要求低，更轻量，速度快，抽象，并且可控性更强，适用于强化学习实验。优点如下：

- 抽象级别高，只需要建模系统重要的方面；例如应用程序级网络通信与数据包级网络模拟，忽略了低层的信息（例如，文件系统、注册表）。
- 灵活性：定义一个新的机器是很容易的，不需要考虑底层的驱动等，可以限制动作空间为可以管理且相关的子集。
- 可有效捕获全局状态，从而简化调试和诊断。
- 轻量级：在单台机器/进程的内存中运行。

CyberBattleSim的仿真固然简单，但是简单是具有优势的。高度抽象的性质使得无法直接应用于现实系统，从而防止了潜在的恶意训练的自动化Agent使用。同时，可以使我们更专注于特定的安全性方面，例如研究和快速实验最新的机器学习和AI算法。

当前的内网渗透实现方式侧重于横向移动，希望理解网络拓扑和配置并施加影响。基于这一目标，没有必要对实际的网络流量进行建模。

该项目主要采用了免模型学习（Model-Free），虽然在效率上不如有模型学习（Model-Based）（缺点是如果模型跟实际场景不一致，那么在实际使用场景下会表现的不好），但是这种方式更加容易实现，也容易在真实场景下调整到很好的状态。所以免模型学习方法更受欢迎，得到更加广泛的开发和测试。

## 模拟如何进行

让我们通过一个Demo示例来介绍如何使用 RL 术语进行模拟。我们的网络环境由有向注释图给出，其中节点代表计算机，边代表其他节点的知识或节点之间发生的通信。

![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-377114ff-cdb7-4bee-88da-cac09640f661.png)

CyberBattleSim中的强化学习建模：

- 环境：状态就是网络，单个Agent，部分可观测（Agent无法观测到所有的结点和边），静态的，确定性的，离散的，post-breach
- 行动空间（Agent可以逐步探索网络）：本地攻击，远程攻击，认证连接
- 观测空间：发现结点，获取结点，发现凭证，特权提升，可用攻击
- 奖励：基于结点的内在价值，如SQL server比测试机器重要

在这里您可以看到一个网络Demo示例，其中机器运行不同的操作系统、软件。每台机器都有属性、价值，并且都存在预先分配的漏洞。蓝色边缘代表节点之间运行的流量，并由通信协议标记。

![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-9f950a75-2c63-457a-b109-56091f84711a.png)

该项目中的环境（environment）定义：

- 网络中结点的属性：如Windows，Linux，ApacheWebSite，MySql，nginx/1.10.3，SQLServer等。
- 开放的ports：如HTTPS，SSH，RDP，PING，GIT等。
- 本地漏洞包括：CredScanBashHistory，CredScan-HomeDirectory，CredScan-HomeDirectory等。
- 远程漏洞包括：ScanPageContent，ScanPageSource，NavigateWebDirectoryFurther，NavigateWebDirectory等。
- 防火墙配置为：允许进出的服务为RDP，SSH，HTTPS，HTTP，其他服务默认不允许。
- 定义了部分奖励与惩罚：发现新结点奖励，发现结点属性奖励，发现新凭证奖励，试图连接未打开端口的处罚，重复使用相同漏洞的惩罚等。

本地 Agent如下，定义了其包含的漏洞，漏洞类型，漏洞描述，端口，价值，花费，服务等。

只有一个智能体：攻击者。

最初，一个节点被感染（泄露后假设）

其目标是通过发现和“拥有”网络中的节点来最大化奖励。

环境是部分可观察的：代理无法提前看到网络图的所有节点和边。

相反，攻击者采取行动逐渐观察环境。共有三种操作 为代理提供了混合利用和探索功能：

- 执行本地攻击，
- 执行远程攻击，
- 连接到其他节点。

奖励是一个浮点数，代表节点的内在价值（例如，SQL 服务器比测试机具有更大的价值）。

攻击者从橙色粗箭头所指左侧的Win7节点侵入网络，

- 然后利用 SMB 中的漏洞横向移动到 Win8 节点，
- 然后使用一些缓存的凭据登录Win7机器，
- 利用IIS远程漏洞拥有IIS服务器，
- 最后使用泄漏的连接字符串来访问 SQL DB。

攻击者通过初始化的攻击节点开始，由于无法获取到当前整个环境当中的网络拓扑图与链接方式，攻击者 Agent 只有三种能力进行横向扩散：本地攻击、远程攻击、链接到其他节点。通过不同的结点的权限和不同的动作典型的比如数据泄露、泄露、权限失陷等行为，分别给不同的分数。

防御 Agent主要通过预测攻击成功的可能性的基础之上实现了识别、减缓攻击的行为。主要通过重装镜像（re-image）的方式抵御攻击，通过计算攻击者的步骤数和持续性的奖励分数来衡量当前攻击策略的优劣性。通过返回的数据字段内容来确认各种攻击的成功性。(防御遍历所有节点，如果发现该节点可能存在漏洞（定义了一个概率函数计算可能性），先使该节点不可用，再通过重装镜像的方式抵御攻击）。

## 模拟支持哪些类型的漏洞和攻击？

模拟健身房环境由网络定义参数化，网络定义由底层网络图本身以及支持的漏洞及其所在节点的描述组成。由于模拟不运行任何代码，因此无法实际实现漏洞和利用。相反，我们通过定义以下内容对每个漏洞进行抽象建模： 确定漏洞在给定节点上是否处于活动状态的前提条件；攻击者成功利用它的概率；以及成功利用的副作用。每个节点都有一组分配的命名属性。然后，前置条件被表示为可能的节点属性（或标志）集上的布尔表达式。

### 漏洞结果

每个漏洞都有一个预定义的结果，其中可能包括：

- 一组泄露的凭据；
- 对网络中另一个节点的引用被泄露；
- 泄露节点信息（节点属性）；
- 节点的所有权；
- 节点上的权限升级。

远程漏洞的示例包括：

- 公开凭据的 SharePoint 站点`ssh`（但不一定是远程计算机的 ID）；
- `ssh`授予机器访问权限的漏洞；
- 一个 github 项目泄露了提交历史记录中的凭证；
- 一个 SharePoint 站点，其文件包含存储帐户的 SAS 令牌；

本地漏洞示例：

- 从系统缓存中提取身份验证令牌或凭据；
- 升级到SYSTEM权限；
- 升级至管理员权限。

漏洞可以在节点级别就地定义，也可以全局定义并由前置条件布尔表达式激活。

每一个step中，都会执行Action（行动）,observation（观察状态）,wrapper（通过当前状态反馈做出改变）。

在强化学习训练的时候，一开始会让Agent更偏向于探索Explore，并不是哪一个Action带来的reward最大就执行该Action，选择Action时具有一定的随机性，目的是为了覆盖更多的Action，尝试每一种可能性。等训练很多轮以后各种状态下的各种Action基本尝试完以后，这时候会大幅降低探索的比例，尽量让Agent更偏向于利用Exploit，哪一个Action返回的reward最大，就选择哪一个Action。

## CyberBattleSim 实例过程

该实例通过强化学习算法查找结点及其漏洞，由初始节点通过本地漏洞探测到一个Website节点，step=6，当前reward=6，其中的step如图5上所示，左下图横坐标为step，纵坐标为reward，右下图为网络拓扑图。此时结点client为红色已拥有，结点Website为绿色未拥有。

再经过6个step后如图6，通过WEBSITE的漏洞ScanPageContent发现了结点GitHubProject，获得reward=6，当前总reward=12。（注意，这部分重复了三次step发现结点Website，说明该算法也有弊端）

![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-4fd79f98-36b2-45ae-82e4-2631aacda090.png)

最终从Agent节点通过各种本地攻击，远程攻击和连接其他节点，获取到网络中存在漏洞的节点如图7，当前step=5600，reward=431。

![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-cdb2b5e1-92f9-4a9e-af9f-b1a9bcae96a5.png)

## 项目中强化学习算法比较

使用多种算法在环境中获得的结果`CyberBattleChain10`：纯随机搜索、带有 epsilon-greedy 的 Q 学习、仅利用学习矩阵。

该项目使用了一些强化学习算法比较其优劣性，分别为Tabular Q-learning, Credential lookups, DQL(deep Q-learning), Exploiting DQL。如图所示，其中X轴是在多个episode，（Y轴）中为获得网络的完全所有权而采取的迭代数量（越低越好）。某些算法（如Exploiting DQL）随着episode增加可以逐渐改进并达很高水平，而有些算法在50 episode后仍在苦苦挣扎！

![image.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-54d83b7b-65d1-4d6a-b0f6-d41b31460c81.png)

如图9提供了另一种比较方法，即跨训练时期模拟步骤的累积奖励。实线显示中位数，而阴影表示一个标准差。这再次显示了某些算法（红色Exploiting DQL、蓝色Tabular Q-learning和绿色DQL）的表现明显优于其他算法（橙色Credential lookups）。

请注意，当所有网络节点都被拥有时，仍然可以通过利用所拥有节点上的漏洞来获得额外的奖励，但在这个实验中，一旦代理拥有所有节点，我们就终止游戏。这解释了为什么优化网络所有权的 DQL 代理尽管击败了所有其他代理，却无法达到最大可能的奖励。

![image.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-f8f00fe7-466f-4d2b-aaee-dd20720854db.png) 











### 功能选择

通过 Q 学习，在对包括特定于网络大小的特征（例如发现的节点数量（左））进行训练时获得了最佳结果。当使用不依赖于网络大小的特征时，也可以获得良好的结果（右）

### 功能选择



通过 Q 学习，在对包括特定于网络大小的特征（例如发现的节点数量（左））进行训练时获得了最佳结果。当使用不依赖于网络大小的特征时，也可以获得良好的结果（右）。

|               | 尺寸不可知的特征                                             | 尺寸相关的功能                                               |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 表格 Q 与随机 | [![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-9b0b1506-880b-43e5-91b0-75f2ce3fb032.png)](https://github.com/microsoft/CyberBattleSim/blob/main/docs/.attachments/image-9b0b1506-880b-43e5-91b0-75f2ce3fb032.png) | [![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-41f45aa6-0af8-4c67-afec-24b1adc910c2.png)](https://github.com/microsoft/CyberBattleSim/blob/main/docs/.attachments/image-41f45aa6-0af8-4c67-afec-24b1adc910c2.png) |

## `chain`环境迁移学习

该基准旨在衡量从一种环境中学习策略并将其应用于不同规模的类似环境的能力。我们在规模较大的环境中训练代理$x$并在规模环境中对其进行评估$y>x$。

正如预期的那样，使用与环境规模成正比的功能（例如节点数量或凭证数量）并不能提供最佳结果。相反，当使用最近发现的端口滑动窗口和节点属性等时间特征时，代理表现得更好。

|               | 在 4 号上训练，在 10 号上评估                                | - 训练尺码 10，评估尺码 4                                    |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 表格 Q 与随机 | [![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-11d24066-875d-43ac-87cb-e91453688028.png)](https://github.com/microsoft/CyberBattleSim/blob/main/docs/.attachments/image-11d24066-875d-43ac-87cb-e91453688028.png) | [![图片.png](https://github.com/microsoft/CyberBattleSim/raw/main/docs/.attachments/image-daf58e3d-a4a8-4810-8a5e-1c976a24b266.png)](https://github.com/microsoft/CyberBattleSim/blob/main/docs/.attachments/image-daf58e3d-a4a8-4810-8a5e-1c976a24b266.png) |

## CyberBattleSim 评估

项目存在的问题

1. CyberBattleSim 除了提供Agent之外还可以通过Gym的基础提供参数化构建的虚拟网络环境、漏洞类型、漏洞出现的节点等。所以该项目其实只是一个强化学习的自动化攻击框架，并没有进行实际的攻击，网络中的所有节点，漏洞，漏洞类型等都是使用各种参数自定义的。

2. 该项目的攻击方式包括本地攻击，远程攻击和连接其他节点，每种攻击只举了几个例子，然而实际过程中远远不止于此，需要学习训练就会是一个很耗时的过程。且该项目采用免模型学习（虽然该方法会更适用于当前网络环境），实际渗透中因为攻击方式众多，需要训练的时间也会更长，具体学习渗透的时间犹未可知。

3. CyberBattleSim项目提供的只是自动化攻击内网渗透项目当中必不可少的沙盒，只是一个用户产生虚拟攻防场景数据的工具，距离真实的项目还有很长的路要走，现有的强化学习最好的例子只存在于游戏（2016年：AlphaGo Master 击败李世石，使用强化学习的 AlphaGo Zero 仅花了40天时间；2019年4月13日：OpenAI 在《Dota2》的比赛中战胜了人类世界冠军），对于复杂的自动化攻击并不一定能胜任。

项目的优势

1. 借助OpenAI工具包，可以为复杂的计算机系统构建高度抽象的模拟，可视化的图像表达，使用户可以容易看到这些AgentAgent的进化行为，通过步骤梳理与累计的分数，可以对当前的场景有个较好的展示，并评估出最合适的强化学习算法（其中经过实验得到的结果为Exploiting DQL算法最优
2. CyberBattleSim的仿真固然简单，但是简单是具有优势的。高度抽象的性质使得无法直接应用于现实系统，从而防止了潜在的恶意训练的自动化Agent使用。同时，这种简单可以更专注于特定的安全性方面，例如研究和快速试验最新的机器学习和AI算法。项目目前专注于横向移动技术，目的是了解网络拓扑和配置如何影响这些技术。考虑到这样的目标，微软认为没有必要对实际的网络流量进行建模，但这是该项目实际应用的重大限制。
3. 该项目相比于其他强化学习自动化渗透项目：如DEEPEXPLOIT框架，AutoPentest-DRL框架，这两个框架都使用了强化学习，nmap扫描，Metasploit攻击，但是他们并没有有效利用强化学习，主要原因在于他们的action只是根据各种漏洞对应相应的payload获取shell，该模式更像是监督学习，因为没有环境观察与反馈。CyberBattleSim项目有它自己的优势，虽然该项目并没有实现真实攻击，但是该项目完整地诠释了强化学习的步骤（包含观察环境与反馈），如果能开发出合适的工具使用，那么就可以实现更高效，准确度更高的渗透。

项目的发展

该项目更适合比较强化学习算法在内网渗透的优劣，因为该项目高度虚拟化，不考虑底层网络的信息，要使该项目成为一个真实的内网渗透工具是一个极大挑战。如下列出可能对该项目有所贡献的改进：

1. 实现一个类似端口扫描操作(非确定性)的nmap，用来收集信息，而且该步骤不仅仅是渗透的开始工作，在渗透过程中也需要更新信息。
2. 与现有的攻击工具结合或者开发更适合强化学习模型的攻击工具，用来真实的攻击。
3. 奖励的定义也是强化学习中重要的一项内容，可以通过通用漏洞评分系统（CVSS）的组成部分所确定的漏洞得分来定义。

## 总结

本文针对自动化内网渗透这一方向对微软的开源项目CyberBattleSim做了介绍，通过对其内部原理和源码的分析，笔者指出了该项目的优势，存在的问题及其发展前景。该项目只是自动化攻击内网渗透项目中必不可少的沙盒，自动化渗透还有很长的路要走。
