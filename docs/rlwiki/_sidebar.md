- [入门教程](rlwiki/hfrlclass/README.md)

  - [第一章：深度强化学习简介](rlwiki/hfrlclass/ch1_introduction.md)
  - [第二章：Q-Learning](rlwiki/hfrlclass/ch2_q-learning.md)
  - [第三章：Deep Q-Learning](rlwiki/hfrlclass/ch3_dqn.md)
  - [第四章：Policy Gradient](rlwiki/hfrlclass/ch4_pg.md)
  - [第五章：Actor-Critic](rlwiki/hfrlclass/ch5_a2c.md)
  - [第六章：近端策略优化 (PPO)](rlwiki/hfrlclass/ch6_ppo.md)
  - [第七章：Decision Transformer](rlwiki/hfrlclass/ch7_decision-transformer.md)
  - [第八章：Multi-Agent RL](rlwiki/hfrlclass/ch8_marl.md)
  - [第九章：强化学习前沿主题](rlwiki/hfrlclass/ch9_advanced.md)
  - [第十章：人类反馈的强化学习](rlwiki/papers/RLHF.md)


- [进阶教程](rlwiki/algorithms/README.md)
  - [Policy gradient theorem的证明](rlwiki/algorithms/ch1_supp_pg.md)
  - [为什么A2C中减去 baseline 函数可以减小方差](rlwiki/algorithms/ch1_supp_a2c.md)
  - [步步深入TRPO](rlwiki/algorithms/ch1_supp_trpo.md)
  - [混合动作空间表征学习方法介绍（HyAR）](rlwiki/algorithms/ch2_supp_hyar.md)
  - [为什么 PPO 需要重要性采样, 而 DDPG 这个 off-policy 算法不需要](rlwiki/algorithms/ch2_supp_ppovsddpg.md)
  - [重参数化与强化学习](rlwiki/algorithms/ch2_supp_reparameterization.md)

- 强化学习环境
  - [Awesome RL Envs](rlwiki/envs/awesome_envs.md)
  - [OpenAI Gym](rlwiki/envs/gym.md)
  - [SMAC](rlwiki/envs/smac.md)
  - [MARL Envs](rlwiki/envs/marl_env.md)
  - [PettingZoo](rlwiki/envs/pettingzoo.md)


- 强化学习工具篇
  - [强化学习代表人物/机构](rlwiki/rltools/awesome_rl.md)
  - [EnvPool: 并行环境模拟器](rlwiki/rltools/envpool.md)
  - [多智能体强化学习代码汇总](rlwiki/rltools/marltool.md)

- AlphaZero & MuZero & 蒙特卡洛树搜索
  - [蒙特卡洛树搜索入门指南](rlwiki/muzero/mcts_guide.md)
  - [蒙特卡洛树搜索(MCTS)详解](rlwiki/muzero/mcts.md)
  - [AlphaGoZero 算法介绍](rlwiki/muzero/alphazero.md)
  - [MuZero算法介绍](rlwiki/muzero/muzero_intro.md)
  - [MuZero伪代码](rlwiki/muzero/muzero_pseudocode.md)

- 多智能体强化学习

  - [MARL](rlwiki/papers/Overview.md)
  - [DRQN](rlwiki/papers/DRQN.md)
  - [IQL](rlwiki/papers/IQL.md)
  - [COMA](rlwiki/papers/COMA.md)
  - [VDN](rlwiki/papers/VDN.md)
  - [QTRAN](rlwiki/papers/QTRAN.md)
  - [QMIX](rlwiki/papers/QMIX.md)
  - [MADDPG](rlwiki/papers/MADDPG.md)
  - [MAT](rlwiki/papers/MAT.md)
