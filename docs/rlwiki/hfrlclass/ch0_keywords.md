# 强化学习关键词

## 马尔科夫性质

智能体采取的行动**仅以当前状态为条件，而与过去的状态和行动无关**。

## 观察/状态

- **状态**：对世界状态的完整描述。
- **观察**：对环境/世界状态的部分描述。

## 动作

- **离散动作**：有限数量的动作，例如向左、向右、向上和向下。
- **连续动作**：动作的无限可能性；例如，在自动驾驶汽车的情况下，驾驶场景有无限可能的动作发生。

## 奖励和折扣

- **奖励**：RL 中的基本因素。告诉智能体所采取的行动是好是坏。
- RL 算法专注于最大化**累积奖励**。
- **奖励假设**：RL 问题可以表述为（累积）回报的最大化。
- **执行折扣**是因为在开始时获得的奖励更有可能发生，因为它们比长期奖励更可预测。

## 探索与开发权衡

- **探索**：通过尝试随机行动并从环境中接收反馈/回报/奖励来探索环境。
- **利用**：利用我们对环境的了解以获得最大回报。
- **Exploration-Exploitation Trade-Off**：它平衡了我们想要**探索**环境的程度和我们想要**利用**我们对环境的了解程度。

## 策略

- **策略**：它被称为智能体的大脑。它告诉我们在给定状态下采取什么行动。
- **Optimal Policy**：当智能体按照它行事时，**最大化**预期回报*的Policy。它是通过训练学会的。

## 基于策略的方法：

- 一种解决 RL 问题的方法。
- 在这种方法中，Policy 是直接学习的。
- 将每个状态映射到该状态下的最佳对应动作。或者在该状态下一组可能的动作的概率分布。

## 基于价值的方法：

- 解决 RL 问题的另一种方法。
- 在这里，我们没有训练策略，而是训练了一个**价值函数**，将每个状态映射到处于该状态的期望值。

## 寻找最优策略的方法

- **基于策略的方法。**该策略通常使用神经网络进行训练，以选择在给定状态下采取的动作。在这种情况下，神经网络输出智能体应该采取的行动，而不是使用价值函数。根据环境接收到的经验，神经网络将重新调整并提供更好的动作。
- **基于价值的方法。**在这种情况下，价值函数被训练来输出代表我们政策的状态或状态-动作对的值。但是，此值未定义智能体应采取的操作。相反，我们需要在给定值函数输出的情况下指定智能体的行为。例如，我们可以决定采用一种策略来采取总是能带来最大回报的行动（贪心策略）。总之，该策略是一个贪婪策略（或用户做出的任何决定），它使用价值函数的值来决定要采取的行动。

## 在基于价值的方法中，我们可以找到两种主要策略

- **状态值函数。**对于每个状态，如果智能体从该状态开始并遵循策略直到结束，则状态值函数是预期的回报。
- **动作价值函数。**与状态值函数相反，动作值函数为每个状态和动作对计算智能体在该状态下开始并采取动作时的预期回报。然后它永远遵循该政策。

## Epsilon-贪心策略：

- 强化学习中使用的常见探索策略涉及平衡探索和开发。
- 选择具有最高预期奖励的动作，概率为 1-epsilon。
- 选择概率为 epsilon 的随机动作。
- Epsilon 通常会随着时间的推移而减少，以将注意力转移到开发上。

## 贪心策略：

- 涉及始终根据当前对环境的了解，选择预期会导致最高奖励的行动。（仅开发）
- 总是选择具有最高预期奖励的动作。
- 不包括任何探索。
- 在具有不确定性或未知最佳操作的环境中可能是不利的。

## **表格方法：**

状态和动作空间足够小以近似值函数以表示为数组和表格的问题类型。 **Q-learning**是表格方法的一个例子，因为表格用于表示不同状态-动作对的值。

## **深度 Q 学习：**

训练神经网络在给定状态下逼近该状态下每个可能动作的不同**Q 值的方法。**用于解决观察空间太大而无法应用表格 Q 学习方法的问题。

## **时间限制：**

当环境状态由帧表示时出现的困难。帧本身不提供时间信息。为了获得时间信息，我们需要将许多帧**堆叠**在一起。

## **深度 Q 学习的阶段：**

- **采样：**执行动作，并将观察到的经验元组存储在**重放存储器中**。
- **训练：**随机选择一批元组，神经网络使用梯度下降更新其权重。

## **稳定 Deep Q-Learning 的解决方案：**

- **经验回放：**创建回放记忆以保存可以在训练期间重复使用的经验样本。这允许智能体多次从相同的经验中学习。此外，它使智能体人在获得新经验时避免忘记以前的经验。 **从回放缓冲区中随机抽样**可以消除观察序列中的相关性，并防止动作值发生灾难性的振荡或发散。
- **Fixed Q-Target：**为了计算**Q-Target**，我们需要使用 Bellman 方程估计下一状态的折扣最优**Q 值**。问题是使用相同的网络权重来计算**Q-Target**和**Q-value**。这意味着每次我们修改**Q-value**时，**Q-Target**也会随之移动。为避免此问题，使用具有固定参数的单独网络来估计时间差异目标。**在某些C 步骤**之后，通过从我们的深度 Q 网络复制参数来更新目标网络。
- **Double DQN：**处理**Q-Values**高估**的方法。该解决方案使用两个网络将动作选择与目标值**生成**解耦：- **DQN 网络**为下一个状态选择最佳动作（具有最高**Q 值**的动作）-**目标网络**计算目标**Q-**在下一个状态下采取该行动的**价值。**这种方法减少了**Q 值的**高估，有助于更快地训练和更稳定的学习。
