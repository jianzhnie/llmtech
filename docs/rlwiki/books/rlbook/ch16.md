
## 第 16 章 非合作关系设定下的多智能体强化学习

上一章研究了多智能体强化学习 (MARL) 中最简单的设定一一完全合作关系, 在这 种设定下, 所有的智能体有相同的奖励、回报、价值、目标函数。本章研究非合作关系, 智能体各自有不同的奖励、回报、价值、目标函数。本章中采用的符号如图 $16.1$ 所示。

第 $16.1$ 节定义非合作关系设定下的策略学习、策略梯度方法、以及收敛判别。第 16.2 节推导非合作关系下的 $\mathrm{A} 2 \mathrm{C}$ 方法, 本书称之为 multi-agent noncooperative $\mathrm{A} 2 \mathrm{C}$, 缩 写 MAN-A2C, 可以用于离散控制问题。第 $16.3$ 节用三种架构实现 MAN-A2C：完全去 中心化、完全中心化、中心化训练 + 去中心化决策。第 $16.4$ 介绍多智能体确定策略梯度 方法, 缩写 MADDPG, 可以用于连续控制问题。

知能体:
![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-247.jpg?height=142&width=626&top_left_y=1002&top_left_x=770)

状态：

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-247.jpg?height=165&width=614&top_left_y=1188&top_left_x=778)

观则：

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-247.jpg?height=195&width=615&top_left_y=1350&top_left_x=775)

动作：

奖励：

$$
R^{1} \quad R^{2} \quad \cdots \quad R^{m}
$$

回报：

动作价值：

$\begin{array}{cccc}U^{1} & U^{2} & \cdots & U^{m} \\ Q_{\pi}^{1} & Q_{\pi}^{2} & \cdots & Q_{\pi}^{m} \\ V_{\pi}^{1} & V_{\pi}^{2} & \cdots & V_{\pi}^{m} \\ J^{1} & J^{2} & \cdots & J^{m}\end{array}$

目标函数：

图 16.1: 多智能体强化学习 (MARL) 的符号。

## 1 非合作关系设定下的策略学习

上一章研究合作关系的 MARL, 即所有智能体的奖励都相等: $R^{1}=\cdots=R^{m}$ 。在 这种设定下, 所有智能体有相同的状态价值函数 $V_{\pi}(s)$ 和目标函数

$$
J\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S}\left[V_{\pi}(s)\right] .
$$

目标函数可以衡量策略网络参数 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 的好坏。策略学习的目的是改进 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 使得 $J$ 变大。合作关系的设定下, 策略学习的收玫标准很明确：如果找不到更好的 $\boldsymbol{\theta}^{1}$, $\cdots, \boldsymbol{\theta}^{m}$ 使得 $J$ 变大, 那么当前的 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 就是最优解。

非合作关系设定下的目标函数 : 如果是非合作关系, 那么不存在这样的关系： $R^{1}=$ $\cdots=R^{m}$ 。两个智能体的奖励不相等 $\left(R^{i} \neq R^{j}\right)$, 那么它们的回报也不相等 $\left(U^{i} \neq U^{j}\right)$, 回报的期望（即价值函数）也不相等。把状态价值记作：

$$
V^{1}(s), \quad V^{2}(s), \quad \cdots, \quad V^{m}(s) .
$$

第 $i$ 号智能体的目标函数是状态价值的期望:

$$
J^{i}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S}\left[V_{\pi}^{i}(S)\right] .
$$

$J^{i}$ 的意义是回报 $U^{i}$ 的期望, 所以能反映出第 $i$ 号智能体的表现好坏。

注 目标函数 $J^{1}, J^{2}, \cdots, J^{m}$ 是各不相同的, 也就是说智能体没有共同的目标（除非是完 全合作关系）。举个例子, 在 predator-prey（捕食者一猎物）的游戏中, 捕食者的目标函 数 $J^{1}$ 与猎物的目标函数 $J^{2}$ 负相关： $J^{1}=-J^{2}$ 。

注 第 $i$ 号智能体的目标函数 $J^{i}$ 依赖于所有智能体的策略网络参数 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 。为什么 一个智能体的目标函数依赖于其他智能体的策略呢? 举个例子, 捕食者改进自己的策略 $\boldsymbol{\theta}^{1}$, 而猎物没有改变策略 $\boldsymbol{\theta}^{2}$ 。虽然猎物的策略 $\boldsymbol{\theta}^{2}$ 没有变化, 但是它的目标函数 $J^{2}$ 会减 小。

非合作关系设定下的策略学习 : 在多智能体的策略学习中, 第 $i$ 号智能体的目标是 改进自己的策略参数 $\boldsymbol{\theta}^{i}$, 使得 $J^{i}$ 尽量大。多智能体的策略学习可以描述为这样的问题:

$$
\begin{array}{cc}
\text { 第 } 1 \text { 个智能体求解 : } & \max _{\boldsymbol{\theta}^{1}} J^{1}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right) \text {, } \\
\text { 第 } 2 \text { 个智能体求解 : } & \max _{\boldsymbol{\theta}^{2}} J^{2}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right), \\
\vdots & \vdots \\
\text { 第 } m \text { 个智能体求解 : } & \max _{\boldsymbol{\theta}^{m}} J^{m}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right) \text {. }
\end{array}
$$

注意, 目标函数 $J^{1}, J^{2}, \cdots, J^{m}$ 是各不相同的, 也就是说智能体没有共同的目标（除非 是完全合作关系)。策略学习的基本思想是让每个智能体各自做策略梯度上升:

$$
\begin{array}{ccc}
\text { 第 } 1 \text { 号智能体执行 : } & \boldsymbol{\theta}^{1} & \leftarrow \boldsymbol{\theta}^{1}+\alpha^{1} \cdot \nabla_{\boldsymbol{\theta}^{1}} J^{1}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right), \\
\text { 第 } 2 \text { 号智能体执行 : } & \boldsymbol{\theta}^{2} & \leftarrow \boldsymbol{\theta}^{2}+\alpha^{2} \cdot \nabla_{\boldsymbol{\theta}^{2}} J^{2}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right), \\
\vdots & & \vdots \\
\text { 第 } m \text { 号智能体执行 : } & \boldsymbol{\theta}^{m} & \leftarrow \boldsymbol{\theta}^{m}+\alpha^{m} \cdot \nabla_{\boldsymbol{\theta}^{m}} J^{m}\left(\boldsymbol{\theta}^{1}, \ldots, \boldsymbol{\theta}^{m}\right) .
\end{array}
$$

公式中的 $\alpha^{1}, \alpha^{2}, \cdots, \alpha^{m}$ 是学习率。由于无法直接计算策略梯度 $\nabla_{\boldsymbol{\theta}^{i}} J^{i}$, 我们需要对其做 近似。各种策略学习方法的区别就在于如何对策略梯度做近似。

收敛的判别 : 在合作关系设定下, 所有智能体有相同的目标函数 $\left(J^{1}=\cdots=J^{m}\right)$, 那么判断收敛的标准就是目标函数值不再增长。也就是说改变任何智能体的策略都无法 让团队的回报增长。

在非合作关系设定下, 智能体的利益是不一致的、甚至是冲突的, 智能体各有各的目 标函数。该如何判断策略学习的收敛呢? 不能用 $J^{1}+J^{2}+\cdots+J^{m}$ 作为判断收敛的标准。 比如在 predator-prey（捕食者一猎物）的游戏中, 双方的目标函数是冲突的： $J^{1}=-J^{2}$ 。 如果捕食者改进策略, 那么 $J^{1}$ 会增长, 而 $J^{2}$ 会下降。自始至终, $J^{1}+J^{2}$ 一直等于零, 不论策略学习有没有收敛。

在非合作关系设定下, 收玫标准是纳什均衡。一个智能体在制定策略的时候, 要考 虑到其他各方的策略。在纳什均衡的情况下, 每一个智能体都在以最优的方式来应对其 他各方的策略。在纳什均衡的情况下, 谁也没有动机去单独改变自己的策略, 因为改变 策略不会增加自己的收益。这样就达到了一种平衡状态, 所有智能体都找不到更好的策 略。这种平衡状态就被认为是收玫。在实验中, 如果所有智能体的平均回报都不再变化, 就可以认为达到了纳什均衡。

## 定义 16.1. 纳什均衡}

多智能体系统中, 在其余所有智能体都不改变策略的情况下, 一个智能体 $i$ 单独改 变策略 $\boldsymbol{\theta}^{i}$ ，无法让其期望回报 $J^{i}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)$ 变大。

评价策略的优劣 : 有两种策略学习的方法 $\mathcal{M}_{+}$和 $\mathcal{M}_{-}$, 把它们训练出的策略网络 参数分别记作 $\theta_{+}^{1}, \cdots, \theta_{+}^{m}$ 和 $\theta_{-}^{1}, \cdots, \theta_{-}^{m}$ 。该如何评价 $\mathcal{M}_{+}$和 $\mathcal{M}_{-}$的优劣呢? 在合作关 系设定下, 很容易评价两种方法的好坏。在收玫之后, 把两种策略的平均回报记作 $J_{+}$和 $J_{-}$。如果 $J_{+}>J_{-}$, 就说明 $\mathcal{M}_{+}$比 $\mathcal{M}_{-}$好; 反之亦然。

在非合作关系的设定下, 不能直接用平均回报评价策略的优劣。以捕食者一猎物的 游戏为例, 我们用两种方法 $\mathcal{M}_{+}$和 $\mathcal{M}_{-}$训练策略网络, 把它们训练出的策略网络记作:

$$
\begin{array}{ll}
\pi\left(a \mid s, \boldsymbol{\theta}_{+}^{\text {predator }}\right), & \pi\left(a \mid s, \boldsymbol{\theta}_{+}^{\text {prey }}\right), \\
\pi\left(a \mid s, \boldsymbol{\theta}_{-}^{\text {predator }}\right), & \pi\left(a \mid s, \boldsymbol{\theta}_{-}^{\text {prey }}\right) .
\end{array}
$$

设收敛时的平均回报为:

$$
\begin{aligned}
& J_{+}^{\text {predator }}=0.8, \quad J_{+}^{\text {prey }}=-0.8, \\
& J_{-}^{\text {predator }}=0.1, \quad J_{-}^{\text {prey }}=-0.1 .
\end{aligned}
$$

请问 $\mathcal{M}_{+}$和 $\mathcal{M}_{-}$孰优孰劣呢? 假如我们的目标是学出强大的捕食者 (predator), 能否说 明 $\mathcal{M}_{+}$比 $\mathcal{M}_{-}$好呢? 答案是否定的。 $J_{+}^{\text {predator }}>J_{-}^{\text {predator }}$ 可能是由于方法 $\mathcal{M}_{+}$没有训练 好猎物（prey）的策略 $\theta_{+}^{\text {prey }}$, 导致捕食者（predator）相对有优势。 $J_{+}^{\text {predator }}>J_{-}^{\text {predator }}$ 不 能说明策略 $\theta_{+}^{\text {predator }}$ 优于 $\theta_{-}^{\text {predator }}$ 。

在非合作关系的设定下, 该如何评价两种方法 $\mathcal{M}_{+}$和 $\mathcal{M}_{-}$的优劣呢? 以捕食者一 猎物的游戏为例, 我们让一种方法训练出的捕食者与另一种方法训练出的猎物对决:

$$
\begin{array}{lll}
\pi\left(a \mid s, \boldsymbol{\theta}_{+}^{\text {predator }}\right) & \text { 对决 } & \pi\left(a \mid s, \boldsymbol{\theta}_{-}^{\text {prey }}\right), \\
\pi\left(a \mid s, \boldsymbol{\theta}_{-}^{\text {predator }}\right) & \text { 对决 } & \pi\left(a \mid s, \boldsymbol{\theta}_{+}^{\text {prey }}\right) .
\end{array}
$$

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-250.jpg?height=63&width=1462&top_left_y=471&top_left_x=360)
$\mathcal{M}_{-}$的优劣。

## $16.2$ 非合作设定下的多智能体 $\mathrm{A2C}$

本节研究“非合作关系”设定下的多智能体 A2C 方法 (multi-agent non-cooperative A2C), 缩写 MAN-A2C。MAN-A2C 是本书创造出来的方法, 目的在于帮助读者理解“非合作关 系”设定下的策略学习、以及与 “合作关系”的区别。

### 1 策略网络和价值网络

MAN-A2C 中, 每个智能体有自己的策略网络和价值网络, 记作:

$$
\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right) \text { 和 } v\left(s ; \boldsymbol{w}^{i}\right) .
$$

第 $i$ 号策略网络需要把所有智能体的观测 $s=\left[o^{1}, \cdots, o^{m}\right]$ 作为输入, 并输出一个概率 分布; 第 $i$ 个智能体依据该概率分布抽样得到动作 $a^{i}$ 。两类神经网络的结构与上一章的 MAC-A2C 完全相同。请注意上一章 MAC-A2C 与本章 MAN-A2C 的区别：

- 上一章的 MAC-A2C 用于完全合作关系, 所有智能体有相同的状态价值函数 $V_{\pi}(s)$, 所以只用一个神经网络近似 $V_{\pi}(s)$, 记作 $v(s ; \boldsymbol{w})$ 。

- 本章的 MAN-A2C 用于非合作关系, 每个智能体各有一个状态价值函数 $V_{\pi}^{i}(s)$, 所 以每个智能体各自对应一个价值网络 $v\left(s ; \boldsymbol{w}^{i}\right)$ 。

MAN-A2C 属于 actor-critic 方法: 策略网络 $\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$ 相当于第 $i$ 个运动员, 负责做决 策。每个运动员都一个专属的评委 $v\left(s ; \boldsymbol{w}^{i}\right)$, 对运动员 $i$ 的表现予以评价。请注意, 虽然评 委 $v\left(s ; \boldsymbol{w}^{i}\right)$ 是对运动员 $i$ 个人做出评价, 但是评委会考虑到全局的状态 $s=\left[o^{1}, \cdots, o^{m}\right]$ 。 举个例子, 在足球比赛中, 评委 $i$ 只对运动员 $i$ 做评价, 目的在于改进运动员 $i$ 的技术。 在比赛中, 想要评价运动员 $i$ 的跑位、传球的好坏, 还需要要考虑到队友、对手的位置, 所以评委 $i$ 会考虑到场上所有球员的表现 $s=\left[o^{1}, \cdots, o^{m}\right]$ 。注意与上一章中 MAC-A2C 的区别：MAC-A2C 中只有一位评委, 他会点评整个团队的表现, 而不会给每位运动员单 独一个评分。

### 2 算法推导}

在非合作关系设定下, 第 $i$ 号智能体的动作价值函数记作 $Q_{\pi}^{i}(s, a)$, 策略网络记作 $\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right)$ 。不难证明下面的策略梯度定理：

## 定理 16.1. 非合作关系 MARL 的策略梯度定理

设基线 $b$ 为不依赖于 $A=\left[A^{1}, \cdots, A^{m}\right]$ 的函数。那么有

$$
\nabla_{\boldsymbol{\theta}^{i}} J^{i}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S, A}\left[\left(Q_{\pi}^{i}(S, A)-b\right) \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(A^{i} \mid S ; \boldsymbol{\theta}^{i}\right)\right] .
$$

期望中的动作 $A$ 的概率质量函数为

$$
\pi\left(A \mid S ; \boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right) \triangleq \pi\left(A^{1} \mid S ; \boldsymbol{\theta}^{1}\right) \times \cdots \times \pi\left(A^{m} \mid S ; \boldsymbol{\theta}^{m}\right) .
$$

我们用 $b=V_{\pi}^{i}(s)$ 作为定理中的基线, 并且用价值网络 $v\left(s ; \boldsymbol{w}^{i}\right)$ 近似 $V_{\pi}^{i}(s)$ 。按照上 一章的算法推导, 我们可以把策略梯度 $\nabla_{\boldsymbol{\theta}^{i}} J^{i}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)$ 近似成:

$$
\tilde{\boldsymbol{g}}^{i}\left(s_{t}, a_{t}^{i} ; \boldsymbol{\theta}^{i}\right) \triangleq\left(r_{t}^{i}+\gamma \cdot v\left(s_{t+1} ; \boldsymbol{w}^{i}\right)-v\left(s_{t} ; \boldsymbol{w}^{i}\right)\right) \cdot \nabla_{\boldsymbol{\theta}^{i}} \pi\left(a_{t}^{i} \mid s_{t} ; \boldsymbol{\theta}^{i}\right) .
$$

观测到状态 $s_{t} 、 s_{t+1}$ 、动作 $a_{t}^{i}$ 、奖励 $r_{t}^{i}$, 这样更新策略网络参数：

$$
\boldsymbol{\theta}^{i} \leftarrow \boldsymbol{\theta}^{i}+\beta \cdot \tilde{\boldsymbol{g}}^{i}\left(s_{t}, a_{t}^{i} ; \boldsymbol{\theta}^{i}\right) .
$$

更新价值网络 $v\left(s ; \boldsymbol{w}^{i}\right)$ 的方法与 $\mathrm{A} 2 \mathrm{C}$ 基本一样。在观测到状态 $s_{t} 、 s_{t+1}$ 、奖励 $r_{t}^{i}$ 之后, 计算 TD 目标：

$$
\widehat{y}_{t}^{i}=r_{t}^{i}+\gamma \cdot v\left(s_{t+1} ; \boldsymbol{w}^{i}\right) .
$$

然后用 $\mathrm{TD}$ 算法更新参数 $\boldsymbol{w}^{i}$, 使得 $v\left(s_{t} ; \boldsymbol{w}^{i}\right)$ 更接近 $\widehat{y}_{t}^{i}$ 。

### 3 训练和决策

训练 : 实现 MAN-A2C 的时候, 应当使用目标网络缓解自举造成的偏差。第 $i$ 号智 能体的目标网络记作 $v\left(s ; \boldsymbol{w}^{i-}\right)$, 它的结构与 $v\left(s ; \boldsymbol{w}^{i}\right)$ 相同, 但是参数不同。设第 $i$ 号智 能体策略网络、价值网络、目标网络当前的参数是 $\boldsymbol{\theta}_{\text {now }}^{i} \boldsymbol{w}_{\text {now }}^{i} \boldsymbol{w}_{\text {now }}^{i-}$ MAN-A2C 重复 下面的步骤更新参数：

1. 观测到当前状态 $s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$, 让每一个智能体独立做随机抽样:

$$
a_{t}^{i} \sim \pi\left(\cdot \mid s_{t} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m,
$$

并执行选中的动作。

2. 从环境中观测到奖励 $r_{t}^{1}, \cdots, r_{t}^{m}$ 与下一时刻状态 $s_{t+1}=\left[o_{t+1}^{1}, \cdots, o_{t+1}^{m}\right]$ 。

3. 让价值网络做预测：

$$
\widehat{v}_{t}^{i}=v\left(s_{t} ; \boldsymbol{w}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

4. 让目标网络做预测：

$$
\widehat{v}_{t+1}^{i-}=v\left(s_{t+1} ; \boldsymbol{w}_{\text {now }}^{i-}\right), \quad \forall i=1, \cdots, m .
$$

5. 计算 TD 目标与 TD 误差：

$$
\widehat{y}_{t}^{i}=r_{t}^{i}+\gamma \cdot \widehat{v}_{t+1}^{i-}, \quad \delta_{t}^{i}=\widehat{v}_{t}^{i}-\widehat{y}_{t}^{i}, \quad \forall i=1, \cdots, m .
$$

6. 更新价值网络参数:

$$
\boldsymbol{w}_{\text {new }}^{i} \leftarrow \boldsymbol{w}_{\text {now }}^{i}-\alpha \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{w}^{i}} v\left(s_{t} ; \boldsymbol{w}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

7. 更新目标网络参数：

$$
\boldsymbol{w}_{\text {new }}^{i-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}^{i}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{i-}, \quad \forall i=1, \cdots, m .
$$

8. 更新策略网络参数：

$$
\boldsymbol{\theta}_{\text {new }}^{i} \leftarrow \boldsymbol{\theta}_{\text {now }}^{i}-\beta \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid s_{t} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

MAN-A2C 属于同策略 (on-policy), 不能使用经验回放。 决策 : 在完成训练之后, 不再需要价值网络 $v\left(s ; \boldsymbol{w}^{1}\right), \cdots, v\left(s ; \boldsymbol{w}^{m}\right)$ 。每个智能体可 以用它自己的策略网络做决策。在时刻 $t$ 观测到全局状态 $s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$, 然后做随机 抽样得到动作：

$$
a_{t}^{i} \sim \pi\left(\cdot \mid s_{t} ; \boldsymbol{\theta}^{i}\right),
$$

并执行动作 $a_{t}^{i}$ 。智能体并不能独立做决策, 因为策略网络需要知道所有的观测 $s_{t}=$ $\left[o_{t}^{1}, \cdots, o_{t}^{m}\right]$ 。

## 3 三种架构

本节介绍 MAN-A2C 的三种实现方法: “中心化训练 + 中心化决策” “去中心化训练 + 去中心化决策”、“心化训练 + 去中心化决策”。

### 1 中心化训练 + 中心化决策

首先讲解用完全中心化 (fully centralized) 的方式实现 MAN-A2C 的训练和决策。这 种方式是不实用的, 仅帮助大家理解算法而已。图 $16.2$ 描述了系统的架构。最上面是中 央控制器 (central controller), 里面部署了所有 $m$ 个价值网络和策略网络：

$$
\begin{array}{rccc}
v\left(s ; \boldsymbol{w}^{1}\right), & v\left(s ; \boldsymbol{w}^{2}\right), & \cdots, & v\left(s ; \boldsymbol{w}^{m}\right), \\
\pi\left(a^{1} \mid s, \boldsymbol{\theta}^{1}\right), & \pi\left(a^{2} \mid s, \boldsymbol{\theta}^{2}\right), & \cdots, & \pi\left(a^{m} \mid s, \boldsymbol{\theta}^{m}\right) .
\end{array}
$$

训练和决策全部由中央控制器完成。智能体负责与环境交互, 执行中央控制器的决策 $a^{i}$, 并把观测到的 $o^{i}$ 和 $r^{i}$ 汇报给中央控制器。这种中心化的方式严格实现了上一节的算法。

中央控制器

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-254.jpg?height=868&width=1237&top_left_y=1291&top_left_x=478)

环境

图 16.2: 中心化训练 + 中心化决策的系统架构。

在上一章中, 我们用完全中心化的方式实现了 MAC-A2C (见图 15.5)。请注意 MAC$\mathrm{A} 2 \mathrm{C}$ 与此处的 MAN-A2C 的区别。第一, MAC-A2C 的中央控制器上只有一个价值网络, 而此处 MAN-A2C 则有 $m$ 个价值网络。第二, MAC-A2C 的每一轮只有一个全局的奖励 $r$, 而 MAN-A2C 的每个智能体都有自己的奖励 $r^{i}$ 。

### 2 去中心化训练 $+$ 去中心化决策

为了避免“完全中心化”中的通信, 可以对策略网络和价值网络做近似, 做到“完全去 中心化”把 MAN-A2C 中的策略网络和价值网络做近似:

$$
\begin{aligned}
\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right) & \Longrightarrow \pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right), \\
v\left(s ; \boldsymbol{w}^{i}\right) & \Longrightarrow v\left(o^{i} ; \boldsymbol{w}^{i}\right) .
\end{aligned}
$$

图 $16.3$ 描述了“完全去中心化”的系统架构。每个智能体上部署一个策略网络和一个价值 网络, 它们的参数记作 $\boldsymbol{\theta}^{i}$ 和 $\boldsymbol{w}^{i}$; 智能体之间不共享参数。这样一来, 训练就可以在智 能体本地完成, 无需中央控制器的参与, 也无需通信。这种实现的本质是单智能体强化 学习, 而非多智能体强化学习。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-255.jpg?height=528&width=1404&top_left_y=924&top_left_x=266)

图 16.3: 去中心化训练 + 去中心化决策的系统架构。这种方法也叫做 independent actor-critic。

此处的的实现与上一章“完全合作关系”设定下的“完全去中心化”几乎完全相同（见 图 15.6)。唯一的区别在于此处每个智能体获得的奖励 $r^{i}$ 是不同的, 而上一章“完全合作 关系”设定下的奖励是相同的 $r^{1}=\cdots=r^{m}=r$.

### 3 中心化训练 $+$ 去中心化决策

第三种实现方式是“中心化训练 + 去中心化决策”。与“完全中心化”的 MAN-A2C 相 比, 唯一的区别在于对策略网络做近似:

$$
\pi\left(a^{i} \mid s ; \boldsymbol{\theta}^{i}\right) \Longrightarrow \pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right), \quad \forall i=1, \cdots, m .
$$

由于用智能体局部观测 $o^{i}$ 替换了全局状态 $s=\left[o^{1}, \cdots, o^{m}\right]$, 策略网络可以部署到每个智 能体上。而价值网络仍然是 $v\left(s ; \boldsymbol{w}^{i}\right)$, 没有做近似。

图 $16.4$ 描述了“中心化训练 + 去中心化决策”的系统架构。中央控制器上有所有的价 值网络及其目标网络（图中没有画出目标网络）:

$$
\begin{array}{cccc}
v\left(s ; \boldsymbol{w}^{1}\right), & v\left(s ; \boldsymbol{w}^{2}\right), & \cdots, & v\left(s ; \boldsymbol{w}^{m}\right), \\
v\left(s ; \boldsymbol{w}^{1-}\right), & v\left(s ; \boldsymbol{w}^{2-}\right), & \cdots, & v\left(s ; \boldsymbol{w}^{m-}\right) .
\end{array}
$$

中央控制器用智能体发来的观测 $\left[o^{1}, \cdots, o^{m}\right]$ 和奖励 $\left[r^{1}, \cdots, r^{m}\right]$ 训练这些价值网络。中 央控制器把 TD 误差 $\delta^{1}, \cdots, \delta^{m}$ 反馈给智能体; 第 $i$ 号智能体用 $\delta^{i}$ 以及本地的 $o^{i} 、 a^{i}$ 来 训练自己的策略网络。

中央控制器

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-256.jpg?height=722&width=1402&top_left_y=410&top_left_x=378)

图 16.4: 中心化训练的系统架构。所有 $m$ 个价值网络部署到中央控制器上, 策略网络部署到每个 智能体上。

上一章“完全合作关系”设定下的“中心化训练”只在中央控制器上部署一个价值网络 $v(s ; \boldsymbol{w})$ 。而此处中央控制器上有 $m$ 个价值网络, 每个价值网络对应一个智能体。这是 因为此处是 “非合作关系”, 每个智能体各自对应一个状态价值函数 $V_{\pi}^{i}(s)$, 而非有共用的 $V_{\pi}$ 。

中心化训练: 训练的过程需要所有 $m$ 个智能体共同参与, 共同改进策略网络参数 $\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}$ 与价值网络参数 $\boldsymbol{w}^{1}, \cdots, \boldsymbol{w}^{m}$ 。设第 $i$ 号智能体的策略网络、价值网络、目标 网络当前的参数分别是 $\boldsymbol{\theta}_{\text {now }}^{i} \boldsymbol{w}_{\text {now }}^{i}, \boldsymbol{w}_{\text {now }}^{i-}$ 训练的流程如下:

1. 每个智能体 $i$ 与环境交互, 获取当前观测 $o_{t}^{i}$, 独立做随机抽样:

$$
a_{t}^{i} \sim \pi\left(\cdot \mid o_{t}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m,
$$

并执行选中的动作。

2. 下一时刻, 每个智能体 $i$ 都观测到 $o_{t+1}^{i}$ 和收到奖励 $r_{t}^{i}$ 。

3. 每个智能体 $i$ 向中央控制器传输观测 $o_{t}^{i} 、 o_{t+1}^{i} 、 r_{t}^{i}$; 中央控制器得到状态

$$
s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right] \quad \text { 和 } \quad s_{t+1}=\left[o_{t+1}^{1}, \cdots, o_{t+1}^{m}\right] .
$$

4. 让价值网络做预测:

$$
\widehat{v}_{t}^{i}=v\left(s_{t} ; \boldsymbol{w}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

5. 让目标网络做预测：

$$
\widehat{v}_{t+1}^{i-}=v\left(s_{t+1} ; \boldsymbol{w}_{\text {now }}^{i-}\right), \quad \forall i=1, \cdots, m .
$$

6. 计算 TD 目标与 TD 误差:

$$
\widehat{y}_{t}^{i}=r_{t}^{i}+\gamma \cdot \widehat{v}_{t+1}^{i-}, \quad \delta_{t}^{i}=\widehat{v}_{t}^{i}-\widehat{y}_{t}^{i}, \quad \forall i=1, \cdots, m
$$

7. 更新价值网络参数：

$$
\boldsymbol{w}_{\text {new }}^{i} \leftarrow \boldsymbol{w}_{\text {now }}^{i}-\alpha \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{w}^{i}} v\left(s_{t} ; \boldsymbol{w}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m
$$

8. 更新目标网络参数：

$$
\boldsymbol{w}_{\text {new }}^{i-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}^{i}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{i-}, \quad \forall i=1, \cdots, m .
$$

9. 更新策略网络参数:

$$
\boldsymbol{\theta}_{\text {new }}^{i} \leftarrow \boldsymbol{\theta}_{\text {now }}^{i}-\beta \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{\theta}^{i}} \ln \pi\left(a_{t}^{i} \mid o_{t}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

去中心化决策 : 在完成训练之后, 不再需要价值网络 $v\left(s ; \boldsymbol{w}^{1}\right), \cdots, v\left(s ; \boldsymbol{w}^{m}\right)$ 。智能 体只需要用其本地部署的策略网络 $\pi\left(a^{i} \mid o^{i} ; \boldsymbol{\theta}^{i}\right)$ 做决策, 决策过程无需通信, 因此决策速 度很快。

## $16.4$ 连续控制与 MADDPG

前两节的 MAN-A2C 仅限于离散控制。本节研究连续控制问题, 即动作空间 $\mathcal{A}^{1}, \mathcal{A}^{2}$, $\cdots, \mathcal{A}^{m}$ 都是连续集合, 动作 $\boldsymbol{a}^{i} \in \mathcal{A}^{i}$ 是向量。本节介绍一种适用于连续控制的多智能 体强化学习 (MARL) 方法。多智能体深度确定策略梯度 (multi-agent deep deterministic policy gradient, 缩写 MADDPG) 是一种很有名的 MARL 方法, 它的架构是 “中心化训练 $+$ 去中心化决策”。

### 1 策略网络和价值网络

设系统里有 $m$ 个智能体。每个智能体对应一个策略网络和一个价值网络：

$$
\boldsymbol{\mu}\left(o^{i} ; \boldsymbol{\theta}^{i}\right) \text { 和 } q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{i}\right) \text {. }
$$

策略网络是确定性的：对于确定的输入 $o^{i}$, 输出的动作 $\boldsymbol{a}^{i}=\boldsymbol{\mu}\left(o^{i} ; \boldsymbol{\theta}^{i}\right)$ 是确定的。价值网 络的输入是全局状态 $s=\left[o^{1}, \cdots, o^{m}\right]$ 与所有智能体的动作 $\boldsymbol{a}=\left[\boldsymbol{a}^{1}, \cdots, \boldsymbol{a}^{m}\right]$, 输出是一 个实数, 表示 “基于状态 $s$ 执行动作 $\boldsymbol{a}$ ”的好坏程度。第 $i$ 号策略网络 $\boldsymbol{\mu}\left(o^{i} ; \boldsymbol{\theta}^{i}\right)$ 用于控制第 $i$ 号智能体, 而价值网络 $q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{i}\right)$ 则用于评价所有动作 $\boldsymbol{a}$, 给出的分数可以指导第 $i$ 号 策略网络做出改进; 见图 16.5。MADDPG 因此可以看做一种 actor-critic 方法。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-258.jpg?height=1022&width=1356&top_left_y=1345&top_left_x=424)

图 16.5: 所有智能体的策略网络与第 $i$ 号智能体的价值网络。

思考一个问题: 既然第 $i$ 号价值网络仅仅评价第 $i$ 号智能体表现的好坏, 为什么第 $i$ 号价值网络需要知道所有智能体的观测 $\left[o^{1}, \cdots, o^{m}\right]$ 与所有智能体的动作 $\left[\boldsymbol{a}^{1}, \cdots, \boldsymbol{a}^{m}\right]$ 呢? 下面举个例子来解答这个问题。在足球机器人的比赛中, 我们用第 $i$ 号价值网络评 价第 $i$ 号机器人的表现。当前第 $i$ 号机器人向前冲, 请问这个动作好不好? 仅仅知道它 自己的观测和动作, 不足以评价 “向前冲”是否是正确的动作。要结合队友、对手的位置、 动作等信息 (比如攻守、跑位、传球), 才能判断第 $i$ 号机器人当前做的动作好不好。

### 2 算法推导

训练策略网络和价值网络的算法与第 $10.2$ 节的单智能体 DPG 非常类似：用确定策 略梯度更新策略网络, 用 TD 算法更新价值网络。MADDPG 是异策略 (off-policy), 我们 可以使用经验回放, 重复利用过去的经验。用一个经验回放数组存储收集到的经验, 每 一条经验都是 $\left(s_{t}, \boldsymbol{a}_{t}, r_{t}, s_{t+1}\right)$ 这样一个四元组, 其中

$$
\begin{aligned}
s_{t} & =\left[o_{t}^{1}, \cdots, o_{t}^{m}\right], \\
\boldsymbol{a}_{t} & =\left[\boldsymbol{a}_{t}^{1}, \cdots, \boldsymbol{a}_{t}^{m}\right], \\
s_{t+1} & =\left[o_{t+1}^{1}, \cdots, o_{t+1}^{m}\right], \\
r_{t} & =\left[r_{t}^{1}, \cdots, r_{t}^{m}\right] .
\end{aligned}
$$

训练策略网络 : 训练第 $i$ 号策略网络 $\boldsymbol{\mu}\left(o^{i} ; \boldsymbol{\theta}^{i}\right)$ 的目标是改进 $\boldsymbol{\theta}^{i}$, 增大第 $i$ 号价值网 络的平均打分。所以目标函数是:

$$
\widehat{J}^{i}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S}\left[q\left(S,\left[\boldsymbol{\mu}\left(O^{1} ; \boldsymbol{\theta}^{1}\right), \cdots, \boldsymbol{\mu}\left(O^{i} ; \boldsymbol{\theta}^{i}\right), \cdots, \boldsymbol{\mu}\left(O^{m} ; \boldsymbol{\theta}^{m}\right)\right] ; \boldsymbol{w}^{i}\right)\right] .
$$

公式中的期望是关于状态 $S=\left[O^{1}, \cdots, O^{m}\right]$ 求的。目标函数的梯度等于： $\nabla_{\boldsymbol{\theta}^{i}} \widehat{J}^{i}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)=\mathbb{E}_{S}\left[\nabla_{\boldsymbol{\theta}^{i}} q\left(S,\left[\boldsymbol{\mu}\left(O^{1} ; \boldsymbol{\theta}^{1}\right), \cdots, \boldsymbol{\mu}\left(O^{i} ; \boldsymbol{\theta}^{i}\right), \cdots, \boldsymbol{\mu}\left(O^{m} ; \boldsymbol{\theta}^{m}\right)\right] ; \boldsymbol{w}^{i}\right)\right]$. 接下来用蒙特卡洛近似公式中的期望。从经验回放数组中随机抽取一个状态： 1

$$
s_{t}=\left[o_{t}^{1}, \cdots, o_{t}^{m}\right],
$$

它可以看做是随机变量 $S$ 的一个观测值。用所有 $m$ 个策略网络计算动作

$$
\widehat{\boldsymbol{a}}_{t}^{1}=\boldsymbol{\mu}\left(o_{t}^{1} ; \boldsymbol{\theta}^{1}\right), \quad \cdots, \quad \widehat{\boldsymbol{a}}_{t}^{m}=\boldsymbol{\mu}\left(o_{t}^{m} ; \boldsymbol{\theta}^{m}\right) .
$$

那么目标函数的梯度 $\nabla_{\boldsymbol{\theta}^{i}} \widehat{J}^{i}\left(\boldsymbol{\theta}^{1}, \cdots, \boldsymbol{\theta}^{m}\right)$ 可以近似成为:

$$
\begin{aligned}
\boldsymbol{g}_{\theta}^{i} & =\nabla_{\boldsymbol{\theta}^{i}} q\left(s_{t},\left[\boldsymbol{\mu}\left(o_{t}^{1} ; \boldsymbol{\theta}^{1}\right), \cdots, \boldsymbol{\mu}\left(o_{t}^{i} ; \boldsymbol{\theta}^{i}\right), \cdots, \boldsymbol{\mu}\left(o_{t}^{m} ; \boldsymbol{\theta}^{m}\right)\right] ; \boldsymbol{w}^{i}\right) \\
& =\nabla_{\boldsymbol{\theta}^{i}} q\left(s_{t},\left[\widehat{\boldsymbol{a}}_{t}^{1}, \cdots, \widehat{\boldsymbol{a}}_{t}^{m}\right] ; \boldsymbol{w}^{i}\right) .
\end{aligned}
$$

由于 $\widehat{\boldsymbol{a}}_{t}^{i}=\boldsymbol{\mu}\left(o_{t}^{i} ; \boldsymbol{\theta}^{i}\right)$, 用链式法则可得：

$$
\boldsymbol{g}_{\theta}^{i}=\nabla_{\boldsymbol{\theta}^{i}} \boldsymbol{\mu}\left(o_{t}^{i} ; \boldsymbol{\theta}^{i}\right) \cdot \nabla_{\widehat{\boldsymbol{a}}^{i}} q\left(s_{t},\left[\widehat{\boldsymbol{a}}_{t}^{1}, \cdots, \widehat{\boldsymbol{a}}_{t}^{m}\right] ; \boldsymbol{w}^{i}\right) .
$$

做梯度上升更新参数 $\boldsymbol{\theta}^{i}$ :

$$
\boldsymbol{\theta}^{i} \leftarrow \boldsymbol{\theta}^{i}+\beta \cdot \boldsymbol{g}_{\theta}^{i} .
$$

注意, 在更新第 $i$ 号策略网络的时候, 除了用到全局状态 $s_{t}$, 还需要用到所有智能体的

1更新策略网络只需要四元组 $\left(s_{t}, \boldsymbol{a}_{t}, r_{t}, s_{t+1}\right)$ 中的 $s_{t}$, 没有用其余三个。 策略网络, 以及第 $i$ 号价值网络 $q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{i}\right)$ 。

训练价值网络：可以用 TD 算法训练第 $i$ 号价值网络 $q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{i}\right)$, 让价值网络更好 拟合价值函数 $Q_{\pi}^{i}(s, \boldsymbol{a})$ 。给定四元组 $\left(s_{t}, \boldsymbol{a}_{t}, r_{t}, s_{t+1}\right)$, 用所有 $m$ 个策略网络计算动作

$$
\widehat{\boldsymbol{a}}_{t+1}^{1}=\boldsymbol{\mu}\left(o_{t+1}^{1} ; \boldsymbol{\theta}^{1}\right), \quad \cdots, \quad \widehat{\boldsymbol{a}}_{t+1}^{m}=\boldsymbol{\mu}\left(o_{t+1}^{m} ; \boldsymbol{\theta}^{m}\right) .
$$

设 $\widehat{\boldsymbol{a}}_{t+1}=\left[\widehat{\boldsymbol{a}}_{t+1}^{1}, \cdots, \widehat{\boldsymbol{a}}_{t+1}^{m}\right]$ 。然后计算 TD 目标：

$$
\widehat{y}_{t}^{i}=r_{t}^{i}+\gamma \cdot q\left(s_{t+1}, \widehat{\boldsymbol{a}}_{t+1} ; \boldsymbol{w}^{i}\right) .
$$

再计算 TD 误差：

$$
\delta_{t}^{i}=q\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{w}^{i}\right)-\widehat{y}_{t}^{i} .
$$

最后做梯度下降更新参数 $\boldsymbol{w}^{i}$ :

$$
\boldsymbol{w}^{i} \leftarrow \boldsymbol{w}^{i}-\alpha \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{w}^{i}} q\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{w}^{i}\right) .
$$

这样可以让价值网络的预测 $q\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{w}^{i}\right)$ 更接近 TD 目标 $\widehat{y}_{t}^{i}$ 。

中央控制器

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-260.jpg?height=1034&width=1231&top_left_y=1353&top_left_x=481)

环境

图 16.6: MADDPG 的中心化训练。

### 3 中心化训练

为了训练第 $i$ 号策略网络和第 $i$ 号价值网络, 我们需要用到如下信息: 经验回放数 组中的一条记录 $\left(s_{t}, \boldsymbol{a}_{t}, s_{t+1}, r_{t}^{i}\right)$ 、所有 $m$ 个策略网络、以及第 $i$ 号价值网络。很显然, 一 个智能体不可能有所有这些信息, 因此 MADDPG 需要“中心化训练”。

中心化训练的系统架构如图 $16.6$ 所示。有一个中央控制器, 上面部署所有的策略网 络和价值网络。训练过程中, 策略网络部署到中央控制器上, 所以智能体不能自主做决 策, 智能体只是执行中央控制器发来的指令。由于训练使用异策略, 可以把收集经验和 更新神经网络参数分开做。

用行为策略收集经验。行为策略 (behavior policy) 可以不同于目标策略 (target policy, 即 $\boldsymbol{\mu})$ 。行为策略是什么都无所谓, 比如第 $i$ 个智能体的行为策略可以是

$$
\boldsymbol{a}^{i}=\boldsymbol{\mu}\left(o^{i} ; \boldsymbol{\theta}_{\mathrm{old}}^{i}\right)+\boldsymbol{\epsilon},
$$

其中 $\boldsymbol{\epsilon}$ 是与 $\boldsymbol{a}^{i}$ 维度相同的向量, 每个元素都是从正态分布中独立抽取的, 相当于随机噪 声。具体实现的时候, 智能体把其观测 $o^{i}$ 发送给中央控制器。控制器往第 $i$ 号策略网络 输出的动作中加入随机噪声 $\boldsymbol{\epsilon}$, 把带噪声的动作 $\boldsymbol{a}^{i}$ 发送给给第 $i$ 号智能体, 智能体执行 $\boldsymbol{a}^{i}$ 。随后智能体观测到奖励 $r^{i}$, 发送给控制器。控制器把每一轮的 $o^{i}, \boldsymbol{a}^{i}, r^{i}$ 依次存入经 验回放数组。

中央控制器更新策略网络和价值网络: 实际实现的时候, 中央控制器上还需要有如 下目标网络（图 16.6 中没有画出):

$$
\begin{array}{cccc}
\pi\left(\boldsymbol{a}^{1} \mid o^{1} ; \boldsymbol{\theta}^{1-}\right), & \pi\left(\boldsymbol{a}^{2} \mid o^{2} ; \boldsymbol{\theta}^{2-}\right), & \cdots, & \pi\left(\boldsymbol{a}^{m} \mid o^{m} ; \boldsymbol{\theta}^{m-}\right) \\
q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{1-}\right), & q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{2-}\right), & \cdots, & q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{m-}\right) .
\end{array}
$$

设第 $i$ 号智能体当前的参数为:

$$
\boldsymbol{\theta}_{\text {now }}^{i}, \quad \boldsymbol{\theta}_{\text {now }}^{i-}, \quad \boldsymbol{w}_{\text {now }}^{i}, \quad \boldsymbol{w}_{\text {now }}^{i-} .
$$

中央控制器每次从经验回放数组中随机抽取一个四元组 $\left(s_{t}, \boldsymbol{a}_{t}, r_{t}, s_{t+1}\right)$, 然后按照下面 的步骤更新所有策略网络和所有价值网络：

1. 让所有 $m$ 个目标策略网络做预测:

$$
\widehat{\boldsymbol{a}}_{t+1}^{i-}=\boldsymbol{\mu}\left(o_{t+1}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i-}\right), \quad \forall i=1, \cdots, m .
$$

把预测汇总成 $\widehat{\boldsymbol{a}}_{t+1}^{-}=\left[\widehat{\boldsymbol{a}}_{t+1}^{1-}, \cdots, \widehat{\boldsymbol{a}}_{t+1}^{m-}\right]$ 。

2. 让所有 $m$ 个目标价值网络做出预测:

$$
\widehat{q}_{t+1}^{i-}=q\left(s_{t+1}, \widehat{\boldsymbol{a}}_{t+1}^{-} ; \boldsymbol{w}_{\text {now }}^{i-}\right), \quad \forall i=1, \cdots, m .
$$

3. 计算 TD 目标：

$$
\widehat{y}_{t}^{i}=r_{t}^{i}+\gamma \cdot \widehat{q}_{t+1}^{i-}, \quad \forall i=1, \cdots, m
$$

4. 让所有 $m$ 个价值网络做预测:

$$
\widehat{q}_{t}^{i}=q\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{w}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

5. 计算 TD 误差:

$$
\delta_{t}^{i}=\widehat{q}_{t}^{i}-\widehat{y}_{t}^{i}, \quad \forall i=1, \cdots, m .
$$

6. 更新所有 $m$ 个价值网络:

$$
\boldsymbol{w}_{\text {new }}^{i} \leftarrow \boldsymbol{w}_{\text {now }}^{i}-\alpha \cdot \delta_{t}^{i} \cdot \nabla_{\boldsymbol{w}^{i}} q\left(s_{t}, \boldsymbol{a}_{t} ; \boldsymbol{w}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

7. 让所有 $m$ 个策略网络做预测 :

$$
\widehat{\boldsymbol{a}}_{t}^{i}=\boldsymbol{\mu}\left(o_{t}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i}\right), \quad \forall i=1, \cdots, m .
$$

把预测汇总成 $\widehat{\boldsymbol{a}}_{t}=\left[\widehat{\boldsymbol{a}}_{t}^{1}, \cdots, \widehat{\boldsymbol{a}}_{t}^{m}\right]$ 。请区别 $\widehat{\boldsymbol{a}}_{t}$ 与经验回放数组中抽出的 $\boldsymbol{a}_{t}$ 。

8. 更新所有 $m$ 个策略网络： $\forall i=1, \cdots, m$,

$$
\boldsymbol{\theta}_{\text {new }}^{i} \leftarrow \boldsymbol{\theta}_{\text {now }}^{i}-\beta \cdot \nabla_{\boldsymbol{\theta}^{i}} \boldsymbol{\mu}\left(o_{t}^{i} ; \boldsymbol{\theta}_{\text {now }}^{i}\right) \cdot \nabla_{\boldsymbol{a}_{t}^{i}} q\left(s_{t}, \widehat{\boldsymbol{a}}_{t} ; \boldsymbol{w}_{\text {now }}^{i}\right)
$$

9. 更新所有 $2 m$ 个目标网络: $\forall i=1, \cdots, m$,

$$
\begin{gathered}
\boldsymbol{\theta}_{\text {new }}^{i-} \leftarrow \tau \cdot \boldsymbol{\theta}_{\text {new }}^{i}+(1-\tau) \cdot \boldsymbol{\theta}_{\text {now }}^{i-}, \\
\boldsymbol{w}_{\text {new }}^{i-} \leftarrow \tau \cdot \boldsymbol{w}_{\text {new }}^{i}+(1-\tau) \cdot \boldsymbol{w}_{\text {now }}^{i-} .
\end{gathered}
$$

改进方法 : 可以用三种方法改进 MADDPG。第一, 用第 $10.4$ 节中 TD3 的三种技巧 改进训练的算法:

- 用截断双 $\mathbf{Q}$ 学习 (clipped double Q-learning) 训练价值网络 $q\left(s, \boldsymbol{a} ; \boldsymbol{w}^{i}\right), \forall i=1, \cdots m$ 。

- 往训练算法第一步中的 $\widehat{\boldsymbol{a}}_{t+1}^{i-}$ 加入噪声。

- 减小更新策略网络和目标网络的频率, 每更新 $k(>1)$ 次价值网络, 更新一次策略 网络和目标网络。

第二, 按照第 11 章中的方法, 在策略网络和价值网络中使用 RNN, 记忆历史观测。第 三, 在价值网络的结构中使用注意力机制, 见下一章。

### 4 去中心化决策

在完成训练之后, 不再需要价值网络, 只需要策略网络做决策。如图 $16.7$ 所示, 把 策略网络部署到对应的智能体上。第 $i$ 号智能体可以基于本地观测的 $o^{i}$, 在本地独立做 决策: $\boldsymbol{a}^{i}=\boldsymbol{\mu}\left(o^{i} ; \boldsymbol{\theta}^{i}\right)$ 。

![](https://cdn.mathpix.com/cropped/2023_02_03_f46f5cf0e4de5b9996dcg-262.jpg?height=462&width=1385&top_left_y=2082&top_left_x=404)

图 16.7: MADDPG 的去中心化决策。

## 第 16 章 知识点

- 在非合作关系的设定下, 不同的智能体可能会有不同的奖励。因此, 不同的智能体 有不同的回报、价值函数、目标函数。

- 由于不同的智能体有不同的目标函数, 我们无法用单一的目标函数判断收玫, 而应 该用纳什均衡判断收玫。如果在其余智能体不改变策略的情况下, 任意一个智能体 单独改变策略, 它的目标函数不会变好, 那么就达到了纳什均衡。

-对于非合作关系, 最常用的架构也是“中心化训练 + 去中心化决策”。每个智能体有 自己的价值网络和策略网络, 价值网络不共享。

- MADDPG 是确定策略梯度（DPG）的多智能体版本, 用于连续控制。MADDPG 采 用“中心化训练 $+$ 去中心化决策”架构。

## 第 16 章 相关文献

MAN-A2C 是本书设计出来的简单方法, 用于讲解非合作设定下的 MARL, 方便读 者理解。MAN-A2C 这个名字并没有出现在任何文献中。本章介绍的 MADDPG 由 Lowe 等人 2017 年的论文提出 ${ }^{[72]}$, 它的改进版本叫做 MATD3。
