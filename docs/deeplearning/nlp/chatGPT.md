## ChatGPT:优化对话语言模型

Openai 训练了一个叫做ChatGPT的模型，它以对话方式进行交互。对话格式使ChatGPT能够回答后续问题、承认错误、质疑不正确的前提和拒绝不适当的请求。ChatGPT 是 InstructGPT 的同级模型，它经过训练可以按照提示中的说明进行操作并提供详细的响应。

## ChatGPT的技术原理

使用人类反馈强化学习 (RLHF) 训练该模型，但数据收集设置略有不同。 我们首先使用有监督微调的方式训练一个初始模型：由人类 AI 训练员提供对话，他们在对话中扮演用户和 AI 助手的角色。 我们让AI培训师可以访问模型编写的建议，以帮助他们撰写回复。

为了创建强化学习的奖励模型，我们需要收集比较数据，其中包含两个或多个按质量排序的模型响应。 为了收集这些数据，我们收集了 AI 培训师与聊天机器人的对话。 我们随机选择一条由模型编写的消息，抽取了几个备选的完成方式，并让 AI 培训师对它们进行排名。 有了这些奖励模型，进一步我们可以使用近端策略优化（PPO）来微调模型。

整体技术路线上，ChatGPT在效果强大的GPT 3.5大规模语言模型（LLM，Large Language Model）基础上，引入“人工标注数据+强化学习”（RLHF，Reinforcement Learning from Human Feedback ，这里的人工反馈其实就是人工标注数据）来不断Fine-tune预训练语言模型，主要目的是让LLM模型学会理解人类的命令指令的含义（比如给我写一段小作文生成类问题、知识回答类问题、头脑风暴类问题等不同类型的命令），以及让LLM学会判断对于给定的prompt输入指令（用户的问题），什么样的答案是优质的（富含信息、内容丰富、对用户有帮助、无害、不包含歧视信息等多种标准）。

在“人工标注数据+强化学习”框架下，具体而言，ChatGPT的训练过程分为以下三个阶段：

![img](https://cdn.openai.com/chatgpt/draft-20221129c/ChatGPT_Diagram.svg)

## ChatGPT:第一阶段

第一阶段：冷启动阶段的监督策略模型。

靠GPT 3.5本身，尽管它很强，但是它很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。为了让GPT 3.5初步具备理解指令中蕴含的意图，首先会从测试用户提交的prompt(就是指令或问题)中随机抽取一批，靠专业的标注人员，给出指定prompt的高质量答案，然后用这些人工标注好的<prompt,answer>数据来Fine-tune GPT 3.5模型。经过这个过程，我们可以认为GPT 3.5初步具备了理解人类prompt中所包含意图，并根据这个意图给出相对高质量回答的能力，但是很明显，仅仅这样做是不够的。

## ChatGPT:第二阶段

第二阶段：训练回报模型（Reward Model,RM）。

这个阶段的主要目的是通过人工标注训练数据，来训练回报模型。具体而言，随机抽样一批用户提交的prompt(大部分和第一阶段的相同)，使用第一阶段Fine-tune好的冷启动模型，对于每个prompt，由冷启动模型生成K个不同的回答，于是模型产生出了<prompt,answer1>,<prompt,answer2>….<prompt,answerK>数据。之后，标注人员对K个结果按照很多标准（上面提到的相关性、富含信息性、有害信息等诸多标准）综合考虑进行排序，给出K个结果的排名顺序，这就是此阶段人工标注的数据。

接下来，我们准备利用这个排序结果数据来训练回报模型，采取的训练模式其实就是平常经常用到的pair-wise learning to rank。对于K个排序结果，两两组合，形成 (k2) 个训练数据对，ChatGPT采取pair-wise loss来训练Reward Model。RM模型接受一个输入<prompt,answer>，给出评价回答质量高低的回报分数Score。对于一对训练数据<answer1,answer2>，我们假设人工排序中answer1排在answer2前面，那么Loss函数则鼓励RM模型对<prompt,answer1>的打分要比<prompt,answer2>的打分要高。

归纳下：在这个阶段里，首先由冷启动后的监督策略模型为每个prompt产生K个结果，人工根据结果质量由高到低排序，以此作为训练数据，通过pair-wise learning to rank模式来训练回报模型。对于学好的RM模型来说，输入<prompt,answer>，输出结果的质量得分，得分越高说明产生的回答质量越高。

## ChatGPT:第三阶段

第三阶段：采用强化学习来增强预训练模型的能力。

本阶段无需人工标注数据，而是利用上一阶段学好的RM模型，靠RM打分结果来更新预训练模型参数。具体而言，首先，从用户提交的prompt里随机采样一批新的命令（指的是和第一第二阶段不同的新的prompt，这个其实是很重要的，对于提升LLM模型理解instruct指令的泛化能力很有帮助），且由冷启动模型来初始化PPO模型的参数。然后，对于随机抽取的prompt，使用PPO模型生成回答answer， 并用上一阶段训练好的RM模型给出answer质量评估的回报分数score，这个回报分数就是RM赋予给整个回答（由单词序列构成）的整体reward。有了单词序列的最终回报，就可以把每个单词看作一个时间步，把reward由后往前依次传递，由此产生的策略梯度可以更新PPO模型参数。这是标准的强化学习过程，目的是训练LLM产生高reward的答案，也即是产生符合RM标准的高质量回答。

如果我们不断重复第二和第三阶段，很明显，每一轮迭代都使得LLM模型能力越来越强。因为第二阶段通过人工标注数据来增强RM模型的能力，而第三阶段，经过增强的RM模型对新prompt产生的回答打分会更准，并利用强化学习来鼓励LLM模型学习新的高质量内容，这起到了类似利用伪标签扩充高质量训练数据的作用，于是LLM模型进一步得到增强。显然，第二阶段和第三阶段有相互促进的作用，这是为何不断迭代会有持续增强效果的原因。

尽管如此，我觉得第三阶段采用强化学习策略，未必是ChatGPT模型效果特别好的主要原因。假设第三阶段不采用强化学习，换成如下方法：类似第二阶段的做法，对于一个新的prompt，冷启动模型可以产生k个回答，由RM模型分别打分，我们选择得分最高的回答，构成新的训练数据<prompt,answer>,去fine-tune LLM模型。假设换成这种模式，我相信起到的作用可能跟强化学习比，虽然没那么精巧，但是效果也未必一定就差很多。第三阶段无论采取哪种技术模式，本质上很可能都是利用第二阶段学会的RM，起到了扩充LLM模型高质量训练数据的作用。

以上是ChatGPT的训练流程，主要参考自instructGPT的论文，ChatGPT是改进的instructGPT，改进点主要在收集标注数据方法上有些区别，在其它方面，包括在模型结构和训练流程等方面基本遵循instructGPT。可以预见的是，这种Reinforcement Learning from Human Feedback技术会快速蔓延到其它内容生成方向，比如一个很容易想到的，类似“A machine translation model based on Reinforcement Learning from Human Feedback”这种，其它还有很多。但是，我个人认为，在NLP的某个具体的内容生成领域再采用这个技术意义应该已经不大了，因为chatGPT本身能处理的任务类型非常多样化，基本涵盖了NLP生成的很多子领域，所以某个NLP子领域如果再单独采用这个技术其实已经不具备太大价值，因为它的可行性可以认为已经被chatGPT验证了。如果把这个技术应用在比如图片、音频、视频等其它模态的生成领域，可能是更值得探索的方向，也许不久后我们就会看到类似“A XXX diffusion model based on Reinforcement Learning from Human Feedback”,诸如此类，这类工作应该还是很有意义的。

另外一个值得关注的采取类似技术的工作是DeepMind的sparrow，这个工作发表时间稍晚于instructGPT，如果你仔细分析的话，大的技术思路和框架与instructGPT的三阶段基本类似，不过明显sparrow在人工标注方面的质量和工作量是不如instructGPT的。反过来，我觉得sparrow里把回报模型分为两个不同RM的思路，是优于instructGPT的，至于原因在下面小节里会讲。

## 局限

ChatGPT 有时会写出看似合理但不正确或荒谬的答案。 解决这个问题具有挑战性，因为：

- 在 RL 训练期间，目前没有真实来源；
- 训练模型更加谨慎导致它拒绝可以正确回答的问题；
- 监督训练会误导模型，因为理想的答案取决于模型知道什么，而不是人类演示者知道什么。

ChatGPT 对输入措辞的调整或多次尝试相同的提示很敏感。 例如，给定一个问题的措辞，模型可以声称不知道答案，但只要稍作改写，就可以正确回答。

该模型通常过于冗长并过度使用某些短语，例如重申它是 OpenAI 训练的语言模型。 这些问题源于训练数据的偏差（训练者更喜欢看起来更全面的更长答案）和众所周知的过度优化问题。

理想情况下，当用户提供模棱两可的查询时，模型会提出澄清问题。 相反，我们当前的模型通常会猜测用户的意图。

虽然我们已努力使模型拒绝不当请求，但它有时会响应有害指令或表现出有偏见的行为。 我们正在使用 Moderation API 来警告或阻止某些类型的不安全内容，但我们预计目前它会有一些漏报。 我们渴望收集用户反馈，以帮助我们正在进行的改进该系统的工作。



## Reference

- https://openai.com/blog/chatgpt/
- https://zhuanlan.zhihu.com/p/589533490
