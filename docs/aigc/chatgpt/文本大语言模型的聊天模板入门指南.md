# 文本大语言模型的聊天模板入门指南

大语言模型（LLMs）日益常见的应用场景是**聊天对话**。在聊天场景中，模型不再是延续单个文本字符串（如标准语言模型那样），而是延续由多条**消息**组成的对话。每条消息都包含一个**角色**（如"user"或"assistant"）和消息内容。

与分词处理类似，不同模型对聊天输入的格式要求差异很大。为此我们引入了**聊天模板**功能。聊天模板是纯文本LLMs分词器或多模态LLMs处理器的组成部分，它指定了如何将对话（表示为消息列表）转换为符合模型预期的单个可分词字符串。

本页将重点介绍纯文本LLMs的聊天模板基础用法。关于多模态模型的详细指南，我们准备了专门的[多模态模型文档](./chat_template_multimodal)，涵盖如何在模板中处理图像、视频和音频输入。

让我们通过`mistralai/Mistral-7B-Instruct-v0.1`模型的示例来具体说明：

```python
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

>>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

注意分词器如何添加控制标记[INST]和[/INST]来标识用户消息的起止（但不用于assistant消息），并将整个对话合并为单个字符串。若使用默认的`tokenize=True`，该字符串还会被自动分词。

现在尝试将模型替换为`HuggingFaceH4/zephyr-7b-beta`，会得到：

```text
<|user|>
Hello, how are you?</s>
<|assistant|>
I'm doing great. How can I help you today?</s>
<|user|>
I'd like to show off how chat templating works!</s>
```

Zephyr和Mistral-Instruct都基于同一基础模型`Mistral-7B-v0.1`微调，但使用了完全不同的聊天格式。没有聊天模板时，您需要为每个模型编写手动格式化代码，而细微的格式错误就可能影响性能！聊天模板帮您处理格式化细节，让您可以编写适用于任何模型的通用代码。

## 如何使用聊天模板？

如示例所示，使用聊天模板非常简单。只需构建包含`role`和`content`键的消息列表，然后根据模型类型调用分词器的[`~PreTrainedTokenizer.apply_chat_template`]方法或处理器的[`~ProcessorMixin.apply_chat_template`]方法。当使用聊天模板作为模型生成输入时，建议设置`add_generation_prompt=True`来添加[生成提示](#什么是生成提示)。

### **纯文本LLMs与聊天模板的适配**

以下是使用Zephyr准备模型生成输入的示例：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)  # 建议使用bfloat16精度并转移到GPU

messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))
```
输出符合Zephyr预期的格式：
```text
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s>
<|user|>
How many helicopters can a human eat in one sitting?</s>
<|assistant|>
```

现在输入已正确格式化，可以使用模型生成响应：

```python
outputs = model.generate(tokenized_chat, max_new_tokens=128)
print(tokenizer.decode(outputs[0]))
```

输出示例：
```text
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s>
<|user|>
How many helicopters can a human eat in one sitting?</s>
<|assistant|>
Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.
```

看，使用模板后生成响应变得非常简单！

### **多模态 LLMs与聊天模板的适配**

对于LLaVA等多模态大型语言模型，提示词的格式化方式类似。主要区别在于需要同时传递文本和输入图像/视频。每个"content"字段必须是由文本或图像/视频组成的列表。

以下是为LLaVA模型准备输入的示例：

```python
from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration

model_id = "llava-hf/llava-onevision-qwen2-0.5b-ov-hf"
model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)  # You may want to use bfloat16 and/or move to GPU here
processor = AutoProcessor.from_pretrained(model_id)

messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a friendly chatbot who always responds in the style of a pirate"}],
    },
    {
      "role": "user",
      "content": [
          {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
          {"type": "text", "text": "What are these?"},
        ],
    },
]

processed_chat = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors="pt")
print(processor.batch_decode(processed_chat["input_ids"][:, :30]))
```

该代码会生成符合LLaVA预期输入格式的字符串，末尾包含多个<image>标记。这些<image>标记是占位符，在前向传播时会被实际图像嵌入替换。processed_chat可进一步传入 `generate()`方法生成文本。

```python
'<|im_start|>system
You are a friendly chatbot who always responds in the style of a pirate<|im_end|><|im_start|>user <image><image><image><image><image><image><image><image>'
```

啊，原来这么简单！


## 是否有自动化的聊天流程？

是的！我们的text generation pipelines支持聊天输入，可以轻松使用聊天模型。过去我们使用专用的"ConversationalPipeline"类，现其功能已合并到[`TextGenerationPipeline`]。用pipeline重试Zephyr示例：

```python
from transformers import pipeline

pipe = pipeline("text-generation", "HuggingFaceH4/zephyr-7b-beta")
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # 打印assistant回复
```

输出：
```text
{'role': 'assistant', 'content': "Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all."}
```

pipeline 会自动处理分词和模板应用，只要模型有聊天模板，初始化pipeline后直接传入消息列表即可！

## 什么是"生成提示"？

`apply_chat_template`方法的`add_generation_prompt`参数用于添加指示AI回复开始的标记。例如：

```python
messages = [
    {"role": "user", "content": "Hi there!"},
    {"role": "assistant", "content": "Nice to meet you!"},
    {"role": "user", "content": "Can I ask a question?"}
]
```

不使用生成提示时的ChatML格式：
```python
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
"""
```

使用生成提示时：
```python
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
<|im_start|>assistant
"""
```

需要注意的是，这次我们添加了指示模型开始生成回复的特殊标记。这种做法能确保模型在生成文本时，会按照预期输出机器人回复，而不是产生意外行为（例如延续用户消息）。请记住，聊天模型本质上仍是语言模型——它们经过训练是为了延续文本，而对话对它们而言只是特殊形式的文本！必须通过恰当的控制标记进行引导，才能使模型明确当前的任务目标。

并非所有模型都需要生成提示。部分模型（如LLaMA）在机器人回复前没有特殊标记。对于这类模型，`add_generation_prompt` 参数将不会产生任何效果。该参数的具体作用效果取决于实际使用的模板配置。


## "continue_final_message"的作用？

当使用`apply_chat_template`或`TextGenerationPipeline`时，设置`continue_final_message=True`可以让模型延续最后一条消息而不是开始新回复。这是通过移除最后消息的结束标记实现的，适用于"预填充"模型响应：

```python
chat = [
    {"role": "user", "content": "Can you format the answer in JSON?"},
    {"role": "assistant", "content": '{"name": "'},
]

formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)
model.generate(**formatted_chat)
```

该模型将生成延续现有JSON字符串的文本，而非创建新消息。当您明确知道希望模型如何开始回复时，这种方法能有效提升模型遵循指令的准确性。

注意`add_generation_prompt`和`continue_final_message`不能同时使用。原因是`add_generation_prompt`和`continue_final_message`在输出结构上存在冲突：
1. **`add_generation_prompt`**：通过添加新消息起始标记（如`"助手："`），强制模型**开启全新回复**，适用于对话式交互场景。
2. **`continue_final_message`**：通过移除消息结束标记（如`</s>`），强制模型**延续当前内容流**，避免生成封闭式结尾。

由于前者要求"新开对话"，后者要求"延续对话"，二者逻辑互斥，同时使用将引发矛盾导致报错。

<Tip>

`TextGenerationPipeline`默认设置`add_generation_prompt=True`。但如果输入的最后消息是 assistant 角色，会自动切换为`continue_final_message=True`，因为多数模型不支持连续 assistant 消息。可通过显式传递参数覆盖此行为。

</Tip>

## 可以在训练中使用聊天模板吗？

是的！这是确保聊天模板与模型训练时所见标记一致的有效方法。我们建议您将聊天模板作为数据集的预处理步骤应用。完成后，您可以像处理其他语言模型训练任务一样继续操作。在训练过程中，通常应设置`add_generation_prompt=False`，因为用于触发助手回复的附加标记在训练阶段并无益处。来看个示例：

```python
from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")

chat1 = [
    {"role": "user", "content": "Which is bigger, the moon or the sun?"},
    {"role": "assistant", "content": "The sun."}
]
chat2 = [
    {"role": "user", "content": "Which is bigger, a virus or a bacterium?"},
    {"role": "assistant", "content": "A bacterium."}
]

dataset = Dataset.from_dict({"chat": [chat1, chat2]})
dataset = dataset.map(lambda x: {"formatted_chat": tokenizer.apply_chat_template(x["chat"], tokenize=False, add_generation_prompt=False)})
print(dataset['formatted_chat'][0])
```
输出：
```text
<|user|>
Which is bigger, the moon or the sun?</s>
<|assistant|>
The sun.</s>
```

在此之后，您只需像处理标准语言模型训练任务一样继续训练流程，使用`formatted_chat`列作为输入即可。

**注意**：默认情况下，部分分词器会对文本自动添加特殊标记（如`<bos>`起始符和`<eos>`结束符）。由于聊天模板已包含所需的所有特殊标记，额外添加的标记可能导致重复或冲突，进而损害模型性能。因此：

1. **若通过`apply_chat_template(tokenize=False)`格式化文本**：
   后续分词时需显式设置`add_special_tokens=False`以避免重复添加标记。

   ```python
   text = apply_chat_template(..., tokenize=False)
   tokenized = tokenizer(text, add_special_tokens=False)  # 关闭自动添加特殊标记
   ```

2. **若使用`apply_chat_template(tokenize=True)`**：
   模板已集成分词逻辑，无需额外处理，系统会自动规避重复标记问题。
