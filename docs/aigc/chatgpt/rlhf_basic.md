# RLHF

## 什么是 RLHF？

Reinforcement Learning with Human Feedback（RLHF）是强化学习（RL）的一个扩展分支，当决策问题的优化目标比较抽象，难以形式化定义具体的奖励函数时，RLHF 系列方法可以**将人类的反馈信息纳入到训练过程**，通过使用这些反馈信息构建一个奖励模型神经网络，以此提供奖励信号来帮助 RL 智能体学习，从而**更加自然地将人类的需求，偏好，观念等信息以一种交互式的学习方式传达给智能体，对齐（align）人类和人工智能之间的优化目标，产生行为方式和人类价值观一致的系统**。

最初，在 2017 年的研究工作《Deep reinforcement learning from human preferences》[1] 中就有研究者尝试将人类反馈信息引入 Atari [2] 、MuJoCo [3] 这样的经典决策学术环境，从而取得了一些有趣的发现。后来，相关内容又进一步衍生出 preference-based RL/Inverse RL [4] 等研究子方向。

从 2020 年起至今，研究者们又进一步发现对于大语言模型（Large Language Model，LLM），RLHF 方法可以有效提升 LLM 生成质量的真实性和信息完整性，在 LLM 的输出和人类需要的对话信息之间架起一座桥梁 [5-6]。而在 2022 年末，ChatGPT [7] 的推出更是技惊四座，短短几个月内已经有超过一亿的用户尝试并领略到了这种强大对话系统的通用性和便利性。RLHF 成功将 LLM 内部蕴含的知识激发出来，高效地促进人工智能和人类偏好之间的同步与协调。

具体来说，RLHF 可能的优势有如下三点：

1. **建立优化范式**：为无法显式定义奖励函数的决策任务，建立新的优化范式。对于需要人类偏好指引的机器学习任务，探索出一条可行且较高效的交互式训练学习方案。
2. **省数据（Data-Efficient）**：相对其他的训练方法，例如监督学习，Top-K 采样等，RLHF 能够利用更少的人类反馈数据达到相近的训练效果。
3. **省参数（Parameter-Efficient）**：相对其他的训练方法，例如监督学习，Top-K 采样等，RLHF 可以让参数量较小的神经网络也能发挥出强大的性能。

## **RLHF 与大语言模型**

想要在大语言模型（LLM）中使用 RLHF，我们首先需要确定以下两个问题：

- 如何收集大语言模型所需要的人类反馈，并训练相应的奖励模型（RM）
- 如何建模大语言模型相关的马尔科夫决策过程（MDP）

### **大语言模型中的 RM 设计**

基于监督学习预训练（Supervised Fine-Tuning，SFT）的大语言模型，奖励模型（RM）依然复用了 SFT 模型的大部分参数，只是修改部分输出层得到一个数值奖励（scalar reward）。在数据收集方面，对于人类反馈直接去评判打分是很困难的，因为没有所谓的参考标准或者基线标准，人类反馈的打分值可能会包含大量的主观偏好，一个更有效的方式是让**标注者去给 SFT 大语言模型输出多个结果进行排序（rank）**，将排序后的数据用于训练。具体的训练方法则很简单，类似经典 preference-based RL/IRL 的相关方法，对于排序后的结果**两两比较**进行训练，具体优化时使用 **Cross-Entropy** 损失函数（即类似二分类问题， A>B 为标签1，A<B为标签0）。

不过，值得注意的是，实际训练中并不是在数据集中取出所有两两比较的数据对分别进行训练，因为如果假设一组排序结果有 K 个数据，那么这样的训练方式会让每个数据被用于 K-1 次更新，很容易导致严重的**过拟合**，所以实践中是将 K 个数据一起输入 RM，得到各自的预测值后，计算所有的两两比较损失函数结果，最终平均后进行更新。

### **大语言模型中的 MDP 建模**

对于自然语言处理任务以及大语言模型，一般定义为如下所述的 MDP，即其中的关键概念为：

- **策略（policy）**：将监督学习预训练（Supervised Fine-Tuning，SFT）的大语言模型作为策略。
- **Sequence/Token-Level MDP**：前者类似经典的 Bandit，策略输入提示词（prompt），输出相应的回答句子，然后给出整体的奖励信息，即一个单步的决策过程。后者则是经典的多步决策过程，每步决策输出一个单词，最终输出完整句子作为一个 episode，并定义相应的单步奖励函数和折扣因子。
- **观察空间（observation）**：以任务特定的提示词（task-specific prompt）为观察信息。每执行动作选择一个词之后，也将这个词加入观察信息，即每一步可以看到 prompt 和之前所有策略选择过的词语。
- **动作空间（action）**：以单词词表作为动作空间。策略需要从词表中选择对应的词进行决策。这是一个超大规模的离散动作空间，例如 GPT-3 的词表规模为 50k。
- **终止条件（termination）**：一般有两种，策略输出句子的结束符（end of sentence，EOS），或 episode 达到预定义的最大长度 T。
- **奖励空间（reward）**：奖励函数包含两部分，第一部分是 RM 在 episode 结束时给出的奖励结果，这是一种稀疏奖励。第二部分则是一种 regularizer，为了防止 RLHF 训练得到的策略偏离监督学习的结果策略太远，定义每步单词预测两个策略之间的 KL 散度为一个惩罚奖励，这是一种稠密奖励。

- **状态转移函数（transition）**：仅适用于 Token-Level MDP，由于通过自回归（auto-regressive）的方式定义观察空间，所以是一种确定性（determinisitc）的状态转移。
- **折扣因子（discount factor）**：仅适用于 Token-Level MDP，在多步决策中平衡当前和未来奖励，如果令折扣因子等于1，那么 Token-Level MDP 其实可以看作等价于 Sequence-Level MDP。

### **Awesome RLHF**

基于 RLHF 加持的 **ChatGPT** [7]将对话系统的能力上限和想象空间向前推进了一大步，因此，研究社区也开始愈加关注 RLHF 技术及其应用，由此诞生了下列这些可能的研究方向：

1. 如何将 RLHF 成功应用到除大语言模型之外的场景中 [8] 。
2. 对于人类反馈信息，使用什么样的数据收集方案、神经网络大小，训练方法最友好 [9]。
3. 除了先学习奖励模型，再结合 RM 对策略进行进行 RL 训练，是否有其他范式可以利用人类反馈信息 [10]。

为了推动相关社区的发展，降低领域入门门槛，OpenDILab 对 RLHF 的一些**经典论文、基准代码库和前沿进展**进行梳理，主要侧重 2020 年以来最新的 RLHF 相关工作，具体的论文和代码库列表已整理好且放置于GitHub平台，并会持续保持更新。

同时也欢迎志同道合的小伙伴发起 pull request，上传相关工作，共同营造健康、可持续的学术生态。



**欢迎体验Awesome-RLHF：**

https://github.com/opendilab/awesome-RLHF
