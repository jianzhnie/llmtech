# LLM 评测体系

## OpenCompass

### 01 | OpenCompass 是什么

OpenCompass 的主要特点包括开源可复现、全面的能力维度、丰富的模型支持、分布式高效评测、多样化评测范式以及灵活化拓展。基于高质量、多层次的能力体系和工具链，OpenCompass 创新了多项能力评测方法，并构建了一套高质量的中英文双语评测基准，涵盖语言与理解、常识与逻辑推理、数学计算与应用、多编程语言代码能力、智能体、创作与对话等多个方面，能够实现对大模型真实能力的全面诊断。

### 02 | OpenCompass 大模型评测“铁三角”

#### 评测工具链体系 CompassKit

CompassKit工具链地址：https://github.com/open-compass

OpenCompass 推出大模型评测全栈工具链 CompassKit，不仅提供完整的开源可复现评测代码，更提供了丰富的模型支持和高效的分布式评测策。CompassKit 中包含：

- OpenCompass 升级版大语言模型评测工具：提供全面的大模型评测功能，包括广泛模型支持、高效评测速度、主观评测能力、数据污染检查和丰富的长文本评测能力。
- VLMEvalKit 多模态大模型评测工具：一站式多模态评测工具，支持主流多模态模型和数据集，助力社区比较不同多模态模型在各种任务上的性能。
- Code-Evaluator 代码评测服务工具：提供基于 docker 的统一编程语言评测环境，确保代码能力评测的稳定性和可复现性。
- MixtralKit MoE 模型入门工具：为 MoE 模型初学者提供学习资料、模型架构解析、推理与评测教程等入门工具。

#### 高质量评测基准社区 CompassHub

CompassHub 社区地址：https://hub.opencompass.org.cn/home

CompassHub 是面向大模型能力评测开源开放的基准社区，提供海量的面向不同能力维度和行业场景的评测基准。OpenCompass 欢迎评测用户在 CompassHub 上传各自构建的高质量评测基准，发布相应的性能榜单，汇聚社区力量助力大模型社区整体快速发展。

#### 权威评测榜单 CompassRank

CompassRank 榜单地址：https://rank.opencompass.org.cn/home

作为 OpenCompass 中各类榜单的承载平台，CompassRank 不受任何商业利益干扰，保持中立性。同时，依托 CompassKit 工具链体系中的各类评测手段，保证了 CompassRank 的客观性。CompassRank 不仅覆盖多领域、多任务下的模型性能，还将定期更新，提供动态的行业洞察。与此同时，OpenCompass 团队将在榜单中提供专业解读，进一步帮助从业者理解技术深意，优化模型选择。

### 03｜OpenCompass 六大亮点功能

作为高效、全面的大模型评测体系及开放平台，OpenCompass 提供完整开源可复现的评测框架，支持大语言模型、多模态模型的一站式评测，基于分布式技术，对大参数量模型亦能实现高效评测。同时，通过零样本评测、小样本评测和思维链评测等多样化评测，OpenCompass 可全方位量化模型在各个的维度能力。

#### 01 全面的能力维度

OpenCompass 的评测维度包括基础能力和综合能力两个层级，涵盖了语言、知识、理解、数学、代码、长文本、智能体等12个一级能力维度，综合设计了50余个二级能力维度。能力维度设计具备可扩展性和增长性，同时可根据未来的大模型应用场景进行动态更新和迭代。

基础能力维度以语言、知识、理解、数学、代码为核心，包括意图识别、情感分析、内容评价与总结、多语言翻译、汉语与中国传统文化、常识百科、自然科学、人文社科、计算能力、数学应用能力、多编程语言代码等20余项细分任务。而综合能力旨在考察模型在综合运用知识、数学推理、代码工具等多种能力完成复杂任务的水平。

在全方位评测维度的基础上，高质量的评测基准对模型评测统一至关重要。OpenCompass 团队构建了一批高质量中英双语大模型能力评测基准，涵盖数学计算与应用、工具调用、代码解释器、中文创作等多个方向，综合评估模型执行复杂任务的能力。

#### 02 丰富模型支持

可对种类丰富的大模型进行评测是 OpenCompass 的重要属性。OpenCompass 不仅可对超过100种开源模型的进行评测，还预留了简洁的模型接口，开发者可自主接入API模型。目前，OpenCompass 已支持OpenAI 接口的调用（支持测试ChatGPT/GPT-4），后续还持续支持 Claude, PaLM 等多种 API 模型的评测。

同时，HuggingFace 作为具有重要影响力的大模型托管平台，承载了当前业界几乎所有的开源模型，OpenCompass 研究团队与技术社区紧密合作，用户可通过 OpenCompass 对 HuggingFace 承载的开源大模型进行“一站式”评测，为学术研究提供直接便利。

#### 03 分布式高效评测

OpenCompass 原生提供分布式评测方案，支持在本机或集群上的计算任务并行分发，实现评测并行式的提速。此外，还通过分割大任务、合并小任务等策略，控制各计算任务的执行时间尽可能相等，实现计算负载均衡，更加充分地利用所有的计算资源。研究团队在测试中发现，当运算资源充足的情况下，OpenCompass 最短仅需 3 个小时即可完成千亿参数量级模型的完全评测，实现了模型训练-评测链路上的快速迭代。

OpenCompass 平台广泛支持超过100种HuggingFace 和 API 模型，融合了 100 多个数据集，包含约40 万个问题，用以从八个维度评估模型。其高效的分布式评估系统能够快速且全面地评估十亿级规模的模型。该平台适应多种评估方法，包括零样本、少样本和思维链评估，并且具有高度可扩展的模块化设计，便于轻松添加新模型、数据集或自定义任务策略。此外，OpenCompass 包括强大的实验管理和报告工具，用于详细跟踪和实时结果展示。

#### 04 多样化评测方式

OpenCompass 提供基础的零样本评测策略，并支持小样本评测策略，同时提供 7 种不同的上下文样例的提取方案，助力提示词构建。未来，OpenCompass 还将提供思维链式 (chain-of-thought) 评测策略。此外，OpenCompass 针对对话模型的特性，首创与模型绑定的提示词模板（Meta Template），允许用户自定义模型的对话模板，从而把提示词以最优的方式传入基座或对话模型。

#### 05 灵活化拓展

OpenCompass 支持灵活便捷地添加评测数据集与模型，用户可通过预留接口，对非开源的自定义模型进行评测。研究团队通过设计代码架构，允许用户新增数据集数据集或自定义数据划分策略，甚至接入新的集群管理后端。为拓展评测模型的类型提供无限可能。

#### 06 开源可复现

作为公开评测方案，OpenCompass 向技术社区开源。当前 OpenCompass 所有支持的数据集及各数据集多版本提示词，用户可一键下载。通过多方位全链路的公开，确保评测结果可以被完整复现。同时，OpenCompass 欢迎各界共同参与贡献，持续优化提示词和测试逻辑，共同打造更强大、更全面的大模型评测基准。

## HELM

HELM：Holistic Evaluation of Language Models（斯坦福，2022）提出了语言模型的整体评估，以提高语言模型的透明度“场景、任务、指标。

### 应用场景分类：

将潜在的语言模型应用场景进行分类，包 括任务和领域方面。任务可以涵盖问答、信息检索、摘要、 情感分析、毒性检测、杂项文本分类等核心场景。领域则 包括来源、用户和时间等因素。

### 评估指标分类：

采用多指标方法对语言模型进行评估。评 估指标包括精度（Accuracy）、校准和不确定性 （Calibration and uncertainty）、稳健性（Robustness）、公 平性（Fairness）、偏见和刻板印象（Bias and stereotypes）、 有毒性（Toxicity）以及效率（Efficiency）。这些指标用于评 估语言模型在不同应用场景下的性能。

### 评估方法

1. 应用场景分类：

将潜在的语言模型应用场景进 行分类，包括任务和领域方面。任务可以涵盖问答、信息检索、摘要、情感分析、毒性检测、杂 项文本分类等核心场景。领域则包括来源、用户 和时间等因素。

2. 评估指标分类：

采用多指标方法对语言模型进 行评估。评估指标包括精度（Accuracy）、校准 和不确定性（Calibration and uncertainty）、稳健 性（Robustness）、公平性（Fairness）、偏见和 刻板印象（Bias and stereotypes）、有毒性 （Toxicity）以及效率（Efficiency）。这些指标用 于评估语言模型在不同应用场景下的性能。

3. 大规模评估方法：

在42个场景下对30个语言模 型进行大规模评估。评估方法是通过修改prompt 并加入5个样例，将语言模型拓展到需要评估的 任务上。这种评估方法可以有效评估和比较语言 模型在不同应用场景下的性能，为进一步改进和 优化提供参考。

### HELM 评价结果

HELM得到了25个结论

1. Instruction-tuning: 优势在于模型参数量小的情况下取得突出结果。

2. 模型准确性与获取方式的关系: 开源模型相对较差，随着时间推移差距可能变大或变小。
3.  校准对模型准确性的影响: 取决于场景和迁移方式，可能成正比或反比。
4.  鲁棒性和公平性对准确性的影响: 在一些扰动下的最坏情况准确度，可能需要权衡。
5.  性能差距与人口统计层面的关系: 不同人群可能存在性能差距。
6. 生成性损伤的固定偏差和毒性: 在核心场景中平均很低，但仍对社会有危害。
7. 准确性与效率的关系: 模型大小和准确度成正比，但训练和推理开销增大。
8. 问题回答中的生成结果差异: 在不同问题回答场景中观察到明显差异。
9. 信息检索任务的模型表现: 比较好但未达到SOTA水平。
10. 摘要任务中模型生成超越官方参考摘要: 需改进摘要基准和评估指标。
11. 情感分析任务的模型表现: 准确性和校准性表现好，但鲁棒性和公平性下降。
12. 毒性检测中模型准确性和鲁棒性差异大: 大多数模型不够准确。
13. 杂项文本分类中模型表现差异: 在不同子集/任务上存在显著差距。
14. 语义理解中模型准确性和语言建模的差异: 最优模型在某些任务上可能表现最差。
15. 模型大小与获取世界知识能力的关系: 模型大小对知识获取能力提升重要。
16. 推理能力的提高对代码模型效果的影响: 代码模型比文本模型表现好。
17. 长序列的版权/证件材料记忆能力: 逐字记忆和准确性成正比。
18. 大模型在生成支持给定论点的逼真标题方面有效: 但生成鼓励特定行动的文本效果有高有低。
19. 准确性与偏见之间的关系: 最准确模型存在与社会一致的偏见/歧视。
20. 毒性生成与核心场景的关系: 核心场景中毒性生成概率很低。
21. 大模型的全面性表现: 超过某些专业化模型。
22. 提示对模型表现的影响: 对提示格式和上下文示例敏感。
23. 多选择迁移方法对模型表现的影响: 迁移方式影响模型表现。
24. 上游复杂度与下游准确度的关系: 上游复杂度不能可靠预测下游准确度。
25. 模型规模的趋势与准确性的关系: 模型规模可预测准确性，但效率可能不够高效。

- Stanford
- 客观
- 5w+

## AGI-EVAL

A Human-Centric Benchmark for Evaluating Foundation Models （微软 2023.4） 专门用于评估基础模型在「以人为本」（human-centric）在标准化考试，如高考、公务员考试、 法学院入学考试、数学竞赛和律师资格等考试中的表现。

###  AGIEval 数据集

AGIEval数据集遵循两个设计原则

- 强调人脑级别的认知任务：
- 与现实世界场景的相关性：

AGIEVAL选择了多种标准化的高质量考试，强调人类水平的推理和现实世界的相关性， 具体包括：

- 普通高校入学考试
- 法学院入学考试
- 律师资格考试
- 研究生管理入学考试（GMAT）
- 高中数学竞赛
- 国内公务员考试。

### AGI-EVAL：评测方式

- 评估了三个模型：GPT-4, ChatGPT和TextDavinci-003

- 采用Zero-shot和Few-shot设置进行评估。 在Zero-shot设置下，模型直接对问题进行 评估；而在Few-shot设置下，模型在对测 试样本进行评估之前，会先看到同一任 务中的少量例子。
- 实验中使用了CoT策略：
  - 接收到提示 「Let’s think step by step」为给定的问题生成解释
  - 接着模型会接收到另一提示 「Explanation is」，根据先前的解释生成最终的答案。
- 对于多选题，使用了标准分类准确率进 行评估；对于填空题，使用了精确匹配 （EM）和F1指标进行评估。

### AGI-EVAL：评测结果

- GPT-4在所有任务中都显著优于其同类产品。

- ChatGPT在需要外部知识的任务中，例如地理、生物、化学、物理和数学，明显优于 Text-Davinci-003，而在依赖语言理解和逻辑推理的任务上，两者的表现相当。
- 虽然这些模型的表现总体上良好，但它们在处理需要复杂推理的任务上仍有局限性。

## LLM-as-a-judge

Judging LLM-as-a-judge with MT-Bench and Chatbot Arena（U.C. Berkeley 2023.6） 使用LLM作为判别器来评估这些模型在更开放的问题上的表现

三种评判方式

- 成对比较：LLM裁判被呈现一个问题和两个答案，并被任务确定哪一个更好或宣布平局。

- 单个答案打分：LLM裁判直接为单个答案分配 分数。

- 参考引导打分：提供参考解决方案，引导LLM 裁判做出判断。（适用于数学题）

两种评测基准

- MT-bench
- Chatbot-arena

问题集

- 80题
- 8个常见的用户提示类别：写作，角色扮演， 提取，推理，数学，编程，知识I（STEM）， 和知识II（人文/社会科学）
- 每个类别设计10个多轮问题

LLM评测

- 每个问题都涉及到两个回合来评估
- 两个完整的对话显示在一个提示中，让LLM法 官专注于第二个问题
- 右图是一个LLM评测MT-bench的示例prompt

偏见 • 位置偏见：更倾向第一个位置 • 冗长偏见：更倾向文本更长的回答 • 自我提升偏见：更倾向于自己生成的回 答 一致率 • 强大的LLM可以达到超过80%的一致性率， 与人类专家之间的一致性水平相当 • 当模型之间存在显著的性能差异时，GPT4与人类的一致性更好 主张为未来的LLM基准采用混合评估框架



## 现有 LLM 评估方法的常见挑战

- 数据污染：确保评估数据的质量和完整性至关重要。受污染的数据可能导致对 LLM 绩效的评估不准确。
- 过度依赖困惑度：过分强调困惑度作为衡量标准可能无法全面反映语言理解和生成能力，从而可能导致评价偏差。
- 人工评估中的主观性：人工评估引入了主观性，使得在评估 LLM 表现时保持一致性和客观性变得具有挑战性。
- 有限的参考数据：多样化和高质量参考数据的有限可用性会阻碍全面评估，尤其是对于处理专业领域或语言的模型。
- 缺乏多样性指标：许多评估方法缺乏专门衡量反应多样性的指标，这对于评估LLM的创造力和适应性至关重要。
- 推广到现实世界场景：在受控环境中评估 LLM 可能无法反映其在现实世界中的表现，因为它们必须处理各种非结构化和动态输入。
- 对抗性攻击： LLM 很容易受到对抗性攻击，评估其在面对此类攻击时的稳健性是评估过程中的一项重大挑战。
