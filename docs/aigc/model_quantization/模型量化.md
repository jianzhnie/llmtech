# 量化(Quantization)

## 引入

- 在 AI 模型训练时，通常使用浮点数（Float32 等）进行计算，这样能够确保更好的精度表现
- 当然浮点数运算也是一把双刃剑，在提升了计算精度的同时带来了更多的计算量和储存空间占用
- 具体的表现就是模型的计算速度较慢、模型的文件体积较大
- 在模型推理的时候，大多数时候并不需要如此高的计算精度，或者说低精度的运算不会对模型精度产生太大的影响
- 这个时候就可以将模型映射到较低精度的运算上，降低计算量，提升运行速度，减少模型文件的体积，方便传输
- 这样的将模型从高精度运算（一般是浮点运算）转换到低精度运算（一般是整数运算）的过程叫做模型量化



**机器学习 (ML) 中的量化**是将 FP32（浮点 32 位）中的数据转换为较小精度（如 INT8（整数 8 位））并执行所有关键操作（如 INT8 中的卷积）的过程。

量化背后的基本思想是，如果我们将权重和输入转换为整数类型，我们消耗的内存更少，并且在某些硬件上，计算速度更快。

然而，有一个权衡：通过量化，我们可能会失去显着的准确性，将浮点数转换为整数会导致误差，并且误差会随着计算而增加，因此保持准确性至关重要。在量化中，精度在原始精度的 1% 以内。

前面说到它已经提到，量化就是在做浮点转定点，也就是前面提到的123.456789转成123，超简单的对吧。如果在AI量化真的是这样的简单那也不用介绍了，AI量化在做得事情是有一组权重(浮点数)，例如是[0.1, 0.2, 1.2, 2.5, 2.1, -1.0, -2.0,-2.5]，怎么「好用利」整数可以表现的范围来表示**这些**浮点数。

在性能方面，由于我们处理的是 8 位而不是 32 位，理论上我们应该快 4 倍。在实际应用中，观察到至少 2 倍的加速。

在本文中，我们将快速了解机器学习中量化的基础知识。阅读本文后，您将能够回答：

- 如何在量化中恢复准确度？
- 在 INT8 或 FP32 中进行计算如何给出相同的结果？

我们深入讨论的量化的主要主题是：

- 较低的精度
- 量化基础
- 范围映射
- 仿射量化
- 比例量化
- 量化粒度
- 校准
- 训练后量化
- 权重量化
- 激活量化
- 恢复量化精度
- 部分量化
- 量化意识训练
- 学习量化参数
- 整体量化过程

## 较低的精度

较低精度的数据类型可以是：

- FP32
- FP16
- INT32
- INT16
- 整数8
- 整数4
- INT1

根据目前的研究状态，我们正在努力保持 INT4 和 INT1 的准确性，而 INT32 oe FP16 的性能改进并不显着。

最受欢迎的选择是：**INT8**

当我们在特定数据类型（例如 INT8）中进行计算时，我们需要另一个具有数据类型的结构，该结构可以保存结果以处理溢出。这称为**累积数据类型**。对于 FP32，累加是 FP32，但对于 INT8，累加是 INT32。

下表让您了解数据大小的减少和数学能力的增加，具体取决于数据类型：

| 数据类型 | 积累  | 数学能力 | 数据大小减少 |
| :------: | :---: | :------: | :----------: |
|   FP32   | FP32  |    1X    |      1X      |
|   FP16   | FP16  |    8X    |      2X      |
|  整数4   | INT32 |   16X    |      4X      |
|  整数4   | INT32 |   32X    |      8X      |
|   INT1   | INT32 |   128X   |     32X      |

## 量化基础

量化有两个基本操作：

- **量化**：将数据转换为较低精度，如 INT8
- **反量化**：将数据转换为更高精度，如 FP32

通常，Quantize 是开始操作，而 Dequantize 是过程中的最后一个操作。

## 范围映射

INT8 可以存储从 -128 到 127 的值。通常，B 位整数的范围可以是 -(2^B) 到 (2^B-1)。

因此，问题是将范围 [A1, A2] 中的所有元素映射到范围 [-(2^B), (2^B-1)]。[A1, A2] 范围之外的元素将被裁剪到最近的边界。

量化中有两种主要类型的范围映射：

- 仿射量化
- 标度量化

对于量化，上述技术主要使用两种类型的映射方程：

```
F(x) = s.x + z
```

其中，s、x 和 z 是实数。

该方程的特例是：

```
F(x) = s.x
```

s 是比例因子，z 是零点。

### 仿射量化

在仿射量化中，参数s和z如下：

```
s = (2^B + 1)/(A1-A2)

z = -(ROUND(A2 * s)) - 2^(B-1)
```

对于 INT8，s 和 z 如下：

```
s = (255)/(A1-A2)

z = -(ROUND(A2 * s)) - 128
```

使用上述等式转换所有输入数据后，我们将获得量化数据。在此数据中，某些值可能超出范围。要将其放入范围内，我们需要另一个操作“剪辑”来将范围外的所有数据映射到范围内。

Clip操作如下：

```
clip(x, l, u) = x   ... if x is within [l, u]

clip(x, l, u) = l   ... if x < l

clip(x, l, u) = u   ... if x > u
```

在上式中，l是量化范围的下限，而u是量化范围的上限。

因此，仿射量化中量化的整体方程为：

```
x_quantize = quantize(x, b, s, z)

    = clip(round(s * x + z),
           −2^(B−1),
           2^(B−1) − 1)
```

对于反量化，仿射量化中的方程为：

```
x_dequantize = dequantize(x_quantize, s, z) = (x_quantize − z) / s
```

### 比例量化

尺度量化（与仿射量化相比）的不同之处在于，在这种情况下，零点 (z) 设置为 0，并且在方程中不起作用。我们在比例量化的计算中使用比例因子 (s)。

我们使用以下等式：

```
F(x) = s.x
```

尺度量化有很多变体，最简单的是对称量化。在此，结果范围是对称的。对于 INT8，范围将为 [-127, 127]。请注意，我们在计算中没有考虑 -128。

因此，在此，我们将量化范围从 [-A1, A1] 到 [-(2^(B-1), 2^(B-1)] 的数据。量化方程为：

```
s = (2^(B - 1) − 1) / A1
```

注意，s 是比例因子。

总方程为：

```
x_quantize = quantize(x, B, s)
           = clip(round(s * x),
                  −2^(B - 1) + 1,
                  2^(B - 1) − 1)
```

反量化方程为：

```
x_dequantize = dequantize(x_quantize, s)
      = x_quantize / s
```

## 量化粒度

输入数据是多维的，为了对其进行量化，我们可以对整个数据使用相同的标度值和零点，或者这些参数对于每个维度中的每个一维数据可以不同，或者对于每个维度不同。

这种为量化和反量化过程对数据进行分组的过程称为**量化粒度**。

Quantization Granularity 的常见选择有：

- 3D 输入的每个通道
- 二维输入的每行或每列

因此，特定数据（如 3D 输入）的比例值将是比例值向量，其中第 i 个值将是 3D 输入的第 i 个通道的比例值。

正确选择粒度有助于以最小的额外比例因子成本更好地表示数据和准确性。

## 校准

量化中的校准是计算输入数据（如权重和激活）的范围 [A1, A2] 的过程。校准主要有以下三种方法：

- 最大限度
- 熵
- 百分位数

**Max**：这是最简单的方法，我们实际比较输入值以获得范围 A1 和 A2 的最大值和最小值。

**熵**：使用KL Divergence方法选择A1和A2，使原始浮点数据和量化数据之间的误差最小

**百分位数**：仅考虑特定百分比的值来计算范围，而忽略可能是离群值的剩余百分比的大值。忽略的值在裁剪操作期间进行处理。

## 训练后量化

训练后量化是在训练过程中量化可用数据（如权重）并将其嵌入到预训练模型中的过程。训练后量化有两种类型：

- 权重量化
- 激活量化

### 权重量化

由于 ML 模型中的权重不依赖于输入，因此可以在训练期间对其进行量化。当使用 Batch normalization 时，我们需要对每个通道进行量化，否则精度损失很大。

因此，对于权重，最常见的选择是**Max Calibration Per channel Quantization**。

### 激活量化

Max Calibration 确实适用于激活量化，但它是特定于模型的。对于某些模型，如 InceptionV4 和 MobileNet 变体，精度下降很明显（超过 1%）。

对于激活量化，最好的结果是**99.99% 或 99.999% 百分位数校准**。确切的百分位值取决于模型。

## 恢复量化精度

量化的主要挑战是保持准确性，并且与 FP32 推理过程相比，准确性下降超过 1%。

有许多 ML 模型在完成量化后精度损失很大。在这种情况下，可以采用两种技术来恢复量化期间的准确性：

- 部分量化
- 量化意识训练
- 学习量化参数

### 部分量化

在部分量化中，想法是只对特定机器学习模型中的几个层进行量化，而忽略其他层。

我们忽略了精度损失最大且难以使用我们探索的其他技术保持精度的层。对于这些层，完成了 FP32 推理。使用部分量化的一个例子是**BERT**。

### 量化意识训练

在 Quantization Aware Training 中，想法是在训练之前在图形中插入伪量化操作，并在微调图形期间使用它。优点是在训练模型和计算权重时，量化因子起到优化作用。

这种技术提高了几乎所有模型的准确性，但 ResNeXt101、Mask RCNN 和 GNMT 除外。这是因为微调不会影响运行变化之外的准确性。

### 学习量化参数

Learning Quantization Parameters 的思想是在计算权重时，在模型训练期间找到量化参数的值，如比例值、零点、范围值等。

由于量化参数与权重相关联，因此大多数模型都具有良好的准确性。

## 整体量化过程

整体的Quantization流程如下：

![量化流程图](assets/quantization_flow_chart.png)

因此，您在权重为量化形式的预训练图上使用量化。量化知识（如本 OPENGENUS 文章所述）有助于将 FP32 输入转换为量化格式，然后像往常一样对卷积或 ReLU 等操作进行操作，并再次量化输出，因为结果数据可能不是量化格式。

有了这个，您对机器学习中的量化有了深刻的理解。这是未来阅读我们所有的量化帖子，以了解这个快速发展的研究领域的最新动态。

# 量化类型

- 线性量化可分为对称量化和非对称量化

## 训练后量化和量化感知训练

### **Post-train Quantization**

简单说就是AI模型的权重训练(浮点数)好之后，直接将模型权重新转换成数。

### **Quantization aware training**

简单说就是AI模型的权重直接使用整机来进行训练，但通常在进行整机训练之前，AI模型会先使用浮点数训练训练后再用整数来进行Fine-tune训练。

此篇文章早在介绍***Post-train Quantization\***，Quantization aware training就不特别介绍(比较复杂一点)。

以INT4为例。
如果先不考疑负号，最简单的方法就是把「浮点数的值域最大价值转换成15」，「浮点数的值域最大转换成15」，「浮点数的值域最小值转换成0”，也就是下图。

![img](assets/1*7z7yMi91sQVqZx9Z0TXSaA.png)

INT4的量化转换。

那浮点数最大最小值转成0和15后，**中间的浮点数字要怎么转**，我们看上图，INT4中0~16个整数有(0,1,2,3,4,5 ,6,7,8,9,10,11,12,13,14,15)，在INT4中每个数字间隔为1，所以最简单的方法是浮点数也想办法切成15格。
在浮点数上这15格大小是多少怎么切也是学术在研究的方法之一。

我们介绍的最简单的方式，就是每格大小都一样，所以只要把浮点的值域(最大值-最小值)除上15格，就可以了，而在浮点数上每次一个格子的大小也称为刻度，见下图。

![img](assets/1*gqviJmE8NCoIMh_ewEbJCg.png)

浮点数上的间隔是一样宽的。

浮点切好的格子，每一个大小都是固定的Scale，每一个内的数字都会对应到INT4内，比如浮点最左边的菊线内的数字 换转到整数的菊线: 取四舍五入，结果是0。
浮点中间的黄线内的数字 换换 到整数的黄线: 取四舍五入，结果是5。

量化其实就是这样非常简单。

## **对称(Symmetric)量化和非对称(Asymmetric)量化**

上面范例介绍的是浮点数转换到0~15，但整个数其实是有正负价值的，也就是INT4其实也可以是(-8, -7,-6,-5,-4,-3,- 2,-1, +0, +1, +2, +3, +4, +5, +6, +7)，也就是第一个位是正负号。

针对量化程序又可分为对称(Symmetric)和非对称(Asymmetric)方式。

假设都是INT4（用16个数字来表示）

对称量化(INT4)值为-8~7，-7,-6,-5,-4,-3,-2,-1, 0, 1, 2, 3, 4, 5, 6, 7, 8，共16个数字。

非相对称(INT4)值域为0~15，0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15，共16个数字。

### **对称(Symmetric)量化**

假装浮点数的AI模型权是

![img](assets/1*rw36WLCgm-GMfqbtXeUZMQ.png)

![img](assets/1*SrDRUAvFawY1y8Dl14tnDg.png)

对称(Symmetric)量化分解三大步骤，

1.查看浮点数据的取绝值后的最大值：

![img](assets/1*x0eLR0Y5yZh-wgOm9S8jkQ.png)

2. 计算尺度

![img](assets/1*03enDMe7wXJ6BIvMv1qAaQ.png)

​	在对称量化尺度中，分项2的次方项有减1，是因为要少去符号位（正负号的表示）。

3. 量化：浮点数到定点

![img](assets/1*IthZV7z4zvF45GFH714p6g.png)

![img](assets/1*C1RN7iPettBOmKWroRhxsA.png)

利 用对称量化将浮点转定点。

如果把转好后的整数在转回浮点数上，此步骤称为去量化。

![img](assets/1*xJvGYbgmViXRCMhScGRUXg.png)

我们来看de-quantizationc后和原始浮点数的差异，

![img](assets/1*eXiyaPtnMyQRxLFdhweytg.png)

### **非对称(Asymmetric)量化**

非对称(Symmetric)量化分解成四大步骤，

1.观察浮点数据的价值：

![img](assets/1*VoS_kgBh5P86lCFr9fCFag.png)

![img](assets/1*XQjYWAHZjku6AWUbcuEg2A.png)

2. 计算尺度

![img](assets/1*F_jdojhibObYILFCUe1Ysw.png)

![img](assets/1*E-rYD-rdaIo9RTvg_j-zNA.png)

3. 计算零点

![img](assets/1*_Z_8U8C4X8y74d_BgW-E4g.png)

![img](assets/1*nBR_5U0S5-AAm-2lnDb14g.png)

4. 量化浮点数到定点

![img](assets/1*6O4jeQTJUnw3D8Gl5ou3Sg.png)

![img](assets/1*oMYCy7gfrPz383VwbcRPIA.png)

![img](assets/1*DhvCUCKcCj-KS-pyEvXdkA.png)

我们来看看在**非对称**方法中如果把转好后的整数在转回浮点数的去量化。

![img](assets/1*Xd1OgxjiyD1lQnFaYabw-Q.png)

![img](assets/1*nwxymcCKbIOOFN72CjWnXA.png)

![img](assets/1*EYsSSHnm6Wsr-7A5yjn3tA.png)

我们整合一下**对称**和**非对称**的总错误值，如下

![img](assets/1*zseXu0KSSEc6ClTdeZASAQ.png)

大家看起来好象没有太多差别，但其实我举个例子举不好，如果好死不死你的模型权重数都是正确的，因为对称量化有一个个位去用来做符号位表示，代表示如果权力重都是值得的，这个位就浪费了，所以结果会差更多，我随手举的例子如下，

![img](assets/1*q95Bfs3RLz2H7EzIHIg0HQ.png)

对称还原错误总合为15
非对称还原错误总合为6

所以量化还原后错误偏差越大代表量化效果越好，所以满多量化相关研究都是在研究怎么量化后和反量化偏差最小化。

AI模型量化等待使用INT跑的好处：模型变小(档案减少)、功绩低一点、运算更快。

有优点就有缺点，量化后的模型通行会有精度的降低，从上面的例子就可以知道每一层量化都会有错误差，而度与深度学习网路通常是非常多层，如果一层错误出来会不断传给下一层，所以错误偏差累乘满可怕的(跟坡度爆炸消失差别不大)，所以量化会造成识别率下降，但是这几年学业的努力，量化减少了很少的表现的情形已经慢慢趋近。 (真心话，小模型可能因为量化异常反响而结果更好，但这种情绪形态比更少发生)。

## 深度学习：数据类型

浮点数有不同的表示形式。深度学习中常用的是 32 位和 16 位浮点数（分别是 FP32 和 FP16）。为了加速深度学习和高性能，有特定于硬件的浮点格式，如 NVIDIA 的[TensorFloat](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/) (TF32)、Google 的[BrainFloat](https://cloud.google.com/tpu/docs/bfloat16) (bfloat16) 和 AMD 的[FP24](https://developer.nvidia.com/gpugems/gpugems2/part-iv-general-purpose-computation-gpus-primer/chapter-32-taking-plunge-gpu)。还有更小的格式，通俗地称为[minifloats](https://en.wikipedia.org/wiki/Minifloat)。这些格式，如 FP8，通常用于嵌入式设备的微控制器。他们的支持在新一代 NVIDIA 的 GPU（如 H100）中宣布。

像 float64 这样的双精度格式在深度学习的上下文中很少使用，因为内存开销大，对深度神经网络没有好处，所以我们不会在这里介绍它们。然而，它们的使用对于统计建模是有益的，单精度是不够的。

![img](https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-1b.png)



## 什么是量化？

量化是一种模型尺寸缩减技术，可将模型权重从高精度浮点表示转换为低精度浮点 (FP) 或整数 (INT) 表示，例如 16 位或 8 位。通过将模型的权重从高精度浮点表示转换为低精度，模型大小和推理速度可以显着提高，而不会牺牲太多精度。此外，量化将通过降低内存带宽需求和提高缓存利用率来提高模型的性能。

在深度神经网络量化的背景下，INT8 表示通俗地称为“量化”。但是，也使用了其他格式，例如 UINT8（无符号版本）或 INT16（可在 x86 处理器上使用）。

不同的模型需要不同的量化方法，成功的量化通常需要先验知识和广泛的微调。此外，量化会在精度和模型大小之间带来新的挑战和权衡，尤其是在使用 INT8 等低精度整数格式时。

量化也会带来一些挑战，尤其是在使用 INT8 等低精度整数格式时。一个主要挑战是这些格式的动态范围有限，这可能导致在从更高精度的浮点表示形式转换时准确性下降。

虽然可以使用 FP16 代替 FP32，在深度神经网络推理的上下文中，表示的准确性只会有很小的损失，但像 INT8 这样的较小动态范围格式会带来挑战。在量化过程中，我们必须将非常高的 FP32 动态范围压缩到 INT8 的 255 个值中，甚至压缩到 INT4 的 15 个值中！

为了减轻这一挑战，已经开发了各种用于量化模型的技术，例如每通道或每层缩放，它们调整权重和激活张量的比例和零点值以更好地适应量化格式。其他技术，例如量化感知训练，也可以通过在训练期间模拟量化过程来帮助准备量化模型。

这种挤压（估计范围）是通过称为校准的过程完成的。

## 量化：校准

![img](https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-2a.jpg)

输入范围为**x ∈ [-α, α]** ，最大 α 值 (amax) 被校准以最大化精度。校准 α 后，通过乘/除比例因子 s 来执行映射。这种方法称为标度量化。

![img](https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-6.png)

α-interval 之外的动态范围内的所有内容都将被剪裁，并且它内部的所有内容都将四舍五入到最接近的整数。我们必须小心选择这些范围——大的 alpha 会“覆盖”更多的值，然而，会导致粗量化和高量化误差，因此这些范围的选择通常是限幅误差和舍入误差之间的权衡。

校准过程可以使用不同的方法执行，具体取决于模型和用例的要求。

最常支持的是大多数框架都支持的*percentile*、*max*和*entropy方法。*

- Max——使用绝对值分布的最大值作为 α **，**使用这种方法我们没有裁剪误差，但如果有异常值，舍入误差将很大。
- 熵——选择最小化量化分布和原始分布之间的 KL 散度的**α 。**如果原始分布接近正态分布，则此方法表现良好，可在裁剪误差和舍入误差之间实现最佳权衡。但是，如果分布远离正态分布，则其性能会受到阻碍。
- 百分位数——使用对应于绝对值分布的第 k 个百分位数的**α 。**作为一种启发式方法，它需要调整附加参数 - *k* - 允许用户在删除异常值的同时实现所需的权衡。

![img](https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-3a.jpg)

在这里，我们可以看到 ResNet50 中某一层的输入激活直方图和使用不同方法校准的范围（alpha）（[图像源](https://arxiv.org/pdf/2004.09602.pdf)）。

## 训练后量化和量化感知训练

训练后量化 (PTQ) 是一种量化技术，其中模型在训练后进行量化。

量化感知训练 (QAT) 是对 PTQ 模型的微调，其中在考虑量化的情况下进一步训练模型。量化过程（缩放、裁剪和舍入）被合并到训练过程中，使模型即使在量化后也能保持其准确性，从而在部署期间带来好处（更低的延迟、更小的模型大小、更低的内存占用）。

![img](https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-4a.jpg)

在 QAT 过程之后不需要校准，因为模型是在训练过程中校准的。该训练通过模拟推理时间量化来优化模型权重，以提高下游任务的模型性能。它在训练期间使用称为 Q/DQ（量化然后反量化）的“假”量化模块来模仿测试或推理阶段的行为。例如，DNN 的权重在训练期间被四舍五入或限制为 16 位浮点或 8 位整数表示。

神经网络的前向和反向传递使用低精度权重。损失函数针对低精度计算误差调整模型。因此，由于模型在训练过程中意识到量化，因此量化模型可以在现实世界推理过程中实现更高的准确性。

如图（[图片来源](https://deci.ai/)）所示，QAT 在训练过程中结合了量化，这意味着模型经过优化，从一开始就在量化环境中表现良好。相比之下，PTQ 在训练后应用量化，这可能会由于原始模型和量化模型之间的不匹配而导致准确性损失。此外，QAT 允许对量化过程进行更细粒度的控制，因为不同的层甚至单个权重和激活可以根据它们对量化误差的敏感度进行不同的量化。这导致更好的精度保持。

## 量化类型：朴素、混合和选择性

到目前为止，我们已经介绍了要考虑的各种量化级别，以及何时应用量化的一些方法。然而，在尝试量化您的模型时，还有另一个因素需要考虑——哪种类型的量化将产生满足您需求的最佳结果。在本节中，我们将比较三种类型的量化。

### 朴素量化

在朴素量化中，所有运算符都被量化为 INT8 精度，并使用相同的方法进行校准。

请记住，某些架构层是敏感的，因此准确性会急剧下降。例如，在像 YOLO 这样的单阶段检测模型中，用于分类和边界框回归的最后一层是最敏感的，因此排除它们是一种常见的做法。

此外，一些运算符序列对 INT8 不友好——延迟不会改善（甚至变得更糟！）。推理框架将运算符序列融合为一个运算符（如 Conv-BN-ReLU），因此在受支持的硬件上执行速度更快。如果其中一些运算符处于不同的数据类型中，这将是不可能的。

此外，有些算子不能在 INT8 中融合，只能在 FP16 中融合，例如 NVIDIA 的 TensorRT 8.4 中的 GroupConv-BN-Swish，这在很大程度上取决于硬件、推理框架及其版本。为了延迟和准确性，最好将它们保留在 FP16 中。

虽然易于实现，但与原始浮点模型相比，朴素量化通常会导致模型精度显着下降。这是因为相同的量化方法适用于所有运算符，而不管它们对量化的敏感度如何。相反，使用更复杂的量化方法，例如混合或选择性量化，可以在延迟和准确性方面提供更好的结果。

### 混合量化

在混合量化中，一些算子被量化为 INT8 精度，而一些则保留在模式代表数据类型中，如 FP16 或 FP32。

为此，您必须事先了解神经网络结构及其量化敏感层，或者您需要执行敏感性分析：逐层排除并观察延迟/准确性的变化。

有些块需要特殊结构才能使用 NVIDIA TensorRT 等推理框架成功转换。虽然它的编译器可以融合 Conv-BN-ReLU，但它需要特别考虑 Conv-BN-Add 序列，向 Add 操作的另一个输入添加一个量化器。这一点尤其突出，在大多数现代 CNN 中都有残余连接，尤其是 ResNet。如果某处有非量化的加法，则整个算子序列都不会被量化，延迟会更差。

### 选择性量化

在选择性量化中，部分算子量化为INT8精度，校准方式不同，粒度不同（per channel or per tensor），残差量化为INT8，敏感层和非友好层保持FP16精度。此外，通过选择性量化，我们可以更改模型的整个部分以更好地适应量化。这种类型为用户在为不同类型的网络选择量化参数方面提供了最大的灵活性，以同时最大限度地提高准确性和最小化延迟。

例如，像 RepVGG 这样的模型在它们的分支中具有截然不同的分布，因此通常最好根据不同的校准来量化不同的层。有些人使用最大校准会更好，而其他人会有异常值，并且使用百分位数来移除它们会更好——所以我们可以对不同的层使用不同的校准器。激活分布不同，权重和激活需要不同的校准器。

为什么要使用选择性量化？
