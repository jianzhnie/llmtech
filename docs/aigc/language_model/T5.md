## 简介

T5全称是 **Text-to-Text Transfer Transformer**，是解决NLP任务的一种范式。最重要作用是给**整个 NLP 预训练模型领域提供了一个通用框架**，把所有任务都转化成一种形式，正如论文里所说的

> Introducing a unified framework that converts every language problem into a text-to-text format.

如下图，就是把所有任务，如分类、相似度计算、文本生成都用一个 Text-to-text（文本到文本）的框架里进行解决。

![img](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821c34a1-b63b-4ea9-9ced-0c73549288d2_2238x1266.png)

- 机器翻译：“That is good.” 英翻德，在输入句子前加上“translate English to German: ”的 prefix ，输入转换成 "translate English to German：That is good."  输入模型，之后就可以直接输出德语翻译 “Das ist gut.”
- 文本分类：输出与目标标签对应的单个单词。例如 NLI 任务中输出entailment、contradiction 或 neutral。如果输出的文本与该任务的标签都不符合，则视为错误(不过未在任何经过训练的模型中出现)。 例如情感分类任务，输入"sentiment：This movie is terrible!"，前面直接加上 “sentiment：”，然后就能输出结果“negative（负面）”。

一些任务为了适应 Text-to-text 框架，需要进行调整：

- STS-B。这是预测 1-5 之间的相似性得分的一项回归任务。作者发现大部分得分是以 0.2 为增量的，于是将分数四舍五入到最接近的 0.2 增量（2.57 四舍五入为 2.6）并将其转为字符串。测试时，如果模型输出字符串对应于 1-5 之间的数字，则转为浮点数进行判断，否则视为错误。这有效地将 STS-B 的回归问题转换为 21 类分类问题。如上图中，输出 3.8 其实不是数值，而是一串文本，之所以能进行这样的操作，应该完全赖于 T5 模型强大的容量。
- Winograd任务(WNLI、WSC 和 DPR)转换为一种更适合Text-to-text框架的格式。此类任务的示例是一段包含一个歧义代词的文本，歧义代词可以指代段落中不止一个名词短语。例如：“The city councilmen refused the demonstrators a permit because they feared violence.” 的 “they” 指代的是“city councilmen”还是“demonstrators”。模型将输入中的歧义代词用*包裹以突出：“The city councilmen refused the demonstrators a permit because *they* feared violence.” 并且要求模型输出答案 “city councilmen” 。WSC 数据集的样本由文章、歧义代词、候选名词及对应的 True/False 标签组成，这就无法知晓标签为 False 的样本的歧义代词所指代的名词是什么，因此只在标签为 True 的样本上进行训练（大约删除了 WSC 训练集的一半）。DPR 数据集则很适用上述格式。
- WNLI 的训练集和验证集与 WSC 的训练集有很大重叠，为了防止验证集出现在训练集中，不训练 WNLI，并且由于其训练集和验证集之间是对抗的（dev=train+扰动且标签相反），所以也不汇报其验证集的结果。将示例从WNLI转换为上述“指称名词预测”变体的过程要复杂得多，参见附录B。

通过这样的方式就能将 NLP 任务都转换成 Text-to-Text 形式，也就可以**用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务**。

## Architectures

### Model structures

Attention masks：不同体系结构的主要区别因素是模型中不同注意力机制所使用的“掩码”。Transformer 中的自注意操作将一个序列作为输入，并输出相同长度的新序列。通过计算输入序列的加权平均值来生成输出序列的每个条目。

图3 是代表不同的注意力掩码矩阵。自注意力机制的输入和输出分别表示为x和y。第 i 行和第 j 列中的黑色单元表示在输出时间步骤 i 允许自注意力机制参与输入元素 j。白色单元格表示不允许自注意力机制参与相应的 i 和 j 组合。

- 左：完全可见的掩码。输出的每个时间步会注意全部输入
- 中：因果掩码。防止第 i 个输出元素依赖于“未来”的任何输入元素
- 右：带前缀的因果掩码。使自注意力机制可以在输入序列的一部分上使用完全可见的掩码。

Transformer体系结构变体的示意图。方框代表序列的元素，线条代表注意力的可见度。不同颜色的块组指示不同的 Transformer 层块。深灰色线对应于完全可见的掩码，浅灰色线对应于因果的掩码。我们使用“·”表示表示预测结束的特殊序列结束标记。输入和输出序列分别表示为 x 和 y。

- 左：标准的编码器-解码器体系结构，在 encoder 和encoder-decoder 注意力中使用完全可见的掩码，在解码器中使用因果掩码。
- 中：语言模型由一个单独的 Transformer 层块组成，并通过使用因果掩码始终输入输入和目标的串联。
- 右：在语言模型中添加前缀并对这部分输入使用完全可见的掩码。

Encoder-decoder：图 4 左侧展示了编码器-解码器结构，编码器使用“完全可见”的注意掩码。这种掩码适用于注意“前缀”，即提供给模型的某些上下文，供以后进行预测时使用。BERT也使用了完全可见掩码，并在输入中附加了特殊的“分类”标记。然后，在与分类令牌相对应的时间步中，BERT的输出将用于对输入序列进行分类的预测。

Language model：Transformer 中的解码器用于自回归生成输出序列，即在每个输出时间步，都会从模型的预测分布中选取令牌，然后将选取的令牌再输入到模型中为下一个输出时间步做出预测。这样，可以将 Transformer 解码器用作语言模型，即仅训练用于下一步预测的模型。此架构的示意图如图 4 中间所示。实际上，针对NLP的迁移学习的早期工作使用这种架构并将语言建模目标作为一种预训练方法。

语言模型通常用于压缩或序列生成。但是，它们也可以简单地通过连接输入和目标而用于 text-to-text 框架中。例如，考虑英语到德语的翻译：如果我们有一个训练数据的输入句子为“ That good.”，目标为“Das ist gut.”，那么我们只需在连接的输入序列“translate English to German: That is good. Target: Das ist gut.” 上对模型进行语言模型训练(错位预测)即可。如果我们想获得此示例的模型预测，则向模型输入前缀“translate English to German: That is good. Target: ”，模型自回归生成序列的其余部分。通过这种方式，该模型可以预测给定输入的输出序列，从而满足 text-to-text 任务。这种方法最近被用来表明语言模型可以学会在无监督的情况下执行一些 text-to-text 的任务。

Prefix LM：在 text-to-text 设置中使用语言模型的一个基本且经常被提到的缺点是，因果掩码会迫使模型对输入序列的第 i 个输入的表示仅取决于直到 i 为止的输入部分。在该框架中，在要求模型进行预测之前，为模型提供了前缀/上下文（例如，前缀为英语句子，并且要求模型预测德语翻译）。使用完全因果掩码，前缀状态的表示只能取决于前缀的先前条目。因此，在预测输出的条目时，模型使用的前缀表示是不需要受到限制的，但却由于语言模型而使其前缀表示受到了限制。在序列到序列模型中使用单向递归神经网络编码器也存在类似问题。

只需更改掩码模式，就可以在基于Transformer的语言模型中避免此问题。在序列的前缀部分使用完全可见的掩码。图 3 和 4 的右边分别显示了此掩码模式和前缀LM的示意图。

### Comparing different model structures

为了实验上比较这些体系结构变体，我们希望每个模型在某种意义上都是等效的：如果两个模型具有相同数量的参数，或者它们需要大致相同的计算量来处理给定的（输入序列，目标序列）对，则可以说是等效的。但不可能同时根据这两个标准将编码器-解码器模型与语言模型体系结构（包含单个Transformer块）进行比较。由于在编码器中具有L层且在解码器中具有L层的编码器-解码器模型具有与具有2L层的语言模型大约相同数量的参数。但是，相同的L + L编解码器模型将具有与仅具有 L 层的语言模型大约相同的计算成本，这是因为语言模型中的L层必须同时应用于输入和输出序列，而编码器仅应用于输入序列，而解码器仅应用于输出序列。所以存在参数量不同，但计算量几乎相同的情况。这些等价是近似的——由于对编码器的注意力，解码器中存在一些额外的参数，并且在注意力层中，序列长度为平方的计算量也很大。然而，实际上，我们观察到L层语言模型与L + L层编码器-解码器模型几乎相同的步长时间，这表明计算成本大致相当。

为了提供合理的比较方法，我们考虑了编码器-解码器模型的多种配置。我们将 大小的层块中的层数和参数分别称为 L 和 P 。

我们将使用 M 来指代L + L层编码器-解码器模型或仅L层的解码器模型处理给定输入目标对所需的FLOP数量。

总的来说，我们将进行比较：

- 在编码器中具有 L 层，在解码器中具有 L 层的编码器-解码器模型。该模型具有 2P 个参数和M FLOP的计算成本。
- 等效模型，但参数在编码器和解码器之间共享，即 P 个参数和 M FLOP计算成本。
- 在编码器和解码器中各具有 L / 2 层的编码器-解码器模型，提供 P 参数和 M/2 FLOP成本。
- 具有 L 层和 P 参数的纯解码器的语言模型，以及由此产生的M FLOP计算成本。
- 具有相同架构（因此具有相同数量的参数和计算成本），但对输入具有完全可见的自我注意力的解码器的前缀LM。
