# Transformerçš„KV Cachingæœºåˆ¶è¯¦è§£

## KV Cachingæ¦‚è¿°

ç”Ÿæˆå¼Transformeræ¨¡å‹ä¸­çš„é”®(Key)å’Œå€¼(Value)çŠ¶æ€ç¼“å­˜æŠ€æœ¯å·²å­˜åœ¨ä¸€æ®µæ—¶é—´ï¼Œä½†æ‚¨å¯èƒ½éœ€è¦ç¡®åˆ‡ç†è§£å®ƒçš„åŸç†åŠå…¶å¸¦æ¥çš„æ˜¾è‘—æ¨ç†åŠ é€Ÿæ•ˆæœã€‚

é”®å€¼çŠ¶æ€ç”¨äºè®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›(scaled dot-product attention)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›åŠå…¶åœ¨Transformeræ¶æ„ä¸­çš„åº”ç”¨ä½ç½®](https://miro.medium.com/v2/resize:fit:690/0*6D_17aytq215gMcF.png)

> ***KV Cachingå‘ç”Ÿåœ¨å¤štokenç”Ÿæˆæ­¥éª¤ä¸­ï¼Œä»…å­˜åœ¨äºè§£ç å™¨éƒ¨åˆ†*** *(ä¾‹å¦‚GPTç­‰çº¯è§£ç å™¨æ¨¡å‹ï¼Œæˆ–T5ç­‰ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„è§£ç å™¨éƒ¨åˆ†)ã€‚åƒBERTè¿™æ ·çš„éç”Ÿæˆå¼æ¨¡å‹ä¸ä½¿ç”¨KV Cachingã€‚*

### è‡ªå›å½’è§£ç æœºåˆ¶

è§£ç å™¨ä»¥è‡ªå›å½’æ–¹å¼å·¥ä½œï¼Œå¦‚ä¸‹é¢GPT-2æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹æ‰€ç¤ºï¼š

![GPT-2è§£ç å™¨çš„è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹](https://miro.medium.com/v2/resize:fit:700/0*sexO6adGhaKr7aH0.gif)

åœ¨è§£ç å™¨çš„è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç„¶åå°†ç»„åˆè¾“å…¥ç”¨äºä¸‹ä¸€æ­¥é¢„æµ‹ã€‚

è¿™ç§è‡ªå›å½’è¡Œä¸ºä¼šé‡å¤æŸäº›è®¡ç®—æ“ä½œã€‚é€šè¿‡æ”¾å¤§è§‚å¯Ÿè§£ç å™¨ä¸­çš„æ©ç ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ¸…æ¥šåœ°ç†è§£è¿™ä¸€ç‚¹ï¼š

![è§£ç å™¨ä¸­ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„é€æ­¥å¯è§†åŒ–](https://miro.medium.com/v2/resize:fit:700/1*8xqD4AYTwn6mQXNw0uhDCg.gif)

ç”±äºè§£ç å™¨æ˜¯å› æœçš„(å³tokençš„æ³¨æ„åŠ›ä»…å–å†³äºå…¶å‰é¢çš„token)ï¼Œåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­æˆ‘ä»¬éƒ½åœ¨é‡å¤è®¡ç®—ç›¸åŒçš„å‰ç½®tokenæ³¨æ„åŠ›ï¼Œè€Œå®é™…ä¸Šæˆ‘ä»¬åªéœ€è¦è®¡ç®—æ–°tokençš„æ³¨æ„åŠ›ã€‚

### è‡ªå›å½’è§£ç ä»£ç ç¤ºä¾‹

```python
import torch

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
# torch.manual_seed(0)

class Sampler:
    def __init__(self , model_name : str ='gpt2-medium') -> None:

        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to("cpu").to(self.device)

    def encode(self, text):
        return self.tokenizer.encode(text, return_tensors='pt').to(self.device)

    def decode(self, ids):
        return self.tokenizer.decode(ids)

    def get_next_token_prob(self, input_ids: torch.Tensor):
        with torch.no_grad():
            logits = self.model(input_ids=input_ids).logits
        logits = logits[0, -1, :]
        return logits

class GreedySampler(Sampler):
    def __call__(self, prompt, max_new_tokens=10):
        predictions = []
        result = prompt
        # generate until max_len
        for i in range(max_new_tokens):

            print(f"step {i} input: {result}")
            input_ids = self.encode(result)
            next_token_probs = self.get_next_token_prob(input_ids=input_ids)

            # choose the token with the highest probability
            id = torch.argmax(next_token_probs, dim=-1).item()
            # convert to token and add new token to text
            result += self.decode(id)

            predictions.append(next_token_probs[id].item())

        return result
```



```shell
gs = GreedySampler()
gs(prompt="Large language models are recent advances in deep learning", max_new_tokens=10)

step 0 input: Large language models are recent advances in deep learning
step 1 input: Large language models are recent advances in deep learning,
step 2 input: Large language models are recent advances in deep learning, which
step 3 input: Large language models are recent advances in deep learning, which uses
step 4 input: Large language models are recent advances in deep learning, which uses deep
step 5 input: Large language models are recent advances in deep learning, which uses deep neural
step 6 input: Large language models are recent advances in deep learning, which uses deep neural networks
step 7 input: Large language models are recent advances in deep learning, which uses deep neural networks to
step 8 input: Large language models are recent advances in deep learning, which uses deep neural networks to learn
step 9 input: Large language models are recent advances in deep learning, which uses deep neural networks to learn to

```

å¯ä»¥çœ‹åˆ°ï¼Œéšç€æ¯æ¬¡æ¨ç†çš„è¾“å…¥tokenå˜é•¿ï¼Œæ¨ç†FLOPs(æµ®ç‚¹è¿ç®—)ä¼šå¢åŠ ã€‚KV Cachingé€šè¿‡å­˜å‚¨å…ˆå‰è®¡ç®—çš„é”®å€¼å¯¹çš„éšè—è¡¨ç¤ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

ä¾‹å¦‚åœ¨ç¬¬4æ­¥ç”Ÿæˆ"deep"æ—¶ï¼Œæˆ‘ä»¬åªéœ€å°†"uses"è¾“å…¥æ¨¡å‹ï¼Œå¹¶ä»ç¼“å­˜ä¸­è·å–"Large language models are recent advances in deep learning, which"çš„è¡¨ç¤ºã€‚

### KV Cachingæ˜¯ä»€ä¹ˆï¼Ÿ

KV Cachingæ˜¯æå‡å¤§æ¨¡å‹æ¨ç†æ€§èƒ½çš„å¸¸ç”¨æŠ€æœ¯ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä¸Šä¸€æ¬¡æ¨ç†çš„KV Cachingæ¥æé«˜æ¨ç†æ€§èƒ½ï¼Œå‡å°‘ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ŒåŒæ—¶ä¸å½±å“å‡†ç¡®æ€§ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦KV Cachingï¼Ÿ

åœ¨GPTç­‰è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸­ç”Ÿæˆæ–‡æœ¬(token)æ—¶ï¼Œæ¯æ¬¡ç”Ÿæˆæ–°tokenéƒ½éœ€è¦å°†ä¹‹å‰ç”Ÿæˆçš„æ‰€æœ‰tokenè¾“å…¥ç½‘ç»œã€‚è¿™æ„å‘³ç€ä¹‹å‰ç”Ÿæˆtokençš„éšè—è¡¨ç¤ºæ¯æ¬¡éƒ½éœ€è¦é‡æ–°è®¡ç®—ï¼Œé€ æˆå¤§é‡è®¡ç®—æµªè´¹ã€‚

## KV Cachingçš„å·¥ä½œåŸç†

åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒTransformeræ¨¡å‹[ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªtoken](https://neptune.ai/blog/customizing-llm-output-post-processing-techniques)ã€‚å½“æˆ‘ä»¬æç¤ºæ¨¡å‹å¼€å§‹ç”Ÿæˆæ—¶(ä¾‹å¦‚è¾“å…¥"She")ï¼Œå®ƒå°†äº§ç”Ÿä¸€ä¸ªè¯(å¦‚"poured")ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†"She poured"ä¼ é€’ç»™æ¨¡å‹ï¼Œå®ƒä¼šç”Ÿæˆ"coffee"ã€‚æ¥ç€æˆ‘ä»¬ä¼ å…¥"She poured coffee"å¹¶è·å¾—åºåˆ—ç»“æŸtokenï¼Œè¡¨ç¤ºç”Ÿæˆå®Œæˆã€‚

è¿™æ„å‘³ç€æˆ‘ä»¬è¿è¡Œäº†ä¸‰æ¬¡å‰å‘ä¼ æ’­ï¼Œæ¯æ¬¡éƒ½å°†æŸ¥è¯¢(queries)ä¸é”®(keys)ç›¸ä¹˜ä»¥è·å¾—æ³¨æ„åŠ›åˆ†æ•°(åŒæ ·é€‚ç”¨äºåç»­ä¸å€¼(values)çš„ä¹˜æ³•)ã€‚

### è®¡ç®—å†—ä½™åˆ†æ

1. ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­åªæœ‰å•ä¸ªè¾“å…¥token("She")ï¼Œäº§ç”Ÿå•ä¸ªé”®å‘é‡å’ŒæŸ¥è¯¢å‘é‡ï¼Œç›¸ä¹˜å¾—åˆ°q1k1æ³¨æ„åŠ›åˆ†æ•°ã€‚

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-2.png" alt="ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­çš„è®¡ç®—" style="zoom:33%;" />

2. ä¼ å…¥"She poured"åï¼Œæ¨¡å‹çœ‹åˆ°ä¸¤ä¸ªè¾“å…¥tokenï¼Œæ³¨æ„åŠ›æ¨¡å—è®¡ç®—å¦‚ä¸‹ï¼š

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-3.png" alt="ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­çš„è®¡ç®—" style="zoom:33%;" />

æˆ‘ä»¬è®¡ç®—äº†ä¸‰ä¸ªé¡¹ï¼Œä½†q1k1æ˜¯ä¸å¿…è¦çš„é‡å¤è®¡ç®—â€”â€”å› ä¸ºï¼š

- q1æ˜¯è¾“å…¥("She")çš„åµŒå…¥ä¹˜ä»¥WqçŸ©é˜µ
- k1æ˜¯è¾“å…¥("She")çš„åµŒå…¥ä¹˜ä»¥WkçŸ©é˜µ
- åµŒå…¥å’Œæƒé‡çŸ©é˜µåœ¨æ¨ç†æ—¶éƒ½æ˜¯æ’å®šçš„

3. ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­çš„æŸ¥è¯¢-é”®è®¡ç®—ï¼š

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-4.png" alt="ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­çš„è®¡ç®—" style="zoom:33%;" />

æˆ‘ä»¬è®¡ç®—äº†å…­ä¸ªå€¼ï¼Œå…¶ä¸­ä¸€åŠæ˜¯å·²çŸ¥ä¸”ä¸éœ€è¦é‡æ–°è®¡ç®—çš„ï¼

### KV Cachingæœºåˆ¶

KV Cachingçš„åŸç†æ˜¯ï¼šåœ¨æ¨ç†æ—¶ï¼Œå½“æˆ‘ä»¬è®¡ç®—é”®(K)å’Œå€¼(V)çŸ©é˜µæ—¶ï¼Œå°†å…¶å…ƒç´ å­˜å‚¨åœ¨ç¼“å­˜ä¸­ã€‚ç¼“å­˜æ˜¯ä¸€ä¸ªè¾…åŠ©å†…å­˜ï¼Œæ”¯æŒé«˜é€Ÿæ£€ç´¢ã€‚åœ¨ç”Ÿæˆåç»­tokenæ—¶ï¼Œæˆ‘ä»¬åªè®¡ç®—æ–°tokençš„é”®å’Œå€¼ã€‚

ä¾‹å¦‚ï¼Œä½¿ç”¨ç¼“å­˜æ—¶ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­å¦‚ä¸‹ï¼š

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-5.png" alt="ä½¿ç”¨ç¼“å­˜åçš„ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­" style="zoom:33%;" />

å¤„ç†ç¬¬ä¸‰ä¸ªtokenæ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦é‡æ–°è®¡ç®—å‰ä¸¤ä¸ªtokençš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¯ä»¥ä»ç¼“å­˜ä¸­æ£€ç´¢å®ƒä»¬çš„é”®å’Œå€¼ï¼Œä»è€ŒèŠ‚çœè®¡ç®—æ—¶é—´ã€‚

### KV Caching åŸç†

è¿™æ­£æ˜¯KV Cachingå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚é€šè¿‡ç¼“å­˜å…ˆå‰çš„Keyså’ŒValuesï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºä»…è®¡ç®—æ–°tokençš„æ³¨æ„åŠ›ï¼š

![ä½¿ç”¨ä¸ä¸ä½¿ç”¨KV Cachingçš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å¯¹æ¯”](https://miro.medium.com/v2/resize:fit:700/1*uyuyOW1VBqmF5Gtv225XHQ.gif)

ä¸ºä»€ä¹ˆè¿™ç§ä¼˜åŒ–å¾ˆé‡è¦ï¼Ÿå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œ**ä½¿ç”¨KV Cachingè·å¾—çš„çŸ©é˜µè¦å°å¾—å¤šï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„çŸ©é˜µä¹˜æ³•è¿ç®—**ã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ›´å¤šçš„GPUæ˜¾å­˜(å¦‚æœä¸ä½¿ç”¨GPUåˆ™éœ€è¦æ›´å¤šCPUå†…å­˜)æ¥ç¼“å­˜Keyå’ŒValueçŠ¶æ€ã€‚

## KV Cachingçš„æ•°å­¦è¡¨è¾¾

ç»™å®šç”Ÿæˆçš„ç¬¬ $t $ ä¸ª token åœ¨ Transformer å±‚ä¸­çš„è¡¨ç¤ºï¼Œè®°ä½œ $t^i \in \mathbb{R}^{b \times 1 \times h} $ï¼Œå…¶ä¸­ï¼š

- $b $ è¡¨ç¤º batch size
- $h $ è¡¨ç¤º hidden dimension

åœ¨ Transformer ç¬¬ $t $ å±‚çš„è®¡ç®—åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

1. KV Cachingçš„æ›´æ–°
2. ä¸‹ä¸€å±‚è¾“å…¥ $t^{i+1} $ çš„è®¡ç®—

### KV Cachingæ›´æ–°å…¬å¼

$$
\begin{aligned}
x_{K}^i &\leftarrow \text{Concat} \left( x_{K}^i, t^i \cdot W_{K}^i \right) \\
x_{V}^i &\leftarrow \text{Concat} \left( x_{V}^i, t^i \cdot W_{V}^i \right)
\end{aligned}
$$

### å‰©ä½™è®¡ç®—æ­¥éª¤

1. Query å‘é‡è®¡ç®—ï¼š
   $$
   t_Q^i = t^i \cdot W_Q^i
   $$

2. Attention è¾“å‡ºï¼š
   $$
   t_{\text{out}}^i = \text{softmax} \left( \frac{t_Q^i x_{K}^{i\top}}{\sqrt{h}} \right) \cdot x_V^i \cdot W_O^i + t^i
   $$

3. Feed-Forward è®¡ç®—ï¼š
   $$
   t^{i+1} = f_{\text{activation}} \left( t_{\text{out}}^i \cdot W_1 \right) \cdot W_2 + t_{\text{out}}^i
   $$



### KV Cachingå®ç°

å‡è®¾æ¶æ„ä¸­æœ‰nä¸ªTransformerå±‚ï¼Œé‚£ä¹ˆæ¯ä¸ªæ³¨æ„åŠ›å¤´å°†ç»´æŠ¤è‡ªå·±ç‹¬ç«‹çš„KV Cachingï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# æ¨¡æ‹Ÿæ¨¡å‹å‚æ•°ç»“æ„
class ModelArgs:
    def __init__(self, dim=16, n_heads=2, max_seq_len=8, max_batch_size=1):
        self.dim = dim
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.max_seq_len = max_seq_len
        self.max_batch_size = max_batch_size


class SelfAttention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.n_heads = args.n_heads
        self.head_dim = args.head_dim
        self.dim = args.dim

        self.w_q = nn.Linear(self.dim, self.dim, bias=False)
        self.w_k = nn.Linear(self.dim, self.dim, bias=False)
        self.w_v = nn.Linear(self.dim, self.dim, bias=False)
        self.w_o = nn.Linear(self.dim, self.dim, bias=False)

        # ç¼“å­˜åˆå§‹åŒ–
        self.register_buffer("cache_k", torch.zeros(
            args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim
        ))
        self.register_buffer("cache_v", torch.zeros(
            args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim
        ))

    def forward(self, x: torch.Tensor, start_pos: int):
        # x shape: (B, 1, D)
        B, S, D = x.size()
        H = self.n_heads
        Hd = self.head_dim

        # Linear projections
        q = self.w_q(x).view(B, S, H, Hd)
        k = self.w_k(x).view(B, S, H, Hd)
        v = self.w_v(x).view(B, S, H, Hd)

        # æ›´æ–°ç¼“å­˜
        self.cache_k[:B, start_pos:start_pos+S] = k
        self.cache_v[:B, start_pos:start_pos+S] = v

        # è·å–å½“å‰å…¨éƒ¨ key/valueï¼ˆä»0åˆ°å½“å‰ä½ç½®ï¼‰
        keys = self.cache_k[:B, :start_pos+S]   # (B, Seq_KV, H, Hd)
        values = self.cache_v[:B, :start_pos+S]

        # Attentionè®¡ç®—
        q = q.transpose(1, 2)           # (B, H, 1, Hd)
        k = keys.transpose(1, 2)        # (B, H, Seq_KV, Hd)
        v = values.transpose(1, 2)      # (B, H, Seq_KV, Hd)

        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (Hd ** 0.5)  # (B, H, 1, Seq_KV)
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)  # (B, H, 1, Hd)

        attn_output = attn_output.transpose(1, 2).contiguous().view(B, S, D)  # (B, 1, D)
        out = self.w_o(attn_output)
        return out

# ====== æµ‹è¯•ç¤ºä¾‹ ======

torch.manual_seed(42)
args = ModelArgs()
attn = SelfAttention(args)

# æ¨¡æ‹Ÿä¸€ä¸ªåºåˆ—åˆ†æ­¥ç”Ÿæˆï¼Œæ¯æ­¥è¾“å…¥ä¸€ä¸ª token
sequence = torch.randn(1, args.max_seq_len, args.dim)
outputs = []
for i in range(args.max_seq_len):
    x = sequence[:, i:i+1, :]  # å½“å‰æ—¶é—´æ­¥çš„ token
    y = attn(x, start_pos=i)   # KV Caching è‡ªåŠ¨ç”Ÿæ•ˆ
    outputs.append(y)

# æ‹¼æ¥ç»“æœ
final_out = torch.cat(outputs, dim=1)
print("ğŸ§¾ æœ€ç»ˆè¾“å‡º shape:", final_out.shape)
print(final_out)
```



## KV Cachingæ€§èƒ½å½±å“è¯„ä¼°

KV Cachingå¯èƒ½å¯¹æ¨ç†æ—¶é—´äº§ç”Ÿé‡å¤§å½±å“ã€‚å½±å“ç¨‹åº¦å–å†³äºæ¨¡å‹æ¶æ„ã€‚å¯ç¼“å­˜çš„è®¡ç®—è¶Šå¤šï¼Œå‡å°‘æ¨ç†æ—¶é—´çš„æ½œåŠ›å°±è¶Šå¤§ã€‚

æˆ‘ä»¬ä½¿ç”¨[transformersğŸ¤—](https://github.com/huggingface/transformers)åº“æ¯”è¾ƒGPT-2åœ¨ä½¿ç”¨å’Œä¸ä½¿ç”¨KV Cachingæ—¶çš„ç”Ÿæˆé€Ÿåº¦ï¼š

```python
import numpy as np
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2").to(device)

for use_cache in (True, False):
  times = []
  for _ in range(10):  # æµ‹é‡10æ¬¡ç”Ÿæˆ
    start = time.time()
    model.generate(**tokenizer("What is KV caching?", return_tensors="pt").to(device), use_cache=use_cache, max_new_tokens=1000)
    times.append(time.time() - start)
  print(f"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds")
```

åœ¨Google Colabç¬”è®°æœ¬ä¸Šä½¿ç”¨Tesla T4 GPUï¼Œç”Ÿæˆ1000ä¸ªæ–°tokençš„å¹³å‡æ—¶é—´å’Œæ ‡å‡†å·®å¦‚ä¸‹ï¼š

> ä½¿ç”¨KV Caching: 11.885 Â± 0.272ç§’
> ä¸ä½¿ç”¨KV Caching: 56.197 Â± 1.855ç§’

æ¨ç†é€Ÿåº¦å·®å¼‚å·¨å¤§ï¼Œè€ŒGPUæ˜¾å­˜ä½¿ç”¨é‡å˜åŒ–å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚å› æ­¤è¯·ç¡®ä¿åœ¨æ‚¨çš„Transformeræ¨¡å‹ä¸­ä½¿ç”¨KV Cachingï¼
