# HybridFlow: A Flexible and Efficient RLHF Framework

- *HybridFlow: A Flexible and Efficient RLHF Framework*

- *论文链接：https://team.doubao.com/zh/publication/hybridflow-a-flexible-and-efficient-rlhf-framework?view_from=research*

- *代码链接：https://github.com/volcengine/veRL*
- 在线文档：https://verl.readthedocs.io/en/latest/index.html


## 1. RL（Post-Training）复杂计算流程给 LLM 训练带来全新的挑战

在深度学习中，数据流（DataFlow）是一种重要的计算模式抽象，用于表示数据经过一系列复杂计算后实现特定功能。神经网络的计算就是典型的 DataFlow ，可以用计算图（Computational Graph）来描述，其中节点代表计算操作，边表示数据依赖。

大模型 RL 的计算流程比传统神经网络更为复杂。在 RLHF 中，需要同时训练多个模型，如 Actor 、Critic 、参考策略（Reference Policy）和奖励模型（Reward Model），并在它们之间传递大量数据。这些模型涉及不同的计算类型（前向反向传播、优化器更新、自回归生成等），可能采用不同的并行策略。

传统的分布式 RL 通常假设模型可在单个 GPU 上训练，或使用数据并行方式 [4,5]，将控制流和计算流合并在同一进程中。这在处理小规模模型时效果良好，但面对大模型，训练需要复杂的多维并行，涉及大量分布式计算，传统方法难以应对。

##  2. HybridFlow 解耦控制流和计算流，兼顾灵活高效

大模型 RL 本质上是一个二维的 DataFlow 问题：high-level 的控制流（描述 RL 算法的流程）+ low-level 的计算流（描述分布式神经网络计算）。

近期开源的 RLHF 框架，如 DeepSpeed-Chat [6]、OpenRLHF [7] 和 NeMo-Aligner [8]，采用了统一的多控制器（Multi-Controller）架构。各计算节点独立管理计算和通信，降低了控制调度的开销。然而，控制流和计算流高度耦合，当设计新的 RL 算法，组合相同的计算流和不同的控制流时，需要重写计算流代码，修改所有相关模型，增加了开发难度。

与此前框架不同，HybridFlow 采用了混合编程模型，控制流由单控制器（Single-Controller）管理，具有全局视图，实现新的控制流简单快捷，计算流由多控制器（Multi-Controller）负责，保证了计算的高效执行，并且可以在不同的控制流中复用。

尽管相比纯粹的多控制器架构，这可能带来一定的控制调度开销，但 HybridFlow 通过优化数据传输，降低了控制流与计算流之间的传输量，兼顾了灵活性和高效性。

## 3. 系统设计之一：Hybrid Programming Model (编程模型创新)

### 框架逻辑分析

![img](https://pic3.zhimg.com/v2-120db60c9af032bb5ddaab7ba831221e_1440w.jpg)



这是目前verl训练框架的配置情况，对于不同的训练角色，可以选择不同的**预训练模型**及**训练后端**的支持（多控制器)。随后将他们与`main_ppo.py`中对应的角色进行绑定，然后以参数的形式传入到`ray_trainer.py`中进行调用。对于trainer文件中实际是以一个单控制流函数`fit()`的方式来进行，只需要从对应的模型中获得计算值的情况，然后再导入到对应的算法模块与工具模块中，就可以快速的开展RL训练任务。

1.运行框架中浅黄色表示推理框架，深黄色表示训练框架

2.虚线模块表示在训练过程中并不一定被需要，可以结合训练算法进行删减

3.Reward模型有基于模型与基于规则的方法，并且目前训练推理框架可能都有支持，所以暂时写为optional。

4.Actor模块由于要进行rollout和training两个阶段，因此会在训练和推理框架之间进行切换(verl中在`sharding_manager`文件目录下)

### 3.1 封装单模型分布式计算

在 HybridFlow 中，每个模型（如 Actor、Critic、参考策略、奖励模型等）的分布式计算被封装为独立的模块，称为模型类。

这些模型类继承于基础的并行 Worker 类（如 3DParallelWorker 、FSDPWorker 等），通过抽象的 API 接口，封装了模型的前向、反向计算、优化器更新和自回归生成等操作。该封装方式提高了代码的复用性，便于模型的维护和扩展。

对于不同的 RL 控制流，用户可以直接复用封装好的模型类，同时自定义部分算法所需的数值计算，实现不同算法。当前 HybridFlow 可使用 Megatron-LM [13] 和 PyTorch FSDP [14] 作为训练后端，同时使用 vLLM [15] 作为自回归生成后端，支持用户使用其他框架的训练和推理脚本进行自定义扩展。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/IrH3BAPESuiahEibicAGHwOUHDjGSoSsicz8mibzJMoGH9c0bPYbbjOWBUiciaFBV3STgEM6HXIW3Vvmib3k6AOJnWt3uQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



### 3.2 灵活的模型部署

HybridFlow 提供了资源池（ResourcePool）概念，可以将一组 GPU 资源虚拟化，并为每个模型分配计算资源。不同的资源池实例可以对应不同设备集合，支持不同模型在同一组或不同组 GPU 上部署。这种灵活的模型部署方式，满足了不同算法、模型和硬件环境下的资源和性能需求。

![img](https://pica.zhimg.com/v2-677f0e776144e14a21c6fa7af61acde6_1440w.jpg)

首先构建的四类模型，会通过Ray，映射放置到不同的机器上。随后

（1）先使用vllm框架和prompt对Actor先进行response的输出（使用专用推理框架可以让推理的速度更快）。

（2）然后将输出的结果输入给其他三个框架进行运行（一般使用训练框架，因为训练框架可以避免精度问题），以获得在RL算法(例如PPO，GRPO等框架)所需要的计算输入。

（3）最后结合计算结果，再使用训练框架来对Actor和Critic模型进行训练。



### 3.3 统一模型间的数据切分

在大模型 RL 计算流程中，不同模型之间的数据传输涉及复杂的多对多广播和数据重分片。

为解决该问题，HybridFlow 设计了一套通用数据传输协议（Transfer Protocol），包括收集（collect）和分发（distribute）两个部分。

通过在模型类的操作上注册相应的传输协议，比如：@register(transfer_mode=3D_PROTO)，HybridFlow 可以在控制器层（Single-Controller）统一管理数据的收集和分发，实现模型间数据的自动重分片，支持不同并行度下的模型通信。

HybridFlow 框架已经支持多种数据传输协议，涵盖大部分数据重切分场景。同时，用户可灵活地自定义收集（collect）和分发（distribute）函数，将其扩展到更复杂的数据传输场景。



![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/IrH3BAPESuiahEibicAGHwOUHDjGSoSsicz8licpiakd4SicTbHtIgeia5U18HXyuJyic8elNC8iaQ5CTM43zcScfAzzq3pA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



### 3.4 支持异步 RL 控制流

在 HybridFlow 中，控制流部分采用单控制器架构，可灵活实现异步 RL 控制流。

当模型部署在不同设备集合上时，不同模型计算可并行执行，这提高了系统的并行度和效率。对于部署在同一组设备上的模型，HybridFlow 通过调度机制实现了顺序执行，避免资源争夺和冲突。

### 3.5 少量代码灵活实现各种 RL 控制流算法


得益于混合编程模型的设计，HybridFlow 可以方便地实现各种 RLHF 算法，如 PPO [9]、ReMax [10]、Safe-RLHF [11]、GRPO [12] 等。用户只需调用模型类的 API 接口，按算法逻辑编写控制流代码，无需关心底层的分布式计算和数据传输细节。

例如，实现 PPO 算法只需少量代码，通过调用 actor.generate_sequences 、critic.compute_values 等函数即可完成。同时，用户只需要修改少量代码即可迁移到 Safe-RLHF 、ReMax 以及 GRPO 算法。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/IrH3BAPESuiahEibicAGHwOUHDjGSoSsicz8Y3fibjOtmOpOHUfCiaiaQicDiaPLBmWebhI2cB4BQTPc6D5KX08AhAawDvA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)






## 4. 系统设计之二：3D-HybridEngine （训练推理混合技术）降低通信内存开销

在 Online RL 算法中，Actor 模型需要在训练和生成（Rollout）阶段之间频繁切换，且两个阶段可能采用不同并行策略。

具体而言，训练阶段，需要存储梯度和优化器状态，模型并行度（Model Parallel Size, MP）可能相应增高，而生成阶段，模型无需存储梯度和优化器状态，MP 和数据并行度（Data Parallel Size, DP）可能较小。因此，在两个阶段之间，模型参数需要重新分片和分配，依赖传统通信组构建方法会带来额外通信和内存开销。

此外，为了在新的并行度配置下使用模型参数，通常需要在所有 GPU 之间进行全聚合（All-Gather）操作，带来了巨大的通信开销，增加了过渡时间。

为解决这个问题，HybridFlow 设计了 3D-HybridEngine ，提升了训练和生成过程效率。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/IrH3BAPESuiahEibicAGHwOUHDjGSoSsicz89JadxTZdMK7U6TpPrLY53F7icJZ8wmjiapJsvoaFL5rpQkOUtmt1BvjA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*注：3D-HybridEngine 一次迭代的流程*

3D-HybridEngine 通过优化并行分组方法，实现了零冗余的模型参数重组，具体包括以下步骤：

- 定义不同的并行组

在训练和生成阶段，3D-HybridEngine 使用不同的三维并行配置，包括：流水线并行（PP）、张量并行（TP）和数据并行（DP）的大小。训练阶段的并行配置为 𝑝-𝑡-𝑑 。在生成阶段，我们新增一个新的微数据并行组（Micro DP Group，𝑑𝑔），用于处理 Actor 模型参数和数据的重组。生成阶段的并行配置为 𝑝𝑔-𝑡𝑔-𝑑𝑔-𝑑 。

- 重组模型参数过程

通过巧妙地重新定义生成阶段的并行分组，可以使每个 GPU 在生成阶段复用训练阶段已有的模型参数分片，避免在 GPU 内存中保存额外的模型参数，消除内存冗余。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/IrH3BAPESuiahEibicAGHwOUHDjGSoSsicz89NqZ1EfmvTr6SC22vgCTFPZibVqTvnXB65VhWorqMwt4CMyvEfxibZIw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



- 减少通信开销

参数重组过程中，3D-HybridEngine 仅在每个微数据并行组（Micro DP Group）内进行 All-Gather 操作，而非所有 GPU 之间进行。这大大减少了通信量，降低过渡时间，提高了整体的训练效率。



## 摘要

从人类反馈中进行强化学习（RLHF）在大语言模型（LLM）对齐中被广泛使用。传统的强化学习可以被建模为一个数据流，其中每个节点代表神经网络（NN）的计算，每条边表示NN之间的数据依赖关系。RLHF通过将每个节点扩展为分布式LLM训练或生成程序，并将每条边扩展为多对多的multicast ，从而复杂化了数据流。传统的RL框架使用单个控制器来指示节点内计算和节点间通信，这在RLHF中可能由于分布式节点内计算的控制调度开销而变得低效。现有的RLHF系统采用多控制器范式，但由于嵌套的分布式计算和数据通信，可能变得不灵活。我们提出了HybridFlow，它以混合方式结合了单控制器和多控制器范式，以实现RLHF数据流的灵活表示和高效执行。我们精心设计了一组分层API，将复杂的RLHF数据流中的计算和数据依赖关系解耦和封装，允许高效的操作编排以实现RLHF算法，并灵活地将计算映射到各种设备上。我们进一步设计了一个3D-HybridEngine，用于在训练和生成阶段之间高效地重新分配 actor 模型的分片，零内存冗余并显著减少通信开销。我们的实验结果表明，使用HybridFlow运行各种RLHF算法时，与最先进的基线相比，吞吐量提高了1.53倍至20.57倍。HybridFlow源代码将在 https://github.com/volcengine/verl 上提供。

## 关键词

分布式系统，从人类反馈中进行强化学习

## 1. 引言

大语言模型（LLM）如GPT [11]、Llama [73] 和 Claude [7] 已经彻底改变了从写作 [2]、搜索 [52] 到编码 [63] 等各种人工智能（AI）应用。LLM首先通过下一个词预测在书籍、网站等上的数万亿个标记上进行预训练，以积累广泛的知识 [11]。接下来，LLM通过监督微调（SFT）在特定领域的数据集上进行训练，以能够遵循人类指令 [11]。尽管LLM在预训练和SFT后在自然语言任务上表现出色，但训练数据集中的有害和偏见内容仍可能误导LLM生成有毒和不受欢迎的内容。从人类反馈中进行强化学习（RLHF）被引入以进一步将LLM与人类价值观对齐，用于构建有帮助且无害的AI应用 [7, 55]。

RLHF建立在传统的RL算法之上 [68, 4, 78]，例如近端策略优化（PPO）[68] 和 REINFORCE [78]。广泛采用的基于PPO的RLHF系统通常由四个LLM组成 [55, 7]：一个actor、一个critic、一个 reference policy network和一个reward model。基于PPO的RLHF在迭代中进行，每个迭代包括三个阶段：

（1）使用actor模型生成一批提示的响应；

（2）通过critic、reference policy 和reward model的单次前向传递对生成的响应进行评分，准备训练数据；

（3）通过前向和后向计算更新actor和critic，从人类偏好中学习。

其他RLHF变体 [19, 43] 遵循类似的阶段，但涉及不同数量的模型和模型之间的数据依赖关系。

传统的RL可以被建模为一个数据流[46]，这是一个有向无环图（DAG）：

- RL数据流中的每个节点代表神经网络的计算（例如，Actor 或 Critic 网络，可以是CNN或MLP）；
- 每条边表示NN计算之间的数据依赖关系（例如，Critic的输出用作Actor训练的输入[68]）。

RLHF数据流更加复杂，涉及更复杂的模型（例如，LLMs用于Actor/Critic/reference/reward model），每个模型运行不同的计算，并且它们之间的数据依赖关系更加多样化（即，分布式模型分区之间的multicast ）。在RLHF数据流中训练和生成LLM需要分布式计算（例如，使用张量/流水线/数据并行）[40, 71]。因此，RLHF数据流中的每个节点是一个复杂的分布式程序，对应于相应LLM的分布式计算。不同节点中的模型通常使用不同的并行策略，因为它们的工作负载不同。边代表数据重新分片，通常是一个多对多的 multicast 。因此，灵活和高效地表示和执行复杂且资源密集型的RLHF至关重要。

传统的RL框架，如RLLib[45]和RLLib Flow[46]，使用层次化的单控制器范式来运行RL数据流。集中式控制器将数据流中的节点分配给不同的进程，并协调它们的执行顺序。每个节点进程可以进一步生成更多的工作器来执行计算，再次遵循单控制器范式。然而，它们只提供数据并行训练的原语，并且仅限于最多几百MB大小的神经网络[45, 46]。在RLHF数据流中，每个节点对应于一个LLM，最多有数十亿个操作符，使用一些复杂的并行性进行计算。由于将操作符分派到分布式加速器的开销很大，单控制器范式效率低下[1, 9]。

现有的RLHF系统采用多控制器范式来管理内部节点计算和节点间数据重新分片[17, 30, 80]。每个控制器独立管理一个设备的计算，并使用多个点对点操作来协调不同节点之间的数据依赖关系。这种多控制器范式在执行LLM计算时引入的调度开销可以忽略不计（详细说明在§2.2）。

然而，没有中央控制，它在实现各种RLHF数据流时缺乏灵活性，因为修改单个节点以适应不同的数据依赖关系需要改变所有依赖节点的实现，阻碍了代码重用。

为了解决这些限制，我们提出了HybridFlow，一个灵活高效的RLHF框架，可以轻松表示和执行多样化的RLHF数据流，实现高吞吐量。我们的关键观察是，在节点间级别使用单控制器范式可以灵活地表达各种数据依赖关系，并轻松协调节点间数据重新分片，开销最小，同时在内部节点计算中集成多控制器范式可以显著提高计算效率。我们提倡使用层次化的混合编程模型来生成RLHF数据流。在节点级别，我们提供了多个模型类，这些模型类将数据流中不同LLM的分布式计算（训练、推理和生成）封装到原始API中。这些API可以无缝支持现有LLM框架中的各种并行策略，包括3D并行[71]、ZeRO[59]和PyTorch FSDP[57])，并在多控制器范式下执行分布式计算。在节点之间，设计了一组传输协议，通过单控制器协调，隐藏数据重新分片的复杂性。这种编程模型抽象了分布式计算的复杂性，允许用户用几行代码实现RLHF数据流，并通过单控制器的单个进程运行RLHF。它还有效地解耦了内部节点计算和节点间数据传输，允许独立优化每个模型而不改变数据流中其他模型的代码。

训练和生成Actor模型代表了RLHF数据流中的主要计算。我们进一步设计了一个3D-HybridEngine，以实现Actor模型的高效执行，引入零内存冗余，并在训练阶段和生成阶段之间显著降低模型参数重新分片过程中的通信开销。我们的混合编程模型还支持灵活地将模型放置在相同或不同的GPU设备集上。这使我们能够提供一个有效的算法来优化GPU分配和放置数据流中的每个节点（模型），以适应任何RLHF数据流。我们在设计HybridFlow方面的贡献总结如下：

- 我们提出了一个层次化的混合编程模型，方便地构建RLHF数据流。这个编程模型实现了内部节点计算的高效分布式执行和灵活的节点间数据重新分片和传输，适用于各种RLHF算法（§4）。
- 我们设计了一个3D-HybridEngine，以高计算效率执行Actor模型的训练和生成，并在训练阶段和生成阶段之间实现零冗余过渡（§5）。
- 我们设计了一个有效的映射算法，自动识别RLHF数据流中每个节点（模型）的优化GPU分配和放置（§6）。
- 我们在各种RLHF算法、模型大小和集群规模下与最先进的RLHF系统[17, 30, 82]进行了广泛的实验比较。我们的评估表明，吞吐量提高了1.53×至20.57×。我们已经开源了HybridFlow，并相信HybridFlow可以促进未来的RLHF研究和发展。

## 2. 背景和动机
### 2.1 从人类反馈中进行强化学习

![Refer to caption](https://arxiv.org/html/2409.19256v2/x3.png)

RLHF工作流程。 RLHF通过使用一组人类排序的给定提示的候选者 [7, 19, 41, 43, 55, 70, 91] 将LLM的语言空间与人类价值观对齐。一个RLHF系统通常由多个模型组成，例如actor、critic、reference policy和一个或多个reward model。actor和reference policy是预训练/微调的LLM（即正在进行RLHF的LLM）。critic 和 reward model可以是不同LLM，微调于人类偏好数据集，语言建模头被标量输出头替换 [7, 55]。RLHF工作流程可以分解为3个阶段（图1），我们以PPO为例：

- 阶段1（生成）： actor使用自回归生成从一批提示中生成响应。
- 阶段2（准备）： 使用提示和生成的响应，critic计算它们的值 [66, 68]，reference policy计算它们的reference 对数概率，reward model计算它们的奖励 [7, 55]，所有这些都通过各自模型的单次前向计算完成。
- 阶段3（学习/训练）： 通过Adam [38] 使用前一阶段生成的数据批和损失函数 [55] 更新actor和critic。

其他RLHF算法大致遵循3阶段工作流程（图1(b)(c)）。Safe-RLHF [19] 在PPO-ptx [55] 之后引入了一个辅助预训练损失，并包括一个额外的成本模型，以同时拟合人类偏好和安全标签。ReMax [43] 需要一个额外的生成通过以减少方差，并在数据流中消除了critic模型。研究人员正在积极探索新的RLHF算法 [41, 70, 91]，并将传统RL方法整合到RLHF领域 [37]。这些变体需要灵活表示RLHF数据流图，以适应不同的算法要求。

### 并行策略

LLM通过数据、pipeline 和张量并行进行训练和服务 [36, 40, 54]。在数据并行（DP）中，输入数据被分割成多个子集；每个子集由一个单独的设备（例如GPU）处理 [69]。ZeRO [59] 是一种针对DP训练的内存优化解决方案，逐步将优化器状态、梯度和模型参数分片到GPU上。pipeline并行（PP）[32, 53] 和张量并行（TP）[71] 将模型参数、梯度和优化器状态分布到多个GPU上。现代分布式训练框架如Megatron-LM [71] 和 MegaScale [36] 使用3D并行或PTD并行[54]，其中P、T、D分别代表PP、TP、DP。在3D并行中，PP大小表示模型训练中的pipeline 阶段数，TP大小指张量被分片的数量，DP大小是模型副本的数量。LLM服务系统在训练时采用类似的3D并行，而仅对模型参数和KVCache进行分片 [16, 29, 40]。

RLHF数据流中的LLM模型可能执行不同的计算，包括训练（一次前向传递、一次后向传递和模型更新）、推理（一次前向传递）和生成（自回归生成，多次前向传递）。特别是，actor模型的训练和生成、critic的训练和推理、reference policy和reward model的推理。不同的并行策略可以应用于不同的模型，以实现最佳吞吐量。

### 2.2  分布式ML的编程模型

单控制器。 它使用集中式控制器来管理分布式程序的整体执行流程。通过集中控制逻辑，用户可以将数据流的核心功能构建为单个进程（图2(b)），而控制器自动生成分布式工作线程来执行计算。由于对硬件和数据流图的全局视图，单控制器范式允许灵活且优化的资源映射和数据流任务之间的执行顺序协调。然而，协调消息从控制器传递到所有工作线程，在大规模集群上执行扩展数据流图时会产生显著的调度开销 [1, 9]。

多控制器。 每个设备（即工作线程）都有自己的控制器。最先进的分布式LLM训练和服务系统采用多控制器范式，因其可扩展性和低调度开销（控制消息主要通过快速PCIe链路从CPU传递到GPU）[36, 40, 60, 71]。如图2(a) 所示，每个模型运行一个单独的程序，所有工作线程执行相同的程序。每个工作线程只拥有系统状态的本地视图，并需要点对点通信来协调模型执行顺序。为了在多控制器架构中实现RLHF工作流程，用户必须在每个设备上运行的程序中精心集成集体通信、计算和点对点数据传输的代码。这导致计算和数据传输代码深度嵌套，难以开发、维护和优化。在图2(a)中，每个模型执行本地计算和all_gather操作（黑色代码），而Actor模型必须显式管理向Critic和reward model发送操作，后者必须相应地在它们的程序中的精确点实现接收操作。

### 2.3 RLHF特性

异构模型工作负载。 RLHF中的actor、critic、reference和reward model可能在不同阶段执行训练、推理或生成，具有不同的内存占用和计算需求。对于reference policy和reward model，只需将其模型参数存储在GPU内存中，因为它们仅执行前向传递计算。对于actor和critic，它们的模型参数、梯度和优化器状态必须存储，因为它们进行模型训练。此外，RLHF中的小actor模型（例如，7B预训练/微调LLM）可以与较大的critic和reward model（例如，70B LLM）配对，以实现更好的对齐 [7]。鉴于这种异构性，需要为每个模型运行RLHF时采用不同的并行策略和定制优化。

actor训练和生成之间的计算不平衡。 在RLHF数据流中，actor模型的训练和生成由两个节点表示（图1），通常在每个RLHF迭代中占据大部分工作负载（例如，使用HybridFlow时占总RLHF时间的58.9%）。actor训练是计算密集型的 [24]，通常需要更大的模型并行（MP）大小（即模型被分区的数量）并将工作负载分布到更多GPU上，例如，在8个GPU上对7B模型进行8个分区。使用与生成相同的并行策略（例如，相同的MP大小）可能导致GPU计算资源利用率不足，因为生成是内存密集型的 [40]。先前的研究表明，结合更大的DP大小和更小的MP大小（混合数据和模型并行），例如，将7B模型分成两个并复制四次在8个GPU上，可以提高生成吞吐量 [44, 92]。尽管为actor训练和生成使用不同的并行策略可以优化两个阶段的吞吐量，但在两个阶段之间在运行时重新分配actor模型权重可能会导致显著的通信和内存开销。例如，对齐70B actor模型需要在每个RLHF迭代中在训练和生成阶段之间传输140GB的模型权重，当两个阶段在不同设备上时，占用迭代时间的36.4% [30]。

多样化的模型放置需求。 根据模型的工作负载和数据依赖关系，战略性地放置RLHF数据流中的模型是必要的。图3给出了一个模型放置计划和相应的RLHF执行流程。放置在不同设备上的模型可以在没有数据依赖关系的情况下并行执行。放置在同一组GPU上的模型称为共置模型，共享GPU内存并按时间共享顺序执行，因为如果共置的LLM同时执行，容易发生内存不足（OOM）错误。

我们观察到一个折衷：将模型放置在不同设备上允许并行处理，但由于RLHF中的阶段模型执行，不可避免地会导致一些GPU空闲时间。在图3中，actor和critic分别放置，并行执行训练，但在其他RLHF阶段导致1/3的GPU时间空闲。支持各种模型放置策略并最大化设备利用率对于在任何模型大小和集群规模上优化RLHF性能至关重要。

### 2.4 现有RLHF系统的局限性

对各种RLHF数据流图的不灵活支持。 现有RLHF系统采用多控制器范式进行数据流实现 [17, 30, 80, 82]。为了实现各种RLHF算法，用户必须导航和管理混合了集体通信、模型计算（可能使用各种分布式训练/服务框架）和点对点数据传输的代码。这种代码结构缺乏模块化/功能封装，使RLHF系统与特定的LLM训练和服务框架紧密耦合。因此，用户需要逐案实现和优化不同的RLHF数据流 [46]，阻碍了代码重用并增加了出错的风险。现有RLHF框架仅支持PPO算法。此外，由于实现复杂性，支持的并行策略有限。例如，要在DeepSpeed-Chat [82] 中结合3D并行为LLM训练和生成，可能需要重新实现整个系统，因为代码结构混杂。

RLHF执行效率低下。 表1总结了现有RLHF系统采用的并行策略、模型放置和执行模式。DeepSpeed-Chat [82] 和 OpenRLHF [30] 采用ZeRO-3进行actor训练和TP进行actor生成。OpenRLHF在不同设备上使用actor模型的不同副本进行训练和生成，导致内存使用冗余和设备之间的频繁权重同步。DeepSpeed-Chat在同一组设备上维护actor模型的相同副本进行训练和生成，并在训练和生成之间重新分配模型权重（由于两个阶段使用的并行性不同），这可能仍会导致大型模型的显著内存和通信开销（详见§5.4）。NeMo-Aligner [17] 在actor训练和生成中使用相同的3D并行配置，生成吞吐量较低（§8.4）。

表 1：RLHF 框架的比较。图表展示了一个 PPO 迭代的执行过程。数字 1-6 分别表示响应生成、奖励模型推理、参考模型推理、critic 推理、actor 训练和 critic 训练。

| RLHF 系统      | 并行策略                              | 训练和生成中的 actor 权重                  | 模型放置                                                     | 执行模式                                                |
| -------------- | ------------------------------------- | ------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------- |
| DeepSpeed-Chat | 训练：ZeRO<br>生成：TP                | 从 ZeRO 到 TP 的模型重新分配               | 所有模型共置于同一组设备上                                   | GPU 进程<br>Actor<br>Critic<br>参考策略<br>奖励模型<br> |
| OpenRLHF       | 训练：ZeRO<br>生成：TP                | 使用两个 actor 权重副本分别用于训练和生成  | 每个模型放置在不同的设备上                                   | GPU 进程<br>Actor<br>Critic<br>参考策略<br>奖励模型<br> |
| NeMo-Aligner   | 训练和生成均使用 3D 并行              | 在两个阶段中使用相同的模型分区（共享权重） | Actor 和参考策略共置于部分 GPU 上，Critic 和奖励模型共置于其他 GPU 上 | GPU 进程<br>Actor<br>Critic<br>参考策略<br>奖励模型     |
| HybridFlow     | 训练：3D、ZeRO、FSDP<br>生成：3D 并行 | 零冗余模型重新分配                         | 支持各种模型放置策略                                         | GPU 进程Actor<br>Critic<br>参考策略<br>奖励模型         |

现有RLHF框架仅限于一种模型放置计划，因此仅限于一种RLHF执行模式，如表1所示。实现不同的放置计划很困难，需要更改模型初始化和节点间数据传输的内部逻辑，如图2中蓝色部分所示。OpenRLHF和NeMo-Aligner允许在准备和学习阶段并行执行模型计算；在生成阶段，除actor外的模型空闲，浪费它们占用的GPU。DeepSpeed-Chat将所有模型共置于同一组设备上，每个设备按RLHF数据流顺序依次运行每个模型。由于模型之间的工作负载不平衡，这种放置在资源利用率上可能效率低下（在§8.3中评估）。

### 2.5 设计考虑

为了解决现有系统的局限性，关键问题是——如何设计一个灵活且高效的编程模型来实现RLHF数据流？在节点间级别，单控制器设计特别有优势，因为它在协调数据传输、执行顺序和不同模型分布式计算之间的资源虚拟化方面具有灵活性 [9, 50]。RLHF数据流图通常只包含几个节点。从单控制器向不同节点调度控制消息的开销可以忽略不计，因为与数据流中节点（模型）所需的分布式计算相比，开销较小。多控制器范式以其向加速器调度操作的低延迟而闻名 [20]，可以用于每个模型的分布式计算。基于这些见解，我们提出了一种分层混合编程模型，用于RLHF数据流实现。我们的关键设计原则是以混合方式结合单控制器和多控制器范式。该设计确保了RLHF数据流的灵活表达和高效执行，同时在节点间和节点内级别保持低控制开销。如图2(b) 所示，该范式解耦了节点内分布式计算和节点间数据传输，允许每个模型专注于本地计算，而无需管理节点间通信。

## 3. HybridFlow概述

![Refer to caption](https://arxiv.org/html/2409.19256v2/x8.png)



图4展示了HybridFlow的架构，主要包括三个组件：混合编程模型、3D-HybridEngine和自动映射算法。混合编程模型包括一组分层API，以实现RLHF数据流的灵活表示和模型的高效计算（§4）。3D-HybridEngine特别设计用于actor模型训练和生成的高效执行，允许在训练和生成阶段之间进行零内存冗余和最小化通信开销的模型参数重新分配（§5）。自动映射算法确定每个模型在RLHF数据流中的优化设备放置，以最大化RLHF的吞吐量（§6）。

我们的RLHF系统的工作流程如下。用户提供以下输入以启动RLHF系统：

（i）模型规格，包括RLHF数据流中actor/critic/reference policy/reward model的架构和大小；

（ii）模型在数据流中的设备放置，由在给定GPU集群配置下运行的自动映射算法获得；

（iii）每个模型在每个阶段运行的并行策略，例如3D并行的（p, t, d）元组，其中p、t、d分别代表PP大小、TP大小和DP大小。

单控制器程序接收这些输入，初始化RLHF数据流中的模型和虚拟化资源池，根据放置计划将操作/模型调度到设备，并调用设备上多个控制器运行的函数来执行每个模型的分布式计算。

多控制器程序实现了ParallelWorker类：它根据每个模型的并行策略在分配的设备之间构建每个模型的并行组，调用3D-HybridEngine进行actor的训练和生成，并可以与现有的LLM引擎 [40, 57, 60, 71] 无缝集成，用于其他模型的训练、推理和生成。传输协议由单控制器程序协调，以支持具有不同并行策略的模型之间的数据（包括提示、响应和其他模型输出）重新分配。actor在训练和生成之间的数据重新分配由3D-HybridEngine处理。

## 4. 混合编程模型

### 4.1 分层API

![Refer to caption](https://arxiv.org/html/2409.19256v2/x9.png)

节点内：封装分布式程序。 对于不同RLHF阶段中每个模型的分布式计算，我们提供了一个基类3DParallelWorker。给定分配的设备，它促进分布式模型权重的初始化，并为每个模型建立3D并行组。一个并行组包括一组GPU，用于托管模型的特定并行维度，例如TP中的不同张量分片和DP中的不同模型副本。图5(a) 展示了使用我们的API初始化actor模型的过程，其他模型的初始化类似。

继承自3DParallelWorker类，提供了几个模型类，分别用于actor、critic、reference和reward model。每个模型类封装了API，以实现模型的分布式前向和后向计算、自回归生成和优化器更新，将分布式计算代码与与其他模型的数据依赖关系解耦。这些API可以通过重用现有LLM系统的计算脚本来轻松实现。例如，ActorWorker（actor模型的类）中的update_actor函数的计算类似于Megatron-LM [71] 中的预训练脚本。模型类封装了实现各种RLHF算法的基本操作，例如actor模型类中的generate_sequences用于根据提示生成响应，reward model类中的compute_reward用于通过前向传递评估响应。（更多API详见附录A）。

除了实现3D并行的3DParallelWorker基类外，我们还提供了PyTorch FSDP（FSDPWorker）和ZeRO（ZeROMorker）的基类，以及继承每个基类的相应模型类，以支持模型计算中的不同并行策略。图4中的ParallelWorker表示这些基类之一。

节点间：统一模型间数据重新分配的实现。 在不同设备上采用不同并行策略的模型之间的数据传输涉及多对多multicast 。我们通过将每个模型类中的每个操作与传输协议关联来统一此数据传输实现，使用@register。每个传输协议包括一个collect函数和一个distribute函数，根据每个模型的并行策略聚合输出数据并分发输入数据。在图5(a) 的示例中，update_actor操作注册到传输协议3D_PROTO，因为actor训练使用3D并行。在3D_PROTO中，collect函数在每个DP组中聚合相应模型函数（例如，update_actor返回的损失标量）的所有输出数据到单控制器，distribute函数将输入数据分发到注册的函数（例如，update_actor的优势）到每个DP组。使用源模型的输出collect函数和目标模型的输入distribute函数启用数据重新分配。图5(b) 展示了actor（生成）和critic（推理）之间的数据重新分配，其中模型的计算采用不同的3D并行策略。单控制器使用actor的3D_PROTO中的collect函数收集数据未来（步骤1-3），并将其发送到critic（步骤4）；critic使用其3D_PROTO中的distribute函数将接收到的数据未来分发到每个DP组（步骤5）。然后，从actor到critic获取远程数据，每个critic的GPU仅根据其DP排名获取actor输出数据的所需本地批次（步骤6）。实际数据传输仅在GPU之间发生，避免了任何中心瓶颈。

我们提供了8个传输协议，包括3D_PROTO、DP_PROTO、ONE_TO_ALL等，涵盖了大多数数据重新分配场景（详见附录B）。用户可以通过实现自定义的collect和distribute函数进一步扩展传输协议。

促进灵活的模型放置。 我们提供了一个ResourcePool类，用于虚拟化一组GPU设备。当将ResourcePool实例应用于模型类时（图5(a)），模型的分布式计算将映射到这些设备。使用相同ResourcePool实例的模型共置于同一组GPU上；当在它们的模型类中应用不同的Resource Pool实例时，模型放置在不同的GPU组上。我们假设不同的ResourcePool实例之间没有重叠。

异步数据流执行。 当模型放置在不同的设备组上时，一旦它们的输入可用，它们的执行就会自动触发 [50]。在图5(b) 中，actor的数据未来在控制器的调用后立即返回（步骤1-3）；控制器随后启动对critic的新调用，并根据传输协议分发未来（步骤4-5）。当一些模型放置在同一组设备上时，它们根据调用顺序依次执行。通过我们的编程模型，HybridFlow无需更改RLHF算法的代码即可灵活支持各种分布式执行模式（图6）。

![Refer to caption](https://arxiv.org/html/2409.19256v2/x10.png)

### 4.2 不同RLHF算法的实现

我们的API实现了各种RLHF算法的简化开发（数据流）。用户可以通过调用模型操作的原始API调用，将分布式计算的模型实现为单个进程程序，在单控制器上运行。PPO、ReMax和Safe-RLHF的示例如图6所示。PPO可以通过调用包括compute_values和generate_sequences在内的模型操作在仅8行代码中实现，这些操作在多个GPU上以多控制器范式执行。为了适应Safe-RLHF，它集成了一个额外的成本模型来评估安全偏好和actor的预训练损失，只需在PPO实现的基础上添加5行代码。为了适应ReMax，需要一个额外的actor生成调用，并且可以删除与critic相关的代码。

实现灵活性。 这种扩展的灵活性对于研究人员探索不同的RLHF算法至关重要：他们可以重用每个模型类中封装的分布式计算，并根据特定算法调整数值计算的代码，例如GAE [67] 和actor和critic的损失函数中的KL散度。简化的开发可以归因于混合编程模型。我们的模块化API设计简化了开发，促进了广泛的代码重用，并能够直接集成现有LLM训练/服务框架的代码库。它还解耦了模型计算和模型之间的数据传输。任何分布式框架的更改都不会影响RLHF算法的代码（图6），从而实现每个模型执行的个性化优化（§5）。支持具有不同工作负载的模型的灵活放置，能够将RLHF数据流优化映射到各种设备上（§6）。

## 5. 3D-HybridEngine
我们设计了3D-HybridEngine，以支持actor模型训练和生成的高效执行，目标是显著提高RLHF的吞吐量。

### 5.1并行组
为了消除actor模型副本的冗余，我们建议在分配给actor的同一组设备 $N_{a}$ GPU上部署actor的训练和生成阶段，并按顺序执行同一组actor模型权重。然而，actor的训练和生成可能采用不同的3D并行策略，即生成阶段通常需要较小的TP和PP大小，但较大的DP大小（§2.3）。3D-HybridEngine在此背景下，在同一组设备上高效地在actor训练和生成阶段之间重新分配模型参数。

设 $p$-$t$-$d$ 表示为actor训练构建的3D并行组，对应于托管 $p$ pipeline阶段、$t$ 张量分片和 $d$ 模型副本的GPU组 [54]。3D-HybridEngine根据不同的3D并行策略分别为actor训练和生成构建不同的并行组。我们使用 $p_{g}$、$t_{g}$ 和 $d_{g}$ 表示生成阶段中的生成pipeline并行组大小、生成张量并行组大小和微数据并行组大小。$d_{g}$ 表示生成中的模型副本数量与训练中的模型副本数量的比率，即训练中的每个DP副本变为 $d_{g}$ 微DP副本，以处理 $d_{g}$ 微批次的提示和响应。我们有 $N_{a}$=$p$$\times$$t$$\times$$d$=$p_{g}$$\times$$t_{g}$$\times$$d_{g}$$\times$$d$，使得 $d_{g}=\frac{p t}{p_{g}t_{g}}$。微DP组仅在actor生成阶段使用，以实现更大的DP大小以充分利用设备。生成并行组表示为 $p_{g}$-$t_{g}$-$d_{g}$-$d$。

### 5.2 3D-HybridEngine工作流程

![Refer to caption](https://arxiv.org/html/2409.19256v2/x11.png)

在RLHF的迭代 $i$ 中的actor训练和迭代 $i+1$ 中的actor生成之间，需要根据两个阶段的并行组配置重新分配actor模型参数和提示数据。在RLHF的迭代 $i+1$ 中，3D-HybridEngine收集在迭代 $i$ 中更新的actor模型参数（图7中的步骤1），以便在每个微DP组内进行生成。然后，将一批提示加载到每个模型副本中（步骤2），生成响应（RLHF的生成阶段）。随后，3D-HybridEngine在每个微DP组内对生成结果执行all-gather操作（步骤3），并根据actor训练的3D并行性重新分配模型参数（步骤4）。在正确重新分配模型权重、提示和响应后，计算actor模型的损失并根据RLHF算法更新actor模型权重（步骤5）——迭代 $i+1$ 的actor训练阶段。

### 5.3 零冗余模型重新分配

![Refer to caption](https://arxiv.org/html/2409.19256v2/x12.png)

3D并行中的并行分组方法通常如下：PP和TP组通过将连续的排名分配给pipeline阶段和张量分片来形成；DP组通过选择由PP大小和TP大小的乘积决定的常规间隔的排名来构建。在图8(a) 中，actor训练使用3D并行组1-4-2：所有GPU有一个PP组（为了说明清晰）；TP组为 [G1, G2, G3, G4]、[G5, G6, G7, G8]，DP组为 [G1, G5]、[G2, G6]、[G3, G7]、[G4, G8]。假设使用相同的并行分组方法但具有不同的并行大小，例如生成中的1-2-2-2（图8(a)）。在从训练到生成的过渡期间，3D-HybridEngine在模型并行组之间应用all-gather操作以聚合所有参数，然后根据设备所属的并行组在每个设备上仅保留模型权重的一个子集。在某些GPU上（例如G2、G3、G6、G7），训练和生成模型权重之间没有重叠，需要单独的内存来维护后续训练的权重（图8(a) 中的灰色框）。我们称使用上述普通并行分组方法的系统为HybridFlow-V。

我们进一步为3D-HybridEngine设计了一种新的并行分组方法，用于生成阶段，消除了权重存储的冗余，并由于训练和生成阶段之间的actor模型重新分配而导致的内存占用和通信最小化。具体来说，我们通过选择由 $\frac{t}{t_{g}}$ 和 $\frac{p}{p_{g}}$ 决定的常规间隔的排名来形成生成TP和PP组，并通过沿生成TP或PP维度顺序分配排名来构建微DP组。在图8(b) 中，生成使用1-2-2-2并行组：生成TP组为 [G1, G3]、[G2, G4]、[G5, G7]、[G6, G8]；微DP组为 [G1, G2]、[G3, G4]、[G5, G6]、[G7, G8]。这种生成并行组的战略性重新排列导致每个设备上的训练和生成模型权重重叠，允许在生成期间重用训练权重，并在设备内存使用中实现零冗余。此外，3D-HybridEngine在每个微DP组内并发执行多个all-gather操作，显著减少了通信开销。

### 5.4 过渡开销

在表2中，我们比较了不同actor引擎设计在训练和生成阶段之间过渡期间的通信开销和内存占用。我们假设actor的模型大小为 $M$，$N_{a}$ GPU用于其训练和生成。DeepSpeedChat的actor引擎在过渡期间在所有GPU上执行all-gather操作；HybridFlow-V在训练TP和PP组内执行此all-gather操作。这些操作的通信量分别为 $\frac{N_{a}-1}{N_{a}}M=\frac{tpd-1}{tpd}M$（DeepSpeedChat）和 $\frac{tp-1}{tp}M$（HybridFlow-V），计算依据 [13]。这两个引擎在每个GPU的内存中聚合所有模型参数，然后根据生成并行组重新分配模型状态，导致模型参数的峰值内存使用为 $M$。由于它们在某些GPU上无法在生成期间重用训练权重，因此需要在这些GPU上维护训练权重，导致冗余内存消耗分别为 $\frac{1}{tpd}M$ 和 $\frac{1}{tp}M$。

通过我们为生成阶段设计的并行分组方法，HybridFlow将all-gather操作限制在每个微DP组内。通信开销减少到 $\frac{d_{g}-1}{tp}M=\frac{tp-t_{g}\theta_{g}}{t_{g}p_{g}t_{P}}M$。每个GPU只需在其微DP组内收集远程参数，并可以在生成期间重用训练权重。因此，HybridFlow中模型参数的峰值内存使用精确匹配生成中每个GPU上的模型分区大小，消除了GPU内存使用中的任何冗余。

## 6. 自动设备映射
我们的混合编程模型要求用户输入以下配置，这些配置称为RLHF数据流到给定设备的映射：（a）数据流中模型的设备放置；（b）每个模型在每个阶段运行的相应并行策略。

我们提供了一个高效的算法（算法1），用于用户在给定设备集群上识别执行RLHF数据流的优化映射，最小化每个RLHF迭代的端到端延迟。给定一个数据流 $D$，我们首先探索给定集群中模型的所有可能放置计划 $\mathcal{P}$（第3行）。例如，PPO算法涉及四个模型，导致15种可能的放置（来自贝尔分区问题 [10, 62]），从完全独立的放置（所有模型放置在不同设备上，例如OpenRLHF的放置）到将所有模型共置于同一组设备上（例如DeepSpeed-Chat的放置）。我们将共置于同一组GPU上的模型称为共置集。共置集中的模型可以在同一组GPU上采用不同的并行策略。我们根据共置模型的内存消耗，识别分配给每个共置模型集的最小GPU数量 $A_{min}$，确保不会发生内存不足错误（第9行）。

接下来，从 $A_{min}$ 中的最小GPU分配开始，我们枚举分配给每个共置模型集的所有可行设备分配（第10-12行）。给定共置集的设备分配 $A$ 和模型集的计算工作负载 $W$，我们在auto_parallel模块中探索每个模型的优化并行策略，最小化模型执行延迟。工作负载 $W$ 包括输入和输出形状以及每个模型的计算（训练、推理或生成）。在auto_parallel中，我们利用模拟器模块sinu来估计不同并行策略的延迟，依据先前的研究 [42, 84, 90, 92]（附录C中的概述）。

d_cost模块根据给定的模型放置和并行策略，通过迭代数据流图中的所有阶段并累加所有阶段的延迟，估计RLHF数据流的端到端延迟（第17、25行）。对于同一共置集中的模型并在同一阶段涉及计算（例如，actor和critic在RLHF训练阶段都执行模型更新），它们的执行延迟相加（第32行）。对于不同共置集中的模型，它们在同一阶段的执行可以并行化，阶段的延迟由不同集中的最大执行时间决定（第33行）。我们识别具有相应并行策略的最佳模型放置，实现每个RLHF迭代的最小执行时间（第18-23行）。

算法1的复杂度为 $O(\frac{(N-1)!}{(k-1)!(N-k)!})$，其中 $k$ 是数据流中的模型数量，$N$ 是运行数据流的总设备数量。这是枚举独立放置策略的所有可能设备分配的最坏情况复杂度（即独立放置），通过将 $N$ 个设备分配给 $k$ 个模型（称为整数分区问题 [6]）。为了提高效率，我们缓存每个模型在一定数量设备 $A$ 上的并行策略，以消除在不同放置策略中将模型放置在不同 $A$ GPU集上时的重复搜索。

尽管我们在运行自动映射算法时假设 $N$ 个同构GPU，但算法1可以轻松扩展以优化异构设备上的模型映射，通过在simu和auto_parallel模块中考虑异构设备 [88]。

## 7. 实现
HybridFlow的实现约为12k行Python代码（LoC）。

混合编程模型。 分层API的实现使用了1.8k LoC。集中式单控制器构建在Ray [50] 之上，并使用远程过程调用（RPC）来协调不同模型的执行顺序，并根据数据流传输数据。这些中间数据存储在TensorDict [57] 中。在我们的多控制器范式中，每个模型函数在各种设备上的单独进程中运行，控制消息从每个控制器的CPU进程传递到相应的GPU。我们的实现支持Megatron-LM、PyTorch FSDP和DeepSpeed作为LLM训练和推理引擎，以及vLLM用于自回归生成。在vLLM中，我们用分布式管理器替换了集中式KVCache管理器，以适应多控制器范式。

3D-HybridEngine。 其主要逻辑在Megatron-LM和vLLM之上实现了2.4k LoC。我们在训练和生成阶段分别存储actor模型权重，在训练期间将生成权重卸载到CPU内存，在过渡期间将生成权重重新加载回GPU内存，并在生成期间使用两个缓冲区。我们使用NCCL通信原语 [35] 在训练和生成阶段之间的过渡期间在每个微DP组内收集和连接模型参数。我们在生成后将KVCache卸载到CPU内存，并在下一个迭代中将其重新加载回GPU。

自动映射算法 使用1.9k LoC实现，包括三个用于训练、推理和生成工作负载的模拟器。该算法在CPU上运行，在RLHF数据流启动前生成设备映射和并行策略以进行数据流初始化。

## 8. 评估
### 8.1 实验设置

测试平台。 我们在16台机器（128个GPU）的集群上部署了HybridFlow。每台机器配备8个NVIDIA A100-80GB GPU，通过600GB/s NVLink互连。机器间带宽为200Gbps。我们的实验使用以下软件版本：CUDA12.1、PyTorch 2.1.2、Megatron-core 0.6.0、NCCL 2.18.1 和 vLLM 0.3.1。

模型和RLHF算法。 我们运行了PPO [68]、ReMax [43] 和 Safe-RLHF [19] 算法的RLHF数据流（图1）。PPO是最流行的RLHF算法之一 [7, 55]，由actor、critic、reference policy和reward model组成。每个模型是Llama [73] 模型，大小从7B到70B不等。Safe-RLHF有一个额外的成本模型，其架构和大小与reward model相同，ReMax消除了critic模型。我们为actor和critic训练使用混合精度，即BF16用于模型参数，FP32用于梯度和优化器状态，所有实验中使用Adam [38] 优化器。BF16用于模型推理和自回归生成。除非另有说明，实验结果来自PPO。

基线。 我们将HybridFlow与最先进的RLHF系统进行比较，包括DeepSpeed-Chat [82] v0.14.0、OpenRLHF [30] v0.2.5 和 NeMo-Aligner [17] v0.2.0（详见表1）。NeMo-Aligner不支持ReMax算法。我们不将HybridFlow与其他框架（如Trlx [27]、HuggingFaceDDP [79] 和 Collosal-Chat [15]）进行比较，因为它们不太具有代表性且比上述基线慢（如 [82] 中所报告）。

我们使用RLHF吞吐量（tokens/sec）作为性能指标，通过将全局批次中的提示和响应的总标记数除以一个RLHF迭代时间来计算。所有报告的性能数据在10次预热迭代后平均超过5次训练迭代。

数据集和超参数。 我们在HuggingFace的“Dahoas/ful-hh-rihf”数据集 [7] 上进行RLHF，该数据集广泛用于LLM对齐 [64, 85]。由于基线系统可能未在生成期间结合连续批次优化 [83]，为了公平比较，我们强制生成的所有响应长度相同。在每个实验中，输入提示长度和输出响应长度均为1024，输入提示的全局批次大小为1024。PPO的epoch数为1，每个epoch的PPO更新迭代次数为8，与之前的RLHF研究 [31, 55, 81] 一致。

### 8.2 端到端性能

图9、10和11分别显示了运行PPO、ReMax和Safe-RLHF时的RLHF吞吐量。本组实验中的actor、critic、reference和reward model大小相同，遵循之前的做法 [7, 55, 82]。实验中使用的GPU数量从运行RLHF而不发生OOM的最小GPU数量到128个GPU不等。我们在实验中未启用优化器状态卸载 [61]，以进行公平比较。

总体性能。 我们观察到，HybridFlow在所有模型规模上始终优于基线。在图9中，HybridFlow在PPO中分别比DeepSpeed-Chat、OpenRLHF和NeMo-Aligner高出3.67倍（最高7.84倍）、3.25倍（最高5.93倍）和12.52倍（最高20.57倍）。这主要是因为HybridFlow通过使用不同的并行策略对模型进行分片，有效地执行了所有RLHF阶段的生成、推理和训练。HybridFlow在训练70B模型时实现了最高的平均加速比9.64倍，因为HybridFlow与DeepSpeed-Chat和OpenRLHF相比，分别减少了71.2%和89.1%的过渡开销，并且在使用ZeRO-3进行训练时也产生了大量的机器间通信。由于生成引擎中缺乏KVCache，NeMo-Aligner的主要性能瓶颈在于生成阶段，占其RLHF迭代时间的81.2%。类似的结果可以在图10和11中观察到，验证了HybridFlow在运行各种RLHF算法时的效率。

可扩展性。 HybridFlow在8个GPU上实现了至少2.09倍的加速。随着GPU数量的增加，HybridFlow在各种模型规模上的强扩展效率为66.8%，计算方法为将最大规模下的吞吐量除以最小规模下的吞吐量，平均超过三个算法和所有模型规模。在固定全局批次大小的情况下扩展到大量GPU会导致每个工作线程的本地批次大小变小，可能导致GPU利用率不足。在128个GPU上运行7B模型时，HybridFlow仍然比最佳基线OpenRLHF高出1.68倍、1.53倍和1.71倍，分别在PPO、ReMax和Safe-RLHF上。这可以归因于HybridFlow能够为不同模型和集群规模适应最佳放置策略，以最小化RLHF时间。OpenRLHF在较大的GPU集群上表现更好，但在较小的集群上效率较低。

### 8.3 模型放置

在本实验中，我们在HybridFlow中实现了PPO算法的各种模型放置，模型和集群设置与§8.2中相同：（i）共置，DeepSpeed-Chat的放置策略；（ii）独立，OpenRLHF的放置策略；（iii）分割，NeMo-Aligner的共置放置（actor和reference policy在同一组设备上，critic和reward model在另一组）；（iv）hybridflow，由算法1获得的优化放置。

不同模型放置的比较。 图12显示了在不同GPU数量下HybridFlow的优化放置策略。从16到64个GPU，将所有模型共置于同一组设备上性能最佳。对于96到128个GPU的34B模型和96个GPU的13B模型，分割策略成为最佳策略。分割策略将GPU均匀分配给两组模型，因为它们的大小相等。对于128个GPU的13B模型，独立策略实现了最高的吞吐量。在这种情况下，HybridFlow为actor分配64个GPU，critic分配32个GPU，reference和reward model各分配16个GPU。在较小的集群中，所有模型的计算可以充分利用GPU资源；共置策略确保在不同RLHF阶段中最大限度地利用GPU。在较大的集群中，共置放置下的RLHF吞吐量未能线性扩展，因为批次大小是固定的，且随着更多GPU上的DP大小增加，计算与通信的比率降低。独立和分割策略将模型放置在不同设备上，每个模型在较大集群中的DP大小较小，促进了同一阶段中不同模型的并行执行。在所有情况下，我们的算法1生成了最佳放置，具有最高的训练吞吐量。

较大的critic和reward model。 我们进一步评估了运行PPO时，当actor和reference policy为13B，critic和reward model为70B时的模型放置（较大的critic和reward model预计会产生更好的对齐 [7]）。图13显示，在最多64个GPU的情况下，共置策略平均高出44.8%，最高高出64个GPU。分割策略在96个GPU时实现了更高的吞吐量。当扩展到128个GPU时，算法1获得的最佳放置将actor、reference和reward model共置于64个GPU上，而将剩余的64个GPU分配给critic。在相同数量的GPU上，actor和reference policy的计算时间远小于critic和reward model，将reward model与actor和reference policy共置减少了经验准备阶段的GPU空闲时间。一般来说，在训练阶段将actor和critic分布在不同设备上并行执行，在大集群中可以实现更高的吞吐量。

### 8.4 3D-HybridEngine

过渡时间比较。 图14显示了在不同模型规模下，actor训练和生成阶段之间的过渡时间，即从训练到生成重新分配模型权重的时间，设置与§8.2中相同。OpenRLHF的过渡时间包括不同设备上两个actor模型副本之间的权重同步时间。HybridFlow平均减少了55.2%（11.7秒）的过渡时间，并在70B模型上最高减少了89.1%（78.2秒）的过渡开销，同时在不同集群规模上保持一致的开销。这归因于我们为生成阶段设计的新并行分组方法（§5.4）。在基线方法中，所有模型参数必须在过渡期间收集，需要多次逐层收集以防止OOM。HybridFlow在过渡期间实现零内存冗余，并且每个微DP组仅需一次all-gather操作。

过渡和生成时间。 我们进一步验证了在HybridFlow中为actor训练和生成使用不同并行大小的必要性。在本实验中，所有模型共置于同一组GPU上，生成的KVCache使用剩余的GPU内存进行分配（即尽力分配）。图15给出了在16个GPU上运行RLHF时，7B和13B模型的过渡和生成时间，训练并行组为1-8-2（遵循p-t-d约定），生成TP组大小 $t_{g}$ 从1到8变化。生成PP组大小保持不变为 $p_{g}$=1，微DP组大小 $d_{g}$ 计算为 $\frac{5}{t_{g}}$。我们观察到，为7B模型应用较小的生成TP组大小 $t_{g}$=2，为13B模型应用 $t_{g}$=4，分别减少了60.3%和36.4%的生成延迟。相反，使用与训练相同的TP大小（$t_{g}$=8），遵循NeMo-Aligner的方法，由于GPU利用率不足，导致最大的生成延迟。进一步减小 $t_{g}$ 未能实现更高的加速，因为较小的 $t_{g}$ 需要维护每个GPU上更大的KVCache。

### 8.5 算法运行时间

图16显示了算法1的运行时间，显著短于实际RLHF训练的数天时间。运行时间呈线性增长，显示出设备映射算法在模型大小和集群规模上的良好可扩展性。大部分运行时间用于估计每个模型的并行策略的执行延迟。更大的模型有更多的并行策略可供选择，需要更多的模拟来识别每个放置计划的最佳策略。我们对每个模型在一定数量设备上的最佳并行策略进行缓存，以消除在不同放置策略中重复搜索最佳放置的时间，最多减少到半小时。

## 9. 讨论
容错性。 HybridFlow与现有的容错方法 [22, 34, 49, 76, 93] 正交，并且已经集成了检查点机制。故障可以通过NCCL错误检测，静默数据损坏通过校验和检测。我们的编程模型允许单控制器通过RPC协调检查点操作，允许在每个ParallelWorker组内保存模型状态。这包括保存actor/critic模型的参数、数据加载器ID和随机数生成器（RNG）状态，以确保系统范围内的一致性。此外，HybridFlow还可以采用基于冗余的容错方法，例如广播参数和CPU检查点，以便在有足够健康模型副本的情况下快速恢复 [76, 93]。

**放置洞察。** 我们总结了RLHF训练中模型放置和GPU分配的三个主要洞察：
1. **分配更多GPU给actor模型可以减少耗时的生成延迟**，因为生成延迟无法与其他模型并行化。
2. **当每个模型的计算能够充分利用GPU资源时，将所有模型共置在同一组设备上是最有效的**，尤其是在相对较小规模的集群上进行训练时。
3. **当扩展到大规模集群时（即强扩展），将actor和critic模型分布在不同设备上，在训练和准备阶段并行执行，可以实现更高的吞吐量**。

**资源复用。** HybridFlow通过时间共享支持共置模型在共享设备上的计算。最近在DNN任务调度方面的研究开发了细粒度的资源复用技术，主要目标是实现单个任务的服务级别目标（Berger et al., 2017; Barghani et al., 2018; Krizhevsky et al., 2019; Zhang et al., 2020; Li et al., 2021; Wang et al., 2022）。尽管ResourcePool实现支持共置模型的并行执行，但HybridFlow通常遵循顺序执行，以防止GPU资源争用或OOM问题，如第2.3节所述。在RLHF训练中应用GPU共享和异构资源带来了独特的挑战，因为它需要在平衡计算工作负载的同时管理复杂的数据依赖关系。研究细粒度的自动映射算法以实现RLHF训练中的GPU共享，结合模型卸载优化和异构设备的集成，将是未来研究的一个有前景的方向。

**从对齐到推理。** 在LLM对齐的RLHF中，奖励信号由奖励模型生成。除了对齐任务外，类似的算法（例如PPO和GRPO (Zhang et al., 2020)）还可以应用于其他领域，如代码生成和数学推理。对于这些任务，每个提示可能存在一个基本事实，可以通过评估每个代码测试用例的输出值的正确性或验证数学结果的准确性来确定。因此，奖励模型可以被非神经网络奖励模块（如沙盒环境 (Wang et al., 2022) 用于评估生成的代码或奖励函数 (Li et al., 2021; Wang et al., 2022) 用于验证数学结果）所取代。HybridFlow可以通过将这些奖励模块包装为远程函数并在单进程脚本中协调其执行，无缝集成这些奖励模块，提供一个灵活且高效的框架，适用于多样化的强化学习应用。

## 10. 相关工作
**RL框架。** 已有大量用于RL的框架，从为小规模DNN设计的一般用途RL系统（Liu et al., 2021; Zhang et al., 2020; Li et al., 2021; Wang et al., 2022; Li et al., 2021; Li et al., 2021）到专门为LLM优化的RLHF系统（Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021）。我们在§2中详细讨论了相关工作，并在本节中讨论更多RL框架。这些RL框架（Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021）与最近的RLHF系统类似，使用多控制器框架来实现其算法。它们建立了多个长期运行的分布式程序，每个组件通过硬编码的数据同步来协调执行顺序。Gear (Gear, 2006) 进一步优化了RL管道的经验回放缓冲区。然而，所有这些框架都无法支持LLM训练、推理和生成。

**LLM训练和服务系统。** TorchDDP (Torch, 1995) 和 Horovod (Horovod, 1994) 支持数据并行训练。ByteScheduler (Hinton et al., 2015) 和 DeepSpeed (Hinton et al., 2015) 通过通信和内存优化扩展了数据并行性。许多系统（Huang et al., 2019; Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021）通过张量并行和管道并行等模型并行性优化了大模型训练，将模型分布到多个设备上。LLM服务系统（Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021; Li et al., 2021）也采用数据和模型并行性来加速自回归生成，并进行了专门的优化，如连续批次处理 (Hinton et al., 2015) 和分块预填充 (Li et al., 2021)。请注意，上述所有框架都采用多控制器范式以实现高效的计算。

**数据流系统。** 数据流系统如MapReduce (Huang et al., 2019)、Spark (Hinton et al., 2015)、Dryad (Hinton et al., 2015) 和 Naiad (Hinton et al., 2015) 在分析和ML工作负载中很流行，但它们缺乏对动态任务图的支持。Ray (Ryu et al., 2019) 在单个动态任务图中统一了任务并行和actor编程模型，并实现了可扩展的分布式调度器和全局控制存储，被许多RL框架（Li et al., 2021; Li et al., 2021）采用。Pathways (Papinenko et al., 2019) 是一个为TPU设计的闭源项目，旨在轻松表达复杂的并行模式和单个DNN模型内的细粒度控制流，如管道并行和稀疏计算的专家混合模型。它采用异步分布式数据流设计，尽管存在数据依赖关系，但仍能实现并行控制平面执行，减少了单控制器范式的调度开销。其主要关注点在于单模型训练，需要对DNN模型的每个子网络进行复杂的编译。HybridFlow可以将Pathways作为子模块集成，以实现RLHF数据流中模型的计算。

## 11. 结论
HybridFlow是一个RLHF框架，能够灵活表示和高效执行多样化的RLHF算法。我们提出了一种混合编程模型，允许用户通过将不同LLM的分布式计算封装为原始API，并在节点间隐藏数据重新分配的复杂性，轻松构建RLHF数据流。我们的3D-HybridEngine确保了actor模型训练和生成的高效执行，在训练和生成阶段之间实现了零内存冗余和显著减少的通信开销。此外，我们的有效映射算法优化了RLHF数据流中模型的GPU分配和放置。广泛的实验表明，HybridFlow在各种模型大小和集群规模下，与最先进的RLHF系统相比，实现了1.53倍至20.57倍的加速。

## 附录 A：HybridFlow 中的原始 API
在 HybridFlow 中，我们通过继承 3DParallelWorker、FSDPWorker 和 ZeROMorker 实现了每个模型在 RLHF 训练中的原始操作。这些模型类的函数设计用于解耦分布式计算代码，并为用户提供 RLHF 中的基本操作。这种原始设计与现有分布式推理和训练框架中的自回归生成、前向传递、后向传递和模型更新操作兼容。用户可以根据算法的设计，通过调整提供的函数中的数值计算，轻松定制 RLHF 训练数据流，并从重用底层分布式计算实现中受益。我们在表 4 中说明了这些 API 的含义和实际计算。

## 附录 B：传输协议
我们实现了传输协议，涵盖了 RLHF 数据流中模型之间数据重新分配的所有常见用例。用户可以利用这些预定义的协议生成任何 RLHF 数据流。此外，用户可以通过实现 collect 函数和 distribute 函数轻松定义自己的传输协议。传输协议解耦了复杂的数据重新分配和分布式训练。我们将 $p$、$t$、$d$ 分别表示为管道、张量和数据并行组中的工作线程排名。我们在表 3 中说明了这些预定义的协议。

## 附录 C：自动并行算法
算法 2 概述了每个模型最优并行策略的搜索过程。从每个模型的最小模型并行大小开始（以防止在多个工作线程重新分配时发生 OOM），我们根据 GPU 数量和每台机器的 GPU 数量 $U$ 枚举所有可行的并行配置。默认的 $U$ 设置为 8。我们使用 simu 模块根据每个模型的工作负载估计其延迟。该模块包括三个模拟器，分别用于训练、推理和生成工作负载，所有模拟器都是基于先前研究 [42, 84, 92] 的分析模型。训练和推理工作负载是计算密集型的，而生成工作负载是内存密集型的。对于 actor 模型，我们首先找到训练阶段的并行策略，并记录训练阶段的内存使用情况。在 actor 生成期间，使用批次大小和最大序列长度计算 KVCache 需求。如果生成阶段的模型并行大小无法同时容纳参数和 KVCache，我们增加它。然后，我们通过比较延迟估计，寻找具有相应 KVCache 分配的最优策略。开发一个全面的自回归生成模拟器，考虑可变 KVCache 大小，可以进一步增强 RLHF 研究中的自动映射过程。

### 表 3：HybridFlow 中的传输协议
| 传输协议        | 分发函数                                               | 收集函数                                                     | 用例                                                         |
| --------------- | ------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ONE_TO_ALL      | 将数据广播到所有排名。                                 | 从所有排名收集数据。                                         | 所有工作线程方法具有相同的输入并运行相同的代码，例如模型初始化。 |
| 3D_PROTO        | 将数据拆分，分散到所有 DP 排名并在组内广播。           | 从所有 DP 组中的 $p\sim i$、$t\sim\theta$ 工作线程收集并连接数据。 | 模型在每个数据并行组内的多个工作线程之间共享。模型的输出仅存在于最后一个管道阶段，并在数据并行组之间复制。这是 Megatron-LM、DeepSpeed 等 3D 并行训练中的典型场景。 |
| 3D_ALL_MICRO_DP | 按微 DP 大小拆分数据，分散到所有微 DP 组并在组内广播。 | 从所有微 DP 组中的 local\_rank=0 工作线程收集并连接数据。    | 与 HybridEngine 一起使用。用于处理策略模型的 3D 并行方案，在训练和推理之间切换时使用。 |
| 3D_PP_ONLY      | 将数据广播到所有排名。                                 | 从所有 PP 组中的 $t\sim\theta$、$d\sim\theta$ 工作线程收集并连接数据。 | 用于检查权重名称，因为它们在 TP 和 DP 组中是相同的。         |
| DP_PROTO        | 将数据拆分为批次并分散到所有 DP 排名。                 | 从所有 DP 排名收集并连接数据。                               | 以数据并行模式训练模型。                                     |
| ALL_TO_ALL      | 无操作。                                               | 从所有排名收集数据。                                         | 用于调试。用户可以手动定义每个工作线程的输入并分别检查其输出。 |

### 表 4：每个模型类中提供的关键函数。用户可以使用这些提供的函数在几行代码中构建各种 RLHF 算法。

| 模型             | API                     | 计算                         | 解释                                                         |
| ---------------- | ----------------------- | ---------------------------- | ------------------------------------------------------------ |
| Actor            | generate\_sequence      | 自回归生成                   | 基于一批提示，actor 模型生成一批响应，并返回响应中每个标记的对数概率。 |
| Actor            | compute\_log\_prob      | 一次前向传递                 | actor 模型计算提示和响应中每个标记的对数概率。此对数概率与使用相同模型精度执行生成时返回的对数概率相同。（在 PPO 中可选） |
| Actor            | compute\_loss           | 一次前向传递                 | actor 模型基于预训练数据集 [7, 19, 55] 计算预训练损失。      |
| Actor            | update\_actor           | 一次前向、后向传递和模型更新 | 基于优势、回报（从 compute\_advantage 计算）和预训练损失，actor 模型计算训练损失并更新其权重。我们实现了各种损失函数，适用于 PPO [55]、Safe-RLHF [19]、ReMax [43]、GRPO [70] 等多样化的 RLHF 算法。 |
| Critic           | compute\_values         | 一次前向传递                 | critic 模型计算每个提示和响应的值。                          |
| Critic           | update\_critic          | 一次前向、后向传递和模型更新 | 基于值和回报，critic 计算平方误差损失以更新其权重。我们还实现了适用于 PPO [55]、Safe-RLHF [19]、ReMax [43]、GRPO [70] 等多样化的 RLHF 算法的 critic 损失函数。 |
| Reference Policy | compute\_ref\_log\_prob | 一次前向传递                 | 参考模型计算提示和响应中每个标记的参考对数概率。此对数概率用作评估 actor 模型偏离的基准，并约束其学习过程。 |
| Reward           | compute\_reward         | 一次前向传递                 | 奖励模型通过前向计算计算给定提示和响应的分数。奖励可以是标记级或样本级。 |
| -                | compute\_advantage      | 数值计算                     | 基于来自值模型和奖励模型的值和奖励，函数估计给定提示和当前策略模型响应的优势。此计算不涉及模型前向传递。 |
