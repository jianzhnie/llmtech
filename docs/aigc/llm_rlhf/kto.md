

## 摘要

Kahneman和Tversky在1992年对前景理论的开创性研究表明，人类对随机变量的感知存在系统性的扭曲；例如，人类对损失的厌恶是众所周知的。我们展示了现有的将大型语言模型（LLMs）与人类反馈对齐的方法隐含地模拟了这些扭曲，使它们成为人类感知损失函数（HALOs）。然而，这些方法赋予人类的效用函数与前理论文献中的效用函数在某些方面仍存在差异。通过弥合这一差距，我们推导出一种HALO，即Kahneman-Tversky优化（KTO），它直接最大化LLM生成的效用，而不是像当前方法那样最大化偏好的对数似然。KTO在从1B到30B的规模上匹配或超过了直接偏好优化方法的性能。而且，由于KTO不需要偏好对——只需要知道输出对于给定输入是可取的还是不可取的——它在现实世界中更容易部署，因为后一种数据要丰富得多。

## 引言

与人类反馈对齐模型已迅速成为机器学习研究中最紧迫的问题之一。然而，这一研究方向与行为经济学相关作品之间的联系尚未得到充分探索。在这份技术报告中，

1. 我们展示了对齐方法之所以有效，部分是因为它们是人类感知损失函数（HALOs）；它们赋予人类一种效用函数，该函数具有在前景理论中通过实证得出的许多特性。通过在Pythia（Biderman等人，2023年）和Llama（Touvron等人，2023年）模型家族上进行一系列实验，我们确定了哪些HALOs产生了更高性能的模型，以及改进在什么规模上出现。

2. 基于前景理论（1992年），我们推导出了一种新的HALO，称为Kahneman-Tversky优化（KTO）损失。与现有的最先进方法不同，KTO不需要成对的偏好数据（$x, y_w, y_l$）——只需要（$x, y$）以及知道$y$是否可取。KTO对齐的模型在1B到30B的规模上与DPO对齐的模型一样好或更好，尽管它没有使用成对的偏好。

3. 为了验证KTO并理解对齐如何在不同模型大小上扩展，我们发布了Archangel，这是迄今为止最大的人类反馈对齐LLM套件。它包括77个模型：{从1B到30B的7个预训练模型} x {11种不同的对齐方法}，所有这些都在几乎相同的训练设置下对齐于Anthropic HH（Ganguli等人，2022年）、Stanford Human Preferences（Ethayarajh等人，2022年）和OpenAssistant（Köpf等人，2023年）数据集的混合。

## 背景

大型语言模型传统上分为三个阶段进行训练：

### 预训练：

给定一些大型语料库，训练模型以预测给定文本的下一个词。损失函数是交叉熵损失（也称为“负对数似然损失”或“标准损失”）。我们称预训练模型为 $\pi$。

### 监督式微调

仍然使用标准损失，微调模型以预测与下游任务更相关的数据上的下一个词。我们称这个版本为 $\pi_{ref}$。

### 从人类反馈中学习

给定一个包含人类偏好的数据集 $D$，其中 $x$ 是输入，$y_w, y_l$ 是偏好和非偏好的输出，$r^*$ 是“真实”的奖励函数——首先假设人类更喜欢 $y_w$ 而不是 $y_l$ 的概率可以用BrxdleyTerry偏好模型（Brxdley 和 Terry，1952）来捕捉。其中 $\pi^*$ 是逻辑函数：
$$
\pi^*(y_w\succ y_l | x) = \pi(r^*(x, y_w) - r^*(x, y_l))
$$
由于从人类那里获得真实奖励的成本会非常高，我们必须学习一个奖励模型 $r_{\pi}^*$ 作为代理，通过最小化人类偏好数据的负对数似然来完成。

$$
L_{R_{\pi}^*} = {E}_{x, y_w, y_l \sim D} [-\log \sigma(r_{\pi} (x, y_w) - r_{\pi} (x, y_l))]
$$
现在我们有了一个人类代理，我们可以使用它来评判 $\pi_{\theta}^*$ 的生成。

但是，仅仅最大化奖励可能会牺牲生成语法正确的文本。为了避免这样的结果，我们需要一个项来限制语言模型可以从经过微调的有用版本 $\pi_{ref}$ 漂移多远。其中 $\pi_{\theta}^*$ 是我们正在优化的模型，$\pi^*$ 是在这两种考虑之间最优权衡的模型，

$$
\pi^* = \arg \max_{\pi_{\theta}} \mathbb{E}_{x \in D, y \in \pi_{\theta}} [r_{\theta} (x, y)] - \beta D_{KL} \left( \pi_{\theta} (y|x) \| \pi_{ref}(y|x) \right)
$$


其中 $KL$ 是两个分布之间的KL散度，$\beta > 0$ 是一个超参数。由于这个目标不是可微的，我们需要使用像PPO（Schulman et al., 2017）这样的RL算法。

## 我们是否需要强化学习？

RLHF（从人类反馈中进行强化学习）并不是唯一的对齐大型语言模型（LLMs）的方法。实际上，鉴于RLHF在分布式设置中的不稳定性，研究社区越来越多地转向可以直接在人类偏好数据集上优化的封闭形式损失函数。正如我们在下一节中将看到的，这些方法也与前景理论（Tversky和Kahneman，1992）有关。

### 直接偏好优化

我们从早期的工作（Peng et al., 2019）中知道，目标（3）的最优语言模型将具有以下分布：
$$
\pi^*(y|r) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} \cdot r^*(x,y)\right)
$$
其中 $Z(x)$ 是一个划分函数，它将右侧转换成概率。在最近的一篇论文中，Rafailov+Sharma+Mitchell et al. (2023) 将上述内容重写为最优奖励项：
$$
r^*(x,y) = \beta \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)
$$
然后他们将这个重新插入公式（1），以仅用最优语言模型分布 $\pi^*$ 和参考分布 $\pi_{ref}$ 来表达偏好概率。这个巧妙的想法使我们能够避免计算显式的奖励：
$$
\pi^*(y_w \succ y_l | x) = \frac{1}{1 + \exp\left(-\frac{\beta}{2} \left(\log \frac{\pi^*(y_w | x)}{\pi_{ref}(y_w | x)} - \log \frac{\pi^*(y_l | x)}{\pi_{ref}(y_l | x)}\right)\right)}
$$
尽管我们不知道 $\pi^*$ 是什么，我们知道我们的语言模型 $\pi_{\theta}$ 与人类偏好对齐得越多，$r^*(y_w \succ y_l |x)$ 就会越大。这意味着我们可以直接优化我们的语言模型，以最小化观察到的人类偏好的负对数似然，这称为直接偏好优化（DPO）损失：
$$
L_{DPO}(\pi_{\theta}, \pi_{SFT}) = \mathbb{E}_{x, y_w, y_l \sim D} \left[-\log \pi^*\left(\beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]
$$
根据作者的说法，他们的方法在理论上和传统的RLHF一样有效，在实践中更好，因为它不受前者训练不稳定性的影响。

### 序列似然校准

Zhao et al. (2023) 采取了一种更简单的方法：只要确保首选输出的对数概率比非首选输出的对数概率至少大 $\beta$ 边际：
$$
L_{cal}(\pi_{\theta}) = \mathbb{E}_{x, y_w, y_l \sim D} \left[ \max(0, \beta - \log \pi_{\theta} (y_w|x) + \log \pi_{\theta} (y_l|x))\right]
$$
如前所述，我们不想从参考模型 $\pi_{ref}$ 漂移得太远，作者通过为来自参考模型 $\pi_{ref}$ 生成的样本添加 $\epsilon$-加权的交叉熵项来执行这一操作。这为我们提供了序列似然校准（SLiC）损失：
$$
L_{SLiC}(\pi_{\theta}, \pi_{ref}) = L_{cal}(\pi_{\theta}) + \lambda_{reg} \mathbb{E}_{x  \sim D, y \sim \pi_{ref}(x)} \left[-\log \pi_{\theta} (y|x)\right]
$$
请注意，这没有像DPO那样与RLHF有整洁的等价性；即使我们只考虑 $L_{cal}(\pi_{\theta})$，隐含的偏好模型看起来像：
$$
\pi^*(y_w \succ y_l | x) = \min \left(0,  \frac{1}{\beta}{\pi_{ref}(y_w | x)} - \frac {1}{\beta}{\pi_{ref}(y_l|x)}  - \beta - \frac{\pi_{ref}(y_l | x)}{\pi_{ref}(y_w|x)} \right)
$$
这看起来不像任何常规的偏好模型。由于从 $\pi_{ref}$ 采样速度慢，对于本文中的实验，我们假设参考分布恢复了SFT分布，并将 $\epsilon$ 加权项视为标准的语言建模损失。由于标准损失已经被纳入，我们只进行一次对齐阶段——否则，模型实际上会经历2个时期的监督微调，这排除了逐个比较的可能性。

### PPO（离线，单步）

标准RLHF目标（2）通常使用一种变体的近端策略优化（PPO）（Schulman et al., 2017）进行优化，它通过“裁剪”我们的语言模型 $\pi_{\theta}$ 与前一步骤的版本 $\pi_{old}$ 漂移的距离来工作。PPO是一个在线算法——代是从我们当前的模型中采样的，由奖励模型进行评判，然后用于更新当前版本。然而，这个过程很慢（主要是由于采样代），在实践中相当不稳定（特别是在分布式设置中），所以我们可以：

1. 永远不更新 $\pi_{old}$ 并将其保持为 $\pi_{ref}$，而不是像我们传统上那样保守地裁剪。
2. 使用现有数据集中的偏好而不是即时推断它们。

Baheti et al. (2023) 发现，这些变化，以及将整个输出序列视为单个动作——而不是将每个标记的生成分别视为动作——大大提高了稳定性；他们称他们的方法为HALo。然而，由于语言模型对齐历来将每个标记视为单独的动作，我们省略了第三个变化，只保留了前两个。为了使这个更简单，我们甚至不会费心学习奖励，只是对 $y_w$ 使用 +1 的奖励，对 $y$ 使用 1 的奖励。得到的损失看起来像：

$$
L_{PPO} (\text{offline}) = -\mathbb{E}_{x, y \sim D} \left[ \min(r_{\theta}A(x, y_{< t}, y_t), \text{clip}(\rho_{\theta}, 1 - \epsilon, 1 + \epsilon) \cdot \mathbb A(x, y_{< t}, y_t))\right]
$$

其中 $\rho_{\theta} = \log \frac{\pi_{\theta}}{\pi_{ref}}$ 并且 $\mathbb A(x, y_{< t}, y_t)$ 是每个标记的优势（即，在给定状态下生成给定标记的额外收益）。请注意，称这种方法为PPO是一种误称，因为这些变化。但是为了避免引入太多新术语，我们将称它为“PPO（离线）”。

### 哪种现有方法最有效？

为了对这些方法进行基准测试，我们在三个著名的人类反馈数据集上对齐了Pythia-{1.4, 2.8, 6.9, 12.0}B（Biderman et al., 2023）和Llama-{7, 13, 30}B（Touvron et al., 2023）模型：Anthropic HH（Ganguli et al., 2022）、OpenAssistant（Köpf et al., 2023）和SHP推荐子集（Ethayarajh et al.,


## 人类感知损失

经济学家Kahneman和Tversky因他们在前景理论上的工作而闻名，这是关于人类如何对不确定结果做出决策的理论（Tversky和Kahneman，1992）。最著名的是，这个理论形式化了诸如损失厌恶这样的概念，即人类对同等大小的损失比收益更敏感。与本工作最相关的两个前景理论点是发现：

1. 某些结果的效用总是相对于某个参考点（例如，一个人开始时拥有的钱或保证会收到的钱）。
2. 人类效用在相对收益或损失上不是线性的；随着你远离参考点，效用变化率会减少。

Tversky和Kahneman（1992）提出了以下人类效用函数的形式，也称为人类价值函数：
$$
h(z, z_ref; \lambda; \alpha) =
\begin{cases}
(z - z_{\text{ref}})^\alpha & \text{if } z > z_{\text{ref}} \\
-\theta(z_{\text{ref}} - z)^\alpha & \text{if } z < z_{\text{ref}}
\end{cases}
$$
其中 $\alpha$ 的中值是0.88， $\lambda$ 的中值是2.25。这些值是通过实验确定的，实验要求人们对赌博的确定性等价物进行评估（例如，一个人愿意接受的最低金额保证赔偿，以代替某个特定的赌博）。例如，对于一个有80%概率返回100美元，20%概率返回0美元的赌博，一个人可能会说他们的确定性等价物是60美元，这比预期价值低，因为人类的损失厌恶倾向。Gurevich等人（2009）在后来的工作中也提出了其他形式。人类价值函数的显著特性是：

1. 存在一个参考点，通过添加或减去以获得相对收益或损失。
2. 相对损失的价值函数是凸的，而在收益上是凹的（即，随着你远离参考点，敏感度减少）。
3. 损失厌恶（在损失区域效用变化率更大）。

在图4中，我们绘制了对齐函数赋予人类的值函数：
$$
h_{\text{RLHF}}(x, y_w, y_l) = \sigma(r_{\text{RLHF}}(x, y_w) - r_{\text{RLHF}}(x, y_l))
$$
$$
h_{\text{DPO}}(x, y_w, y_l) = [\log \sigma(r_{\text{DPO}}(x, y_w) - r_{\text{DPO}}(x, y_l))]
$$
$$
h_{\text{SLiC}}(x, y_w, y_l) = \min (0, r_{\text{SLiC}}(x, y_w) - r_{\text{SLiC}}(x, y_l) - \beta)
$$
所有这些都具有Kahneman-Tversky值函数的特性：所有这些都承认参考点的存在（即非首选 $y_l$ 的奖励）；大多数在收益上是凹的，在损失上是凸的；大多数表现出损失厌恶。因此，我们称这些方法为人类感知损失函数（HALOs）。DPO性能可以与离线PPO在假奖励上相匹配（直到13B参数），如3.4节所讨论，这一事实挑战了LLM对齐中重视奖励学习的传统智慧，而是暗示隐式建模人类偏差在HALOs的成功中起着重要作用。

##  Kahneman-Tversky优化

如果对齐方法的有效性主要基于它们是HALOs，那么可能不需要偏好对。我们可以直接最大化输出的效用，而不是最大化偏好的可能性。我们可以通过将Kahneman-Tversky人类价值函数（6）适应于LLM设置来实现这一点：

1. 原始函数中的指数使其难以优化，所以我们设 $h$ 为 $h(z, z_{\text{ref}}) = \pi(z - z_{\text{ref}})$，假设逻辑函数 $\pi$ 在收益上是凹的，在损失上是凸的。我们用两个超参数 $\epsilon_d^+, \epsilon_d^-$ 替换损失厌恶系数，分别加权可取和不可取示例的损失。

2. 由于LLM生成没有与之相关的货币价值，我们用RLHF目标下的隐式奖励替换货币奖励（3）。

3. 人类对所有可能跟随 $x$ 的 $y$ 有一定的了解，不仅仅是 $y_w, y_l$。因此，更有意义的是参考点 $z_{\text{ref}}$ 是最优策略下的预期奖励，不仅仅是跟随 $x$ 的生成，而是跟随任何输入 $x'$：$\mathbb{E}_{x' \sim D, y' \sim \pi^*} [r^*(x', y')]$。

结合这些变化，并假设 $Z(x)$ 在（3）中对所有输入都是一样的，我们得到一个新的目标：
$$
h(x, y; \beta) = \pi(r^*(x, y) - \mathbb{E}_{x' \sim D, y' \sim \pi^*} [r^*(x', y')])
$$
$$
= \pi(\beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} - \mathbb{E}_{x' \sim D} [\beta KL(\pi^* \| \pi_{\text{ref}})])
$$

其中 $\pi^*, \pi_{\text{ref}}$ 分别是 $\pi^*(y|x), \pi_{\text{ref}}(y|x)$ 的简写。我们不知道 $\pi^*$ 是什么，但我们知道我们的语言模型越一致，$h(x, y; \beta)$ 的值就越大。因此，根据给定的生成 $y$ 是否被认为是“可取”或“不可取”，我们可以优化以下损失：
$$
L_{\text{KTO}}(\pi_{\theta}, \pi_{\text{ref}}) = \mathbb{E}_{x, y \sim D} [w(y)(1 - \hat{h}(x, y; \beta))]
$$

$$
g(x, y; \beta) = \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)} - \mathbb{E}_{x' \sim D} [\beta KL(\pi_{\theta} \| \pi_{\text{ref}})]
$$

$$
h(x, y; \beta) =
\begin{cases}
\pi(\sigma(x, y; \beta)) & \text{if } y \sim y_\text{desirable} |x \\
\pi(-\sigma(x, y; \beta)) & \text{if } y \sim y_\text{undesirable} |x
\end{cases}
$$



不可取损失的有效比例应该在1:1到1.33:1之间。例如，如果我们随机丢弃了90%的可取数据，那么 $\frac{\epsilon_d^+}{\epsilon_d^-} = 0.1$，所以 $\epsilon_d^+$ 应该在10到13.33之间。

结果

我们使用KTO损失对第3节中的相同模型套件在相同数据上进行了对齐（见图5）。我们发现：

1. SFT+KTO 在所有规模上与 SFT+DPO 具有竞争力，尽管它不使用偏好对。
2. KTO 单独对于 Llama-{7B, 13B, 30B} 模型明显优于 DPO 单独。实际上，一个经过KTO对齐的 Llama-{13B, 30B} 模型与它的 SFT+KTO 对应物具有竞争力，尽管它没有首先经过监督微调，并且是经过我们测试的对齐方法中唯一显示出这种行为的方法。
3. 我们可以在对齐KTO之前随机丢弃高达90%的可取数据，仍然超过DPO性能（对于不可取数据也是如此，如图6所示）。

值得注意的是，这些结果低估了KTO相对于DPO的实际改进。在现实环境中，KTO将拥有比DPO方法更多的数据，因为它不依赖于成对的偏好数据。例如，零售公司将拥有大量客户互动数据以及这些互动是否顺利（即，是否购买了商品）的了解；他们几乎没有反事实数据类型（即，什么会使不成功的客户互动 $y_l$ 成为成功的 $y_w$）。

## Archangel

我们作为Archangel套件发布了我们训练的所有77个模型：{4个Pythia模型 + 3个Llama模型} x {SFT, SLiC, SFT+SLiC, DPO, SFT+DPO, PPO (offline), SFT+PPO (offline), KTO, SFT+KTO (offline), CSFT, SFT+CSFT}。1 这些模型都是在几乎相同的设置下训练和采样的（例如，相同的随机种子，相同的优化器，相同的学习率调度程序，有效批量大小为32等）。特定于模型的超参数是根据扫描设置的。不足为奇的是，跨不同损失函数具有相同含义的超参数值（例如，KTO和DPO中的$\beta$）最终具有相同的值。因为一些方法依赖于偏好对而其他方法则不然，所以训练数据被看到的次数在两种损失类型（例如，基于偏好的与无偏好的）之间是不同的，但在同种类型的损失内是相同的。用于采样GPT-4判断的模型提示在所有模型中都是相同的。模型提示遵循TULU 2的聊天格式（Ivison等人，2023）。此外，使用条件标记训练的模型应在提示后附加\<|good|>或\<|bad|>。通过在接近相同的设置下对齐这77个模型，我们希望研究社区能更好地理解不同方法和不同规模下对齐的有效性如何发展。

## 未来的工作

HALOs作为一个独特的功能类别的存在提出了许多有趣的问题：

- 是否存在一个人类价值函数——以及相应的HALO——更好地描述人类如何看待语言？KTO损失基于货币收益和损失的人类价值函数中值，这几乎可以肯定是与人类感知文本的好坏不同的。那么特定于语言的人类价值函数是什么样子的？它的中值形式是什么，以及它如何在个体之间变化？

- 在不同规模上会出现哪些不同的帮助/有害差异？所有其他条件保持不变，当它们更大时，经过反馈对齐的LLMs是否更有可能变得阿谀奉承（Perez等人，2022），正如其他人所指出的？还是有害性更是一个小模型的问题，仅仅是因为它们对好坏的感知更差？

- 鉴于KTO所需的数据更加易于获取，我们可以在多大程度上推动合成数据的发展？例如，如果我们想要创建一个毒性数据集来对齐我们的模型以减少毒性，创建一个元组（$x, y_w, y_l$），其中 $y_w$ 比 $y_l$ 更具毒性是棘手的。然而，有了KTO，我们可以轻松地创建一个数据集（$x, y, [y \text{ is desirable}]$），其中可取性是由某个黑盒毒性检测API确定的。使用基于分数的数据对齐模型的能力是PPO的一个巨大吸引力，KTO允许这种二进制版本。
