# 通过想象、搜索和批评实现LLM的自我改进

## 摘要

尽管大型语言模型（LLMs）在各种任务中展现了令人印象深刻的能力，但在涉及复杂推理和规划的场景中，它们仍然面临挑战。自我修正和自我学习成为可行的解决方案，通过允许LLMs基于自我评估的奖励来优化其输出并从中学习。然而，LLMs在自我优化其响应方面的效果，尤其是在复杂推理和规划任务中，仍然存疑。本文介绍了AlphaLLM，一种用于LLM自我改进的框架，它将蒙特卡洛树搜索（MCTS）与LLMs结合，建立了一个自我改进的循环，从而在不增加额外标注的情况下提升LLMs的能力。受AlphaGo成功的启发，AlphaLLM解决了将MCTS与LLM结合进行自我改进的独特挑战，包括数据稀缺性、语言任务的巨大搜索空间以及语言任务反馈的主观性。AlphaLLM由提示合成组件、针对语言任务优化的高效MCTS方法以及三个批评模型组成，用于提供精确的反馈。我们在数学推理任务中的实验结果表明，AlphaLLM显著提升了LLMs的性能，且无需额外标注，展示了LLMs自我改进的潜力。代码可在[https://github.com/YeTianJHU/AlphaLLM](https://github.com/YeTianJHU/AlphaLLM)获取。

## 1 引言

LLMs经过数万亿个标记的训练，拥有数十亿个参数，在广泛的自然语言处理任务中展现了无与伦比的能力[16, 17]。然而，它们在需要复杂推理和战略规划的场景中仍然面临挑战[20, 24]。尽管有Chain、Tree、Graph-of-Thought等先进的提示方法[25, 26, 4]，但通过大量高质量的有监督数据对LLMs进行微调仍然是根本提升模型性能的关键[27, 8, 3]。这种方法本质上受限于人类提供的数据范围和质量。

考虑到这些挑战，自我修正和自我学习的概念被提出作为有前景的解决方案[21, 19, 6]。在这些框架中，LLMs通常通过两种主要策略进行操作：1）它们基于过去响应的反馈不断优化其响应；2）它们广泛采样响应，然后从自我判断的偏好中学习，使用PPO或DPO作为奖励模型（Yuan et al. (); Chen et al. (2024)）。然而，LLMs是否能够有效地批评自己的输出以提升响应质量，或者应用标量奖励来指示响应的质量，尤其是在需要复杂规划和推理的上下文中，仍然是一个持续研究的问题（Valmeekam et al. (2022); Stechly et al. (2024); Huang et al. (2023); Hong et al. (2023)）。另一方面，结合强化学习的高级搜索算法（如MCTS）使得模型能够通过自我对弈学习，并在复杂任务（如围棋）中达到或超越人类水平（Silver et al. (2016, 2017)）。这自然引发了一个问题：是否可以利用MCTS的优势与LLMs结合，开创一种新的自我改进范式？更具体地说，MCTS的整合是否能够使LLMs更有效地探索更好的响应，并通过战略信号引导，进而优化这些响应以提升整体性能？

为了回答这个问题，我们首先系统性地研究了AlphaGo，确定了其成功的三个关键方面：（_i_）大量的数据，包括自我对弈数据。（_ii_）使用树搜索，通过统计采样探索潜在的动作。（_iii_）准确且明确的环境反馈；围棋提供的直接且准确的反馈（赢或输）为学习提供了清晰且明确的信号（Silver et al. (2017)）。将MCTS与LLMs结合进行自我改进面临几个挑战：（_i_）数据有限：LLMs的高质量标注数据通常稀缺。此外，如何构建类似于AlphaGo自我对弈数据的合成数据仍然不明确。（_ii_）搜索效率：自然语言任务中潜在的标记组合数量巨大，导致搜索空间呈指数级增长，这对MCTS的效率提出了重大挑战（Ramamurthy et al. (2022)）。（_iii_）不完美的反馈：与围棋中明确的输赢反馈不同，自然语言任务中的反馈通常是主观且微妙的，没有直接的成功衡量标准。

在本文中，我们介绍了AlphaLLM，一个为LLM自我改进设计的想象-搜索-批评框架。AlphaLLM由三个关键组件组成，如图1所示。首先，想象组件用于合成提示，缓解数据稀缺问题。其次，我们提出了$\eta$Mcts，专门为语言任务中的高效搜索而设计。特别是，研究表明，在具有长时间跨度和大动作空间的强化学习问题中，多层次的时序抽象规划至关重要（Sutton et al. (); Peng et al. (2017); Luketina et al. (2019)）。因此，我们提出将文本生成过程建模为马尔可夫决策过程（MDP）问题中的选项，每个选项代表为特定子任务生成的一系列标记，类似于Chain-of-Thought提示中的链。这种建模通过显著减少搜索深度来提高搜索效率。此外，我们提出了状态合并和自适应分支因子，以进一步通过平衡搜索宽度和深度之间的权衡来提高搜索效率。最后，由于准确的反馈对MCTS的成功至关重要，我们引入了三个批评模型来指导$\eta$Mcts，包括用于估计预期奖励的价值函数、用于评估节点正确性的过程奖励模型以及用于评估整个轨迹的结果奖励模型。对于LLMs难以评估的复杂任务（如算术计算和代码执行），为了确保反馈的准确性，我们增强了批评模型的能力，使其能够动态决定使用哪些工具、何时使用以及如何有效使用。在$\eta$MCTs阶段之后，我们从批评模型中收集具有最大奖励的轨迹作为训练样本，以改进LLMs。

在数学推理任务中的实验结果表明，AlphaLLM能够高效搜索更好的响应，并使用它们来提升LLMs的性能，形成一个有效的自我改进循环。值得注意的是，基于Llama-2-70b和WizardMath-70B-V1.0，AlphaLLM在GSM8K上的表现从57.8提升到92.0，在MATH上的表现从20.7提升到51.0，与GPT-4的表现相当。

## 2 相关工作

### 与LLM结合的搜索

有效的搜索策略已被证明对涉及复杂推理和规划的任务至关重要，例如围棋（Silver et al., 2016）和数学推理（Cobbe et al., 2021; Hendrycks et al., 2021）。对于数学推理任务，已经研究了各种搜索方法。一个研究方向（Zhu et al., 2024; Xie et al., 2024）设计了带有动态剪枝的束搜索，其中低质量的束项被剪枝。另一项工作（Yao et al., 2024; Long, 2023; Besta et al., 2024; Hao et al., 2023; Feng et al., 2023）维护了一个树或图，表示当前解决输入问题的进展，其中潜在的分支被迭代扩展。我们的方法和Feng et al. (2023)都基于MCTS算法，而一个主要区别在于如何定义搜索步骤：Feng et al. (2023)将搜索步骤固定为单个标记或句子，而我们的方法在决定步骤时更加灵活。我们还精心设计了MCTS过程，结合了多个批评信号以更有效地引导搜索，并引入了自适应搜索参数以改进状态探索。因此，我们的方法实现了更好的性能。

### LLM的自我改进

作为可扩展监督成功的关键（Bowman et al., 2022），LLM的自我改进旨在通过主要利用LLM内部知识的监督来使LLM与人类偏好和价值观对齐（Zelikman et al., 2022, 2024）。自我改进的一个关键部分是如何获得可靠的批评信号，以区分LLM生成的好响应和坏响应。最初的工作（Bai et al., 2022; Wang et al., 2022）首先要求LLM生成多样化任务的输入查询及其相应的输出。然后，他们依赖于手工设计的启发式规则来过滤掉冗余或低质量的数据对（例如，查询太长或太短）。由于编写有效的启发式规则并非易事，后续工作（Sun et al., 2023; Li et al., 2023; Guo et al., 2024）提出了一些通用原则或判断标准，并要求LLM本身根据这些指导评估其响应的质量，希望LLM能够自动将这些原则应用到每个数据点，以更好地指导数据过滤。然而，这要求LLM具备强大的能力，能够将这些原则应用于每个具体案例并做出正确的判断。与之前的工作不同，我们提出利用MCTS的监督来进行LLM的自我改进：将MCTS的输出用于继续训练LLM。这是因为MCTS的输出通常比标准的核心采样质量更高，且这种巨大的差距确保了LLM能够自我改进。

## 3 预备知识

### 问题公式化

在本文中，我们考虑一个由概率$p_{\theta}$表征的LLM，并表示为策略$\pi_{\theta}$。它以序列${x}=[x_{1},\cdots,x_{n}]$作为输入，通常称为提示，生成响应${y}=[y_{1},\cdots,y_{m}]$。在LLM的上下文中，每个$x_{i}$和$y_{i}$表示预定义词汇表中的一个标记。策略$\pi_{\theta}$以自回归方式运行，每个标记依次生成，仅依赖于先前生成的标记提供的上下文。因此，策略构成了一个马尔可夫过程，其中条件概率分布$p_{\theta}({y}|{x})$可以分解并用链式法则表示为$p_{\theta}({y}|{x})=\prod_{i=1}^{m}p_{\theta}(y_{i}|{x},{y}_{<i})$。

基于这一特性，文本生成任务可以公式化为一个马尔可夫决策过程（MDP）问题，由$(\mathcal{S},\mathcal{A},T,R,\gamma)$组成，其中${s}_{t}\in\mathcal{S}$表示当前轨迹的上下文信息，即生成过程的当前状态，例如对提示的部分响应；$a_{t}\in\mathcal{A}$表示从词汇表中采样的单个动作或标记，导致通过连接${s}_{i}$和$a_{i}$转换到新状态$s_{i+1}$；${r}_{t}=R({s}_{i},a_{t})$表示生成对提示的评估，反映每个状态-动作对的期望或偏好。

这个MDP框架为应用强化学习（RL）方法优化策略$\pi_{\theta}$以最大化预期累积奖励$R$奠定了基础。基于这些设置，我们描述了自我改进问题。给定一个LLM $\pi_{\theta}$和一个初始数据集$\mathcal{D}^{0}$，该数据集由$N$个专家生成的提示-响应对$\{({{x}}_{t}^{0},{y}_{t}^{0})\mid i\in[N]\}$组成，自我改进的目标是迭代优化$\pi_{\theta}$以最大化奖励。优化过程包括从合成的提示和相应的响应中学习。这些响应是通过使用高级搜索算法在可能的响应空间中导航以最大化预期奖励而获得的。详细过程在附录中的算法3.2中描述。形成有效自我改进循环的主要挑战在于合成合适的提示、在巨大的动作空间中高效搜索以及获得精确的反馈，这些将在第4节中讨论。

### 蒙特卡洛树搜索

MCTS是一种基于采样的搜索算法，用于决策问题中的策略优化。它通过重复四个阶段来迭代构建搜索树：选择、扩展、评估和回传。在选择阶段，它通过上置信界（UCB）[1]从根节点递归选择子节点，$UCB(i)=w_{i}+C*\sqrt{2*\ln\frac{N_{i}}{n_{i}}}$，其中$n_{i}$和$N_{i}$分别是节点$i$及其父节点的访问计数，$C$表示平衡探索和利用的超参数，$w_{i}$是节点$i$的所有后代节点的平均值。

## 4 ALPHALLM

### 概述

ALPHALLM的架构如图1所示，由三个关键组件组成。首先，想象组件负责合成提示作为学习样本。其次，提出了一个高效的搜索组件，名为$\eta$Mcts，用于搜索高质量轨迹以优化策略。最后，搜索过程由专门设计的批评模型引导，以提供可靠的信号。

### 数据合成

设$\mathcal{D}^{0}=\{({{x}}_{i},{y}_{i})\mid i\in[N]\}$表示由$N$个专家生成的提示-响应对组成的初始数据集。数据合成过程旨在通过生成一组合成提示$\mathcal{D}^{1}=\{({{x}}_{i}^{1},\cdots)\mid i\in[N]\}$来扩展该数据集。每个合成提示${{x}}_{i}^{1}$的生成可以数学描述为应用于$\mathcal{D}^{0}$中一个或多个示例的变换$g$，${{x}}_{i}^{1}=g({{x}}_{t_{1}}^{0},\cdots,{{x}}_{t_{m}}^{0},\pi^{0})$，其中${{x}}_{t_{1}}^{0},\cdots,{{x}}_{t_{m}}^{0}$是从$\mathcal{D}^{0}$中选择的示例。变换函数$g$控制合成过程，可以是可学习的函数、手动定义的启发式规则、强大的LLM或配备数据合成指令的策略模型本身$\pi^{0}$。数据合成过程旨在丰富训练策略模型时呈现的多样性和复杂性。在多种策略中，如Self-instruct [25]、Evol-instruct [24]，我们选择了类似于Yu et al. [22]中描述的方法。

### $\eta$Mcts

#### 4.3.1 选项级MCTS

当将MCTS应用于LLMs时，自然可以执行标记级搜索，其中每个标记被视为一个动作[8]。然而，LLMs的词汇量通常很大，这带来了重大挑战，即在如此巨大的空间中进行深度搜索随着搜索空间的指数级扩展变得越来越复杂。为了缓解这一问题，一些努力提出了句子级搜索，将每个句子或步骤视为一个搜索节点[6]。虽然这种方法减少了搜索空间，但可能会影响MCTS在LLMs中应用的灵活性和有效性，特别是在标记的细微变化可能显著影响结果的任务中，或者需要超越句子的更全面搜索时。

#### 4.3.2 基于重要性的自适应分支

在之前与选项/句子级树搜索相关的工作中（Feng et al., 2023; Yao et al., 2024），通常假设树中的每个节点具有相同的预定义宽度，即分支因子。这一假设是由于与具有有限动作空间的标记级MCTS不同，选项级的样本空间非常大，具有无限数量的标记组合。因此，有必要为每个节点设置预定义的最大宽度。然而，这个预定义的分支因子很难设置，因为不当的选择可能导致搜索树过浅或过薄，从而导致搜索空间的低效探索。

为了量化由分支因子限制引起的误差，我们定义了分支误差$E_{\phi}(t)$。对于具有分支因子$m_{t}$的节点$t$，它旨在使用$m_{t}$个子选项$o_{i}^{t}\sim D_{children}^{t}$（其中$i\in\{1,\ldots,m_{t}\}$）来表示所有可能的选项。因此，对于来自选项空间的合法选项$o_{j}^{t}\sim\pi(s_{t})$，我们可以计算它与现有$m_{t}$个选项之间的最小价值差异，这捕获了使用$m_{t}$个可用选项表示其他可能选项的误差。它可以表示为$E_{\phi}(t)=E_{o_{j}^{t}\sim\pi(s_{t})}[\min_{o_{i}^{t}}|v_{\phi}^{\pi}([s_{t},o_{j}^{t}])-v_{\phi}^{\pi}([s_{t},o_{i}^{t}])|]$，其中$v_{\phi}^{\pi}$是价值函数，将在第4.4节中详细说明。这里我们将节点$s_{t}$的重要性定义为$I(s_{t})=\max_{o_{i}^{t}}|v_{\phi}^{\pi}([s_{t},o_{i}^{t}])-v_{\phi}^{\pi}(s_{t})|$。为简单起见，我们假设子节点的价值是均匀分布的（高斯分布的详细分析见附录A.4）。在这种假设下，我们在附录A.3中证明了$E_{\phi}(t)\leq\frac{I(s_{t})}{m_{t}-1}$。当$E_{\phi}$小于某个$\epsilon$时，我们旨在使用更少的节点总数以提高效率。

**定理4.1**：在树搜索中，最优分支因子$m_{t}$设置为$m_{t}-1$与节点重要性$I(s_{t})$成正比，条件是$\frac{I(s_{t})}{m_{t}-1}\leq\epsilon$。详细证明见附录A.3。

类似的概念也在Taylor et al. (2014); Clouse (1996)中提出。直观上，$I(s_{t})$捕捉了当前状态的最大价值偏差。当这个值较小时，没有必要在该节点上进一步探索，因为在该节点上展开不会有显著差异。相反，如果该值较大，则值得尝试不同的子节点。我们设置节点$n(s_{t})$（在提取1之后）允许的子节点数量与该重要性线性相关，使用因子$\alpha$。在实践中，为了避免早期阶段$I(s_{t})$的极端情况，我们通过深度相关的常数$c_{min}(t)$和$c_{max}(t)$限制子节点数量，$n(s_{t})=\max(c_{min}(t),\min(\lfloor\alpha I(s_{t})\rfloor+1,c_{max}(t)))$。

#### 4.3.3 状态合并

在确定$n({s}_{t})$后，另一个问题是同一节点下的选项可能非常相似，导致许多不必要的子树。由于我们无法直接控制${o}_{t}\sim\pi({s}_{t})$，一种缓解此问题的策略是利用Van Eyck和Muller (2012)中讨论的移动组概念。通过将相似节点合并到同一组中，我们可以增加组之间的多样性，从而在有限的搜索展开中覆盖更大的问题空间，使搜索过程更高效。

在这里，我们采用了Abel et al. (2018)和Fu et al. (2024)中的节点谓词$p_{vM}$的定义，以表示两个节点是否极其相似。在实践中，每次我们从策略生成一个新选项时，我们使用启发式函数作为$p_{vM}$来检查其与所有现有组的相似性。启发式函数可以是更快的基于规则的测量（例如编辑距离）或基于模型的方法（例如提示语言模型）。基于此，我们决定是将此选项与之前的选项合并还是创建一个新组。

#### 4.3.4 使用专用LM的快速展开

模拟操作使用展开策略从给定状态预测未来轨迹，对于有效的MCTS至关重要。这一过程显著提高了探索和利用的效率，并增强了奖励估计的准确性2。在轨迹末端进行的估计通常具有较低的偏差但较高的方差；因此，模拟多个可能的轨迹会产生低偏差、低方差的估计，从而实现更明智和有效的搜索过程。理想情况下，$\pi_{\theta}$将作为展开策略，但其计算需求使其不适用于MCTS所需的快速模拟。为了解决这一挑战，我们提出使用较小的专用LM作为快速展开策略$\pi^{\texttt{fast}}$。给定状态${s}_{t}$，快速展开策略$\pi^{\texttt{fast}}$高效地继续生成，直到达到终止条件，表示为$\pi^{\texttt{fast}}({s}_{t})$。

脚注2：通常，模拟越接近终止状态，奖励估计越准确。

### 批评模型

在AlphaLLM中，我们设计了三种类型的批评模型来指导搜索过程。

**价值函数**：价值函数表示为$v^{\pi}({s})$，表示从状态${s}$开始并遵循策略$\pi$后的预期回报，由$v^{\pi}({s})=\mathbb{E}_{\tau\sim\pi}[R(\tau)|_{\texttt{so}}={s}]$给出，其中$R(\tau)$表示轨迹$\tau$的折现回报。为了训练参数化价值函数$v^{\pi}_{v^{\bar{\theta}}_{v}}({s})$，给定提示$\mathcal{D}=\left\{\left({x}_{i},\cdots\right)\mid i\in[N]\right\}$，对于每个提示${x}_{i}$，我们通过遵循策略$\pi$生成多个轨迹${\tau}^{j}_{i}=\left\{{x}_{i},{o}^{j}_{i1},{o}^{j}_{i2},\cdots,{o}^{j}_{\ell T}\right\}$，共$J$次。最终奖励$\tau^{j}_{i}$表示${\tau}^{j}_{i}$是否与${y}_{i}$一致——例如，奖励包含数学任务中正确答案的轨迹或紧密遵循指令的轨迹作为真实值。然后我们构建数据集$\mathcal{D}_{\texttt{value}}=\left\{({s}^{j}_{it},v^{j}_{it})\mid i\in[N],t\in[T],j\in[J]\right\}$，其中${s}^{j}_{it}=[{x}_{i}\cdot{o}^{j}_{<it}]$，$v^{j}_{it}=\tau^{j}_{i}$。价值函数$v^{\pi}_{v}$通过最小化均方误差进行优化：$\mathcal{L}_{\phi}=-\mathbb{E}_{({s},v)\sim\mathcal{D}_{\texttt{value}}}(v^{\pi}_{v}({s})-v)^{2}$。与Feng et al. (2023)类似，$v^{\pi}_{v}$是一个LLM，顶部有一个MLP层，用于在每个标记上输出标量，使用每个状态最后一个标记的标量预测作为价值。

**PRM**：价值函数通常面临信用分配问题（Sutton, 1984），其学习可能由于延迟和稀疏的奖励而效率低下（Sutton和Barto, 2018）。因此，我们提出引入PRM，通过过程监督（Lightman et al., 2023）直接评估选项。PRM生成内在奖励（Chentanez et al., 2004），以鼓励探索有利的选项，通过提供即时的、特定于动作的奖励，有效缓解奖励稀疏性问题。给定状态${s}_{t}$和时间$t$的选项${o}_{t}$，PRM旨在预测采取选项${o}_{t}$在状态${s}_{t}$中产生的即时奖励$r^{\texttt{PRM}}_{t}$。形式上，PRM是一个函数$R({s}_{t},{o}_{t})\to r^{\texttt{PRM}}_{t}$。虽然PRM理想情况下需要每个状态的质量标签（Uesato et al., 2022），但由于获取这些标签的高成本和时间消耗，使用前缀采样的MC估计（Wang et al., 2023）作为代理，这与价值函数的目标一致。与在策略模型顶部添加MLP层以输出标量奖励（Ouyang et al., 2022）不同，我们将PRM制定为文本生成任务，以充分利用LLM的内在知识来评估选项的质量。我们调整了为价值函数构建的数据集，得到$\mathcal{D}_{\text{PRM}}=\{(s_{ti},\sigma_{t},r_{t}^{\text{PRM}})|i\in[N],t\in[T]\}$，其中$r_{t}^{\text{PRM}}$是奖励的文本描述，例如，如果$v_{ti}$大于某个阈值，则可以将选项视为良好。为了训练PRM，我们从策略模型$\pi$初始化它，并使用以下提示模板和典型的语言模型损失。提示模板见附录A.5。

**ORM**：除了价值函数和PRM外，ORM还用于指导MCTS。ORM旨在评估整个选项序列，评估完整轨迹与期望目标的契合程度（Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Feng et al., 2023）。结果评估通过提供对轨迹的全面评估，补充了价值函数和PRM。ORM在MCTS的模拟阶段起着至关重要的作用，通过在终端状态提供更准确的信号，从而促进探索和利用策略之间的平衡。ORM被制定为文本生成任务，类似于PRM。我们利用价值函数训练的相同数据集，并构建$\mathcal{D}_{\text{ORM}}=\{({{x}}_{i},{{o}}_{i,T}^{1},r_{i}^{\text{ORM}})|i\in[N]\}$，其中每个实例包括初始状态或提示${{x}}_{i}$，从该状态采取的一系列动作或选项${{o}}_{i,T}^{1}$，以及指示序列成功或质量的文本奖励$r_{i}^{\text{ORM}}$。同样，ORM从策略模型$\pi$初始化，并使用以下提示模板和语言模型损失进行训练。提示模板见附录A.5。

状态${s}$的最终评分评估是价值函数、PRM和ORM的加权和：$s({s})=\beta_{\text{value}}\cdot v_{y}^{T}({s})+\beta_{\text{PRM}}\cdot\text{PRM}({s})+\beta_{\text{ORM}}\cdot\mathbb{E}_{\tau\sim\pi^{\texttt{fast}}({s})}[\text{ORM}(\tau)]$，其中$\tau\sim\pi^{\texttt{fast}}({s})$表示在$\pi^{\texttt{fast}}$下从${s}$开始的轨迹，$\beta_{\text{value}}$、$\beta_{\text{PRM}}$、$\beta_{\text{ORM}}$是超参数。在实践中，我们发现价值函数模型具有更好的精度和校准，而PRM具有更高的召回率（附录A.10）。尽管带有快速展开的ORM提供了低偏差、低方差的估计，但它仍然继承了$\pi^{\texttt{fast}}$的一些偏差。因此，结合这些批评模型可以产生更强的评估信号。

### 策略自我改进

策略改进是一个迭代过程，每次迭代包含两个主要步骤：_数据生成_和_策略微调_。

**数据生成**：在这一步骤中，我们假设在第$k$轮拥有当前策略$\pi_{\theta_{k}}$和合成提示$\mathcal{D}_{k}=\{{{x}}_{1}^{k},\ldots\}$，其中每个${{x}}_{i}^{k}$表示一个问题。我们通过首先在$\mathcal{D}_{k}$上执行$\eta\text{MCTS}$（第4.3节），然后从相应的树中为每个问题${{x}}_{i}^{k}$采样一个轨迹${{y}}_{i}^{k}$，获得策略$\pi_{\theta_{k}}$的相应训练数据$\mathcal{D}_{k}$。在这里，我们选择每个输入问题在叶节点上产生最高批评分数的轨迹。接下来，我们过滤掉相应轨迹不达标的实例，形成$\mathcal{D}_{k}=\{({{x}}_{i}^{k},{{y}}_{i}^{k})\mid f({{x}}_{i}^{k},{{y}}_{i}^{k})>\gamma\}$，其中$f$表示质量评分函数，$\gamma$表示阈值。可以有几种方法来实现该函数，这里我们简单地使用ORM（第4.4节）。

**策略微调**：使用获得的训练数据$\mathcal{D}_{k}$，我们将数据组织到附录A.5中所示的提示模板中。然后使用目标损失对策略$\pi_{\theta_{k}}$进行微调：$\mathcal{L}_{\theta_{k}}=\mathbb{E}_{({{x}}_{i}^{k},{{y}}_{i}^{k})\sim\mathcal{D}_{k}}\big{[}\log\pi_{\theta_{k}}({{y}}_{i}^{k}|{{x}}_{i}^{k})\big{]}$，从而得到更新后的策略$\pi_{\theta_{k+1}}$。我们将其他训练方法（如DPO（Rafailov et al., 2023）或PPO（Schulman et al., 2017））留待未来工作。

## 5 实验

### 实验设置

AlphaLLM通常适用于广泛的任务。作为早期探索，本文在数学推理问题上进行了实验，其中学习信号清晰可定义，即最终答案正确或错误。我们选择在两个广泛使用的数据集GSM8K（Cobbe et al., 2021）和MATH（Hendrycks et al., 2021）上进行评估。对于GSM8K，我们使用整个测试集，而对于MATH，由于计算限制，我们使用Lightman et al. (2023)相同的程序选取子集。我们评估策略模型预测答案正确的性能。此外，我们计算平均展开次数，表示为树中的节点数，作为计算效率的衡量标准。我们将AlphaLLM的性能与一系列专有模型进行比较，包括OpenAI的GPT-4和GPT-3.5、Anthropic的Claude-2以及Google的PaLM-2和Gemini模型家族。为了确保公平和一致的评估，我们采用CoT作为主要提示方法。此外，我们还与强大的开源模型进行比较，包括Llama-2-70b（Touvron et al., ）和WizardMath-70B-V1.0（Luo et al., 2023）。

我们选择Llama-2-70b作为GSM8K数据集的策略模型，选择WizardMath-70B-V1.0作为MATH数据集的策略模型。为了构建价值函数、PRM和ORM的训练数据集，我们为每个提示生成50个轨迹，并按照第4.4节构建训练目标。PRM和ORM均使用策略模型的权重进行初始化，而价值函数使用较小的Llama-2-13b模型，因为我们观察到增加价值函数模型大小没有性能提升。在ORM的设计中，GSM8K未纳入工具使用。然而，对于MATH，我们通过结合python sympy等工具来增强ORM，以评估轨迹的质量，类似于Gou et al. (2023)中描述的方式。训练采用1e-6的学习率，训练一个epoch。对于快速展开策略模型，我们选择Abel-002-7B模型（Chern et al., 2023）用于GSM8K和MATH任务，因其高效性和卓越性能。对于MCTS参数，它们在不同尺度上配置，如附录A.6所示。我们将$\beta_{\text{value}}$、$\beta_{\text{PRM}}$和$\beta_{\text{ORM}}$均设置为1.0。

对于策略自我改进（第4.5节），我们将策略模型训练最多3个epoch，批量大小设置为128，学习率为$5\times 10^{-6}$，最小学习率为$1\times 10^{-6}$。使用线性预热和衰减，预热百分比为10%。我们基于从训练实例中保留的开发集进行早停。对于GSM8K实验，我们进行两轮自我改进，分别合成6.4k和7.9k提示（Yu et al., 2023）以获得相应的MCTS输出进行训练。对于MATH实验，由于计算资源有限，我们仅进行一轮自我改进，并合成了5.9k提示。

选项的终止函数可以是学习的或基于规则的。在实践中，对于GSM8K数据集，终止条件发生在每行的末尾。这是基于该数据集的典型结构，其中每行代表一个独立的步骤或点。对于MATH数据集，由于其复杂性和基础模型倾向于生成许多$\backslash$n$\backslash$n换行符，其中包含一些不太有意义的内容，终止条件发生在检测到公式模式的行尾。在推理过程中，如果遇到$\backslash$n$\backslash$n，我们执行基于规则的公式模式检查。如果找到模式，则终止，否则继续生成，直到下一个$\backslash$n$\backslash$n。

### 结果

表2列出了各种方法在GSM8K和MATH数据集上的性能比较。我们的研究结果表明，基于Llama-2-70B和WizardMath-70B-V1.0的AlphaLLM仅使用最终答案注释，并通过训练$\eta$Mcts的响应继续改进。这一比较突显了我们想象-搜索-批评自我改进框架的有效性和广泛适用性。此外，当我们的模型增强$\eta$Mcts解码策略时，其性能显著提升，在GSM8K和MATH数据集上分别达到88.9和48.7的分数。经过两轮使用合成提示的自我改进后，AlphaLLM表现出与GPT-4相当的性能。这表明，在自我改进的方式下，利用最少的标注数据，可以显著提升LLMs在复杂问题解决任务中的能力。我们还在附录A.8中分析了各种搜索方法的性能。

### 消融研究

我们评估了AlphaLLM中每个组件的有效性，并在GSM8K上报告了结果，如表3(a)所示。仅配置价值函数和每个节点固定子节点数的Vanilla MCTS达到了79.5%的准确率。这作为评估每个附加组件引入的增量效益的参考点。使用自适应分支将准确率提高到84.9%。添加PRM将准确率略微提高到85.9%，显示了过程监督在搜索中的有效性。引入带有快速展开的ORM带来了更显著的改进，将准确率提高到86.5%。集成状态合并进一步提高了准确率，达到87.0%。最后，结合增加展开次数与其他组件，在该任务上取得了最佳性能。

表3(b)展示了选项公式化和工具增强批评模型对MATH数据集的消融研究。我们提出的$\eta$Mcts在148次展开中达到了45.4的准确率。当排除选项时，恢复到本质上句子级的MCTS，性能下降到44.1，展开次数显著增加到198。这表明选项公式化为MCTS引入了增强的灵活性，能够在较少的搜索努力下实现更好的性能。此外，当ORM仅利用内在知识时，性能下降最为显著，准确率降至38.8。这表明缺乏外部工具严重阻碍了ORM有效评估具有挑战性的数学问题的能力。

图2展示了在GSM8K上两轮自我改进训练的比较结果，使用重新排序和$\eta$Mcts收集的轨迹。我们报告了贪婪解码、$\eta$Mcts（展开次数较少，50-60次）和$\eta$Mcts（展开次数较多，200-300次）在每个模型上的性能。我们观察到：1）使用重新排序或$\eta$Mcts生成的轨迹训练的模型显著优于初始策略。此外，性能可以通过训练迭代改进，表明自我改进具有实现持续性能提升的潜力。2）虽然重新排序和$\eta$Mcts都可以生成高质量的自我改进轨迹，但$\eta$Mcts在高效率和更好准确性方面表现出色。使用其生成的轨迹训练的模型不仅超过了使用重新排序轨迹训练的模型的性能，而且在解码时表现出与GPT-4相当的性能，表明AlphaLLM是一个有效的自我改进框架。

我们进一步分析了每个组件的不同超参数和设计选择的影响。表4(a)显示，状态合并的不同启发式函数（带有超参数）对性能的影响有限。表4(b)显示，随着快速展开次数的增加，性能相应提高。这是由于估计的方差减少。我们在实验中使用$n=4$，以在性能和效率之间取得更好的平衡。关于快速展开模型选择的额外消融研究见附录A.7。

## 6 结论

在本文中，我们介绍了AlphaLLM，一个为LLM自我改进设计的想象-搜索-批评框架，无需额外注释。其核心是将MCTS与LLMs结合。为了解决与此集成相关的固有挑战，包括数据稀缺性、语言任务的巨大搜索空间以及语言任务反馈的主观性，我们引入了一个数据合成器用于战略提示合成，一个针对语言任务优化的MCTS，以及三个批评模型以提供精确的反馈。我们在数学推理任务中的实验结果表明，AlphaLLM显著提升了LLMs的性能，且无需额外数据注释。此外，当使用$\eta$MCTS解码时，AlphaLLM表现出与GPT-4相当的性能，突显了LLMs自我改进的潜力。
