# _OpenR_: 一个用于增强大语言模型推理能力的开源框架

## 摘要

在本技术报告中，我们介绍了 _OpenR_，一个开源框架，旨在整合关键组件以增强大语言模型（LLMs）的推理能力。_OpenR_ 将数据获取、强化学习训练（包括在线和离线）以及非自回归解码统一到一个软件平台中。我们的目标是建立一个开源平台和社区，以加速LLM推理的发展。受OpenAI的o1模型成功的启发，该模型通过逐步推理和强化学习展示了改进的推理能力，_OpenR_ 整合了测试时计算、强化学习和过程监督，以提高LLMs的推理能力。我们的工作是第一个提供开源框架，探索OpenAI的o1模型核心技术的框架，结合强化学习，实现了超越传统自回归方法的高级推理能力。我们通过在MATH数据集上评估 _OpenR_，利用公开可用的数据和搜索方法，展示了其有效性。我们的初步实验证实了显著的提升，推理和性能的相对改进由测试时计算和通过过程奖励模型的强化学习驱动。_OpenR_ 框架，包括代码、模型和数据集，可在 https://openreasoner.github.io 访问。

## 1 引言

OpenAI最近发布了o1 [1]，这是一个突破性的大语言模型（LLM），代表了强人工智能的巨大飞跃。据报道，该模型在数学和编码方面的能力是之前GPT-4o的五倍，特别是在各个领域表现出色：在竞争性编程中排名第89百分位，在美国著名数学奥林匹克资格赛中排名前500名学生，并在物理、生物和化学基准测试中超越了人类博士水平的准确性。通过强化学习技术训练，o1在复杂推理任务中表现出色，通过在LLMs中显式嵌入“思维链”（NCoT）过程，使其能够在生成响应之前通过逐步推理进行“深度思考”。o1的一个关键创新是它允许在推理过程中花费更多时间进行推理，标志着从快速、直接的响应转向缓慢、深思熟虑的多步推理时计算，如图1所示。

![Refer to caption](https://arxiv.org/html/2410.09671v1/x1.png)

**图1：推理时间计算**

- **(a)** 自回归LLM直接通过给定问题（Q）生成答案（A）。
- **(b)** 思维链或逐步思考的概念涉及在生成最终答案（A）之前引入中间推理步骤（R）。这些重复操作允许：
  1. 重新审视和修订先前的输出，
  2. 推进到后续推理阶段，
  3. `探索多个推理路径或轨迹`。

有趣的是，在人类认知中，有两种相关但不同的认知处理模式来指导人类决策和行为，每种模式在大脑回路和神经通路之间都有部分区别。系统1思维是快速、自动和直觉的，操作毫不费力且通常是无意识的。它依赖于神经通路，能够快速处理，特别是在需要快速反应或认知资源受限的情况下。系统2思维是深思熟虑、费力且有意识的，涉及集中注意力和分析推理。它处理信息较慢，用于复杂的问题解决、逻辑推理和决策任务。

o1模型是人工智能的一个令人兴奋的发展，因为LLMs现在不仅可以利用学习到的模式生成快速响应，更重要的是，通过思维链或其他形式的搜索机制模拟复杂的推理过程，类似于人类进行更深层次的逐步思考。o1改进的推理技能对多个领域产生了影响，包括科学、编码和数学。在编码竞赛中，o1的一个专门版本取得了令人印象深刻的成绩，在2024年国际信息学奥林匹克竞赛中排名第49百分位，并在模拟的Codeforces竞赛中超越了93%的人类竞争对手。除了其技术能力外，o1还代表了人工智能安全和对齐的进步。该模型的思维链推理为整合人类价值观和原则提供了新的机会，从而在安全评估和越狱测试中提高了性能。

思维链推理 [27] 和逐步思考在大语言模型（LLMs）中的想法并不新鲜。先前的研究表明，简单地在输入问题中添加“描述你的推理步骤”或“逐步解释你的答案”等指令，或提供少量示例，可以触发LLMs生成中间推理步骤（如图1所示），从而改善问题解决，特别是在数学和编码任务中 [27, 28]。然而，这些方法建立在现有的LLMs之上，而没有真正将思维链能力嵌入到模型本身。因此，LLMs无法固有地学习这种推理能力，导致如何将其直接整合到模型训练中的积极研究。提出的方法从收集专门的训练数据到构建奖励模型 [8, 14, 15] 以及增加解码的计算复杂性 [30, 29]，但尚未在大规模上取得显著的性能突破。

目前尚不清楚o1的创新是否源于模型本身，而不是依赖于外部提示系统。如果它确实涉及在架构中显式嵌入逐步推理，这将代表一个重大突破。基于显著的性能提升，o1表明传统上在训练期间应用的扩展原则 [21, 30] 现在与推理阶段相关。我们应该重新分配我们的计算重点，平衡预训练工作与推理时计算的有效使用。允许LLMs通过增加测试时计算来增强其输出，是创建能够管理开放式强推理和决策任务的自我改进代理的重要一步。这个方向，我们称之为LLM-原生思维链（NativeCoT），应该能够固有地反映人类系统2思维的深思熟虑、分析过程 [19]。

在本报告中，我们介绍了 _OpenR_，一个基于OpenAI的o1模型原则构建的开源框架，旨在复制和扩展其推理能力。我们的方法通过整合过程监督、强化学习（RL）和推理时计算策略（如引导搜索）来改进LLM推理。_OpenR_ 实现了关键组件，如过程监督的数据增强、通过RL进行策略学习以及高效解码算法。通过这样做，它将重点从仅仅在预训练期间扩展模型参数转移到在测试时利用更智能的推理策略。这些技术帮助模型逐步完善其推理，使其能够在测试时计算中暂停、评估中间推理并选择更好的解决路径。通过在公开可用的基准测试（如MATH数据集）上的实验，我们展示了过程奖励模型和引导搜索的结合将测试时推理性能提高了约10%。

总之，我们介绍了 _OpenR_，一个开源框架，通过整合测试时计算和过程监督来增强LLMs的推理能力，提供了一个包含模型、数据和代码的开放平台，以促进合作并加速LLM推理的研究。据我们所知，_OpenR_ 是第一个探索OpenAI的o1模型核心方法的开源框架，结合了强化学习技术。该框架包括旨在优化训练期间决策的强化学习算法，从而实现更准确和深思熟虑的逐步推理。此外，_OpenR_ 提供了生成合成过程奖励数据的工具，减少了对昂贵人工注释的依赖，并支持可扩展的过程监督。通过实验，我们展示了过程奖励模型和测试时引导搜索的有效性。

## 2 相关工作

在提高大语言模型（LLMs）推理能力领域的关键参考文献中，突出了几种创新方法，包括推理时计算、过程奖励模型和数据获取方法。

**推理时计算。** 为了讨论推理时计算在大语言模型（LLMs）中的作用，最近的研究集中在优化推理过程中推理的效率和有效性，而不仅仅是依赖于训练时间计算的扩展定律。一项关键研究，Feng等人 [2024] 展示了使用MCTS作为解码机制的好处，通过积极规划和选择更高质量的响应来增强推理计算。这种方法与Hao等人 [2023] 提出的推理即规划方法一致，其中推理被视为类似于决策过程中的规划，进一步强调了推理时间逐步推理的中心地位。最近，Snell等人 [2024] 的工作强化了优化推理策略可以产生优于简单增加模型规模的性能提升，强调了测试时间计算的关键作用。最后，Goyal等人 [2023] 的工作引入了隐式推理模型，通过引入暂停标记来鼓励生成过程中的深思熟虑推理。总的来说，这些最近的进展表明，推理时间优化——无论是通过基于规划的推理模型还是计算优化——作为提高LLM能力的关键因素，倡导超越训练时间扩展的策略，以增强推理、规划和计算效率。

**从结果监督到过程监督。** 在语言模型训练中，从结果监督到过程监督的转变在最近的研究中得到了重视，特别是在增强推理能力方面。Cobbe等人 [2021] 的基础工作引入了结果监督奖励模型（ORM）和广泛使用的数学推理数据集GSM8K，其中验证器被训练来评估生成解决方案的最终正确性。虽然ORM在早期阶段发挥了关键作用，但它主要关注评估最终结果，而不是导致最终输出的推理步骤。

在此基础上，过程奖励模型（PRM）的概念被引入，作为一种更细粒度和透明的方法。DeepMind提出了在监督最终结果的同时监督中间推理步骤的想法，允许在推理过程中提供更详细的反馈 [Uesato等人，2022]。这项研究为后续过程验证的发展奠定了基础。另一方面，OpenAI的工作 [Lightman等人，2023] 通过后续研究继续这一趋势，强调通过提供高质量的人工标注过程监督数据集（即PRM800K）来验证每个中间步骤，该数据集在我们的工作中得到了丰富。

同样，Li等人 [2022] 强调了将验证器模型与多数投票方案集成的实际应用。该方法使用验证器仔细检查每个推理步骤，同时结合多数投票以提高最终结果的可靠性。此外，Yu等人 [2024] 引入了另一种使用强化学习来增强LLMs规划和推理过程的方法，提供了结果和过程监督技术的混合。除了传统的基于标量的奖励模型外，最近提出的生成奖励模型（GenRM）Zhang等人 [2024] 引起了广泛关注，因为验证器和生成器可以以更密集的基于文本的方式进行交互。当奖励模型监控策略时，它不仅输出每个答案的分数，还详细说明错误的原因。最近研究中的这一趋势反映了向更复杂的过程监督方法的广泛转变，这在本项目中得到了全面覆盖。

**数据获取。** PRM的数据获取问题已经显著发展，重点是自动化提取逐步推理数据，这对于训练能够处理复杂推理任务的模型至关重要。STaR技术Zelikman等人 [2022] 提出了一种新颖的自学推理方法，模型生成并引导自己的推理过程以进行进一步训练，从而在没有大量标注数据集的情况下提高推理能力。在STaR奠定的基础上，Zelikman等人 [2024] 展示了这些技术如何可以推广到数学问题解决等特定领域之外。通过将推理过程扩展到任意任务并将该方法整合到预训练中，Quiet-STaR突出了自动化过程监督在各种任务中的多功能性，标志着推理任务数据获取扩展的重要一步。此外，Luo等人 [2024] 代表了该领域的最新进展，特别关注数学推理。这项工作改进了自动化数据获取的方法，使该过程更加稳健，适用于日益复杂的问题解决场景。此外，Wang等人 将自动过程监督的概念更进一步，提出了一种不依赖人工标注数据的实用解决方案。最后，Wang等人 的实证结果通过测试其在编码任务中的适用性，展示了**过程监督**可以有效地由模型本身诱导。这些工作强调了自动化数据获取方法的日益依赖，其中模型能够提取和验证其自我推理过程。为了促进这一方向的研究，我们将生成的数据集和代码公开提供。

总之，OpenAI的o1等模型的高级推理严重依赖于仔细的数据选择、复杂的PRM训练和增强的解码方法。基于树的搜索、强化学习和步骤感知验证器等方法使这些模型能够处理更复杂的任务。随着研究的进展，LLMs有望进一步增强其自主推理、规划和问题解决能力。我们的项目旨在作为一个起点，透明地调查和评估推理时计算的潜力。

![Refer to caption](https://arxiv.org/html/2410.09671v1/x2.png)

图2：在这个MDP公式中，LLM的任务是逐步生成推理步骤和问题的最终答案。LLM策略通过生成标记来操作，这些标记形成更高级别的推理结构。状态表示到目前为止的推理步骤序列，动作对应于选择新的推理步骤或最终答案。LLM策略管理动作的选择，过程奖励模型（PRM）提供关于推理步骤和最终答案质量的反馈。通过优化策略以最大化奖励，PRM可以引导LLM生成准确且有意义的推理过程。

## 3 _OpenR_ LLM推理框架

为了在问答或问题解决等任务中建模推理过程，我们使用 $Q\to\{R\}\to A$ 序列来构建推理任务，其中：

* *$Q$ 表示启动推理过程的问题或提示；
* *$R$ 表示模型生成的中间推理步骤序列，以构建解决方案；
* *$A$ 表示推理步骤后生成的最终答案或解决方案。

这种结构允许LLM生成一系列推理步骤，逻辑上将问题 $Q$ 连接到最终答案 $A$。我们可以将推理过程定义为马尔可夫决策过程（MDP）。MDP表示提供了一个灵活的框架来建模推理。它允许模型逐步生成顺序推理步骤，同时通过在每个步骤采样多个路径来探索替代推理轨迹，从而创建多样化的解决方案，形成一个全面且多功能的推理过程。

在MDP中（如图2所示），LLM策略通过生成标记来操作，这些标记组合形成更高级别的推理结构。状态表示到目前为止采取的推理步骤序列，而动作涉及选择下一个推理步骤或最终答案。LLM策略生成这些动作选择，过程奖励模型（PRM） 提供关于推理步骤和最终答案质量的反馈。通过优化策略以最大化奖励，PRM引导LLM生成准确且有意义的推理过程。

### 系统设计

过程奖励模型（PRM）在增强LLM策略方面发挥着关键作用，主要体现在两个方面。首先，在训练期间，PRM通过策略优化技术（如图3所示的策略迭代）改进LLM策略。其次，在解码阶段，PRM引导LLM的搜索过程，引导推理朝着更有效的结果发展（如图3所示）。正如我们接下来将展示的，LLM策略还有助于识别缺失的中间推理步骤，从而进一步训练和完善PRM。如图3所示，这种迭代交互使LLM和PRM能够不断解锁彼此的潜力，以实现改进的推理。

![Refer to caption](https://arxiv.org/html/2410.09671v1/x3.png)

图3：_OpenR_ LLM推理框架。将PRM的价值函数与LLM的策略生成相结合，确保引导和控制的结果。在训练期间，LLM策略生成的生成和PRM提供的评估相互加强，导致两个组件的持续自我改进和完善。

### 数据增强

对于大语言模型（LLMs）提供的解决方案或思维链，我们使用更精确和细粒度的反馈，而不仅仅依赖于最终答案。我们收集用于过程监督的数据，为给定解决方案提供逐步反馈。形式上，PRM计算 $p_{t}=\text{PRM}([q,x_{1:t-1}],x_{t})$，其中 $x_{1:t}=[x_{1},\cdots,x_{t}]$ 表示解决方案的前 $t$ 步。与结果奖励模型（ORMs）相比，这种方法提供了更精确和细粒度的反馈，因为它识别了问题解决过程中的确切错误位置 [Lighthman等人，2023]。

MATH-APS。我们通过自动生成合成样本来增强数据。除了依赖昂贵人工标注且难以扩展的PRM800k数据集 [Lighthman等人，2023] 外，我们引入了一个基于MATH [Hendrycks等人，2021] 的新数据集MATH-APS，使用OmegaPRM [Luo等人，2024] 等自动化方法。这种方法减少了对昂贵人工注释的依赖，实现了更可扩展的数据收集。OmegaPRM、Math-Shepherd [Wang等人，] 和MiPS [Wang等人，] 等自动方法高效地收集高质量的过程监督数据。虽然Math-Shepherd和MiPS提供了自动注释的过程监督，但它们需要大量的策略调用，使其计算成本高昂。OmegaPRM通过迭代划分解决方案、执行rollout并识别模型解决方案中的第一个错误步骤来改进这一过程。

我们遵循OmegaPRM [Luo等人，2024] 并通过使用LLMs构建状态-动作树来收集PRM训练示例。对于每个问题，构建一棵树，其中每个节点包含问题 $q$、解决方案前缀 $s$ 和所有先前的rollout $\{(s,r_{i})\}_{i=1}^{k}$（$r_{i}$ 表示第 $i$ 个rollout）。每条边表示从节点开始的单个步骤或一系列步骤。对于每个节点，我们计算蒙特卡罗估计 $MC(s)$ 和价值函数 $Q(s,r)$ 以指导在树遍历期间选择rollout。价值函数定义为：$Q(s,r)=\alpha\cdot\frac{1}{1-MC(s)}\cdot\beta\cdot\frac{\text{len}(r)}{1-\alpha}$，其中 $\alpha$、$\beta$ 和 $L$ 是常数，$\text{len}(r)$ 是rollout的长度。我们还计算探索项：$U(s)=c_{\text{puct}}\cdot\frac{\sqrt{\sum_{i}N(s_{i})}}{1+N(s)}$，其中 $N(s)$ 是访问计数，$c_{\text{puct}}$ 是鼓励探索的常数。在选择阶段，使用PUCT算法的变体选择rollout：$(s,r)=\arg\max_{(s,r)}[Q(s,r)+U(s)]$。这种启发式方法选择最有价值的rollout。然后使用二分搜索识别所选rollout中的第一个错误，并将 $0<MC(s)$ 的rollout添加到候选池中。所有在第一个错误之前的位置成为进一步探索的新状态。

### PRM的监督训练

在PRM中，目标是确定解决方案过程的序列当前是否在正确的轨道上，因此它应该输出一个正确性的二元指标。具体来说，我们给定一个问题 $q$ 和一个解决方案步骤序列 $x_{1}\to x_{t}$，分配一个介于0和1之间的分数 $y_{t}$。该分数表示当前问题解决过程的正确性。因此，问题被重新定义为 $y_{t}=\text{PRM}(q,x_{1},x_{2},\cdots,x_{t})$，可以将其视为二元分类任务。PRM通过在LLM上进行监督微调进行训练，正确/错误的区分作为分类标签。然后我们使用LLM预测步骤标记的下一个标记。

Math-psa。PRM通过在LLM上进行监督微调进行训练，正确/错误的区分作为分类标签。我们使用PRM800K [Lighthman等人，2023]、Math-Shepherd [Wang等人，] 和我们的MATH-APS数据集（见第3.2节）训练了一个名为Math-psa的PRM。这些数据集分为三个组件：_问题_、_过程_和_标签_。输入由_问题_和_过程_的串联组成。在_过程_中，解决方案被分为多个步骤，每个步骤由一个特殊的步骤标记（"whwhwhw"）分隔，标记每个步骤的结束，PRM可以在这些位置进行预测。_标签_是对整个过程的分类，每个步骤根据解决方案的正确性标记为“+”或“-”。

在训练期间，数据作为下一个标记预测任务输入到LLM。模型被训练为在每个步骤标记后立即预测一个正面或负面标记。如数据部分所述，输入由_问题_与_过程_的串联组成，步骤标记分隔过程中的步骤。标签被分配为在步骤标记的位置，标签是正面或负面标记，而所有其他位置在损失计算期间被忽略。注意力掩码设置为1，除了步骤标记位置，确保在训练期间，LLM仅关注输入序列 $(q,x_{1}\to x_{t})$，而不关注步骤标记本身。

### LLM的策略学习

我们将数学问题转化为语言增强的马尔可夫决策过程（MDP） $\mathcal{M}=(\mathcal{V},\mathcal{S},\mathcal{A},\mathcal{T},R,\gamma)$(Van Otterlo和Wiering，2012；Carta等人，2023)。给定 $\mathcal{V}$ 词汇表和 $w\in\mathcal{V}$ 标记，$\mathcal{A}\subset\mathcal{V}^{N}$，$\mathcal{S}\subset\mathcal{V}^{N}$ 分别是动作和状态空间，即动作和状态是标记序列。$\mathcal{T}:\mathcal{S}\times\mathcal{A}\mapsto\mathcal{S}$ 是状态转移函数。$R:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}$ 是响应每个动作的奖励函数，$\gamma$ 是通常小于1的折扣因子。初始状态 $s_{0}$ 表示数学问题解决场景中的给定问题。语言模型接收此输入并生成一个中间推理步骤，表示为动作 $a_{0}$。然后，此动作 $a_{0}$ 与初始问题 $s_{0}$ 连接以形成后续状态 $s_{1}$，用于推断下一个动作 $a_{1}$，此迭代过程继续，每个状态-动作对依次通知下一个状态，即 $\mathcal{T}:s_{t+1}=\{s_{t},a_{t}\}$ 在时间步 $t$，直到模型得出最终答案。在推断每个动作 $a_{t}$ 后，模型从训练良好的PRM接收奖励信号 $r^{PRM}_{t}=R(s_{t},a_{t})$。遵循此过程，轨迹的最大时间步为 $T$，代理获得折扣累积回报 $R^{\gamma}=\sum_{t=0}^{T}\gamma^{t}r^{PRM}_{t}$，RL算法的目标是最大化此回报。我们相应地实现此MDP作为强化学习环境，如OpenAI的Gym。在这些环境中，数学问题被呈现为任务，模型采取顺序动作来解决问题，对正确动作给予奖励，对错误动作给予惩罚，从而使模型能够通过试错迭代学习和完善其问题解决策略，最终增强其数学推理技能。

RL训练。使用强化学习训练LLMs通常涉及近端策略优化（PPO）(Schulman等人，2017)，以使生成的语言输出与期望的动作对齐。这种方法通过强化生成既上下文准确又与预定义目标一致的响应，有效地将语言理解与战略规划联系起来。我们提供传统的PPO和PPO的高效变体，即组相对策略优化（GRPO）(Shao等人，2024)。这两者之间的主要区别在于它们对优势值估计的方法。具体来说，PPO利用网络来近似状态价值函数，利用广义优势估计（GAE）技术 (Schulman等人，2015) 来推导优势。相比之下，GRPO通过直接使用归一化的奖励信号来估计动作的优势，即 $A(s_{t},a_{t})=\frac{r^{PRM}_{t}-\text{mean}(r^{PRM}_{t})}{\text{std}(r^{PRM})}$。与PPO相比，GRPO绕过了额外批评网络的需求，并减少了训练期间消耗的资源，然而，它更强调PRM的稳定性。

### 解码：推理时间引导搜索和规划

遵循Snell等人 [2024]，我们使用PRM评估每个解决方案步骤的准确性。一旦训练出高质量的过程奖励模型，我们将其与语言模型集成到解码过程中，实现引导搜索和跨多个生成的评分或投票。

为了使用PRM作为验证器，我们定义了一种评估LLM生成解决方案正确性的方法。具体来说，我们将单个步骤的分数 $\{r^{PRM}_{t}\}_{t=0}^{T}$ 映射到最终分数。遵循Lightman等人 [2023] 和Snell等人 [2024] 概述的策略，我们采用两种方法：

* *_PRM-Min_：选择所有分数中的最小值，即 $v=\min\{r^{PRM}_{t}\}_{t=0}^{T}$。
* *_PRM-Last_：选择最后一步的分数作为最终分数，即 $v=r^{PRM}_{T}$。在Snell等人 [2024] 中，这种策略与_PR
* M-Min_ 表现相当。

  一旦通过扩展测试时计算生成多个答案，我们需要根据它们的分数选择最佳答案的策略。我们采用了Feng等人 [2024] 的三种策略：

  * **_多数投票_**：使用多数投票聚合答案：$f^{*}=\arg\max_{f}\sum_{\mathbf{y}^{j}}\mathbbm{1}_{\text{final\_ans}(\mathbf{y}^{j})=f}$，其中 $\mathbbm{1}$ 是指示函数。
  * **_RM-Max_**：给定一个结果奖励模型，聚合可以选择具有最大最终奖励的答案 $f$，即 $f^{*}=\text{final\_ans}(\arg\max_{y^{j}}v(\mathbf{y}^{j}|\mathbf{x}))$。
  * **_RM-Vote_**：给定一个结果奖励模型，聚合可以选择具有奖励总和的答案 $f$，即 $f^{*}=\arg\max_{f}\sum_{y^{j}:\text{final\_ans}(y^{j})=f}v(\mathbf{y}^{j}|\mathbf{x})$。

  结合这些策略，我们可以定义多答案加权方法，例如 _PRM-Last-Max_，即使用 _PRM-Last_ 与 _RM-Max_。

  我们的框架允许我们选择各种搜索算法——例如束搜索、最佳N选择等——每种算法都有独特的优势，具体取决于PRM的质量。复杂的搜索算法可能在更困难的任务上表现更好，而更简单的方法（如最佳N）通常对于不太具有挑战性的情况表现足够好（Snell等人，2024）。

  我们主要采用两种策略：

  * **_最佳N_**：给定一个基础模型，最佳N采样方法并行生成 $N$ 个输出，并根据使用PRM学习到的分数选择得分最高的答案。这种方法类似于之前利用验证器或奖励模型的工作（Cobbe等人，2021b；Lightman等人，2023）。虽然简单，但它是一个有效的基线，利用测试时计算来提高LLM的性能。PRM可以作为密集验证器（Lightman等人，2023；Wang等人，2024b），直观上，提供强信号可以带来更好的结果。由于可以为基础模型的解决方案获得密集奖励，我们需要考虑如何最好地利用这种反馈来优化测试时计算。
  * **_束搜索_**：LLM为第一步生成 $N$ 个不同的输出，然后使用PRM对其进行评分。这些 $N$ 个输出使用PRM进行评分，并保留前 $N/m$（$\frac{N}{m}\in\mathbb{Z}$）个得分最高的输出。然后我们仅保留这些 $N/m$ 个输出用于当前步骤。对于每个输出，我们通过基础模型采样 $M$ 个潜在的下一步，返回到 $N$ 个总输出。重复此过程：对新的候选进行评分、过滤并采样后续步骤。PRM的分数是引导此搜索的核心。与最佳N方法一样，我们使用最后投票和多数投票策略来聚合分数，后者依赖于PRM的分数总和（Wang等人，2022）。

  我们将继续开发更复杂的推理时间引导搜索解码方法，例如蒙特卡罗树搜索（MCTS），该方法已在OpenR的代码库中涵盖，以及其他方法，如顺序修订（Snell等人，2024）。

  ## 4 实验

  为了展示我们的OpenR框架的能力，我们在大语言模型推理和训练方面展示了定量结果。我们使用MATH数据集（Hendrycks等人，2021）评估我们的开放框架，该数据集包含广泛的高中竞赛级数学问题。这使其成为测试推理技能的理想基准。为了确保与之前工作的公平比较并减少过拟合，我们遵循Lightman等人 [2023] 并使用500个问题的子集进行评估，称为MATH500，其中问题随机采样。

  ### 扩展LLM测试时计算

  **设置**。我们的PRM模型Math-psa是从Qwen2.5-Math-7B-Instruct（Yang等人，2024）模型微调而来，使用了多个数据集，包括PRM500K（Lightman等人，2023）、Math-Shepherd（Wang等人，2024a）和我们的MATH-APS数据集（最初从Qwen2.5 Math模型收集了超过50万的状态-值对，经过清理和预处理后减少到约15万对）。同时，我们还使用Math-Shepherd PRM进行比较实验。遵循Snell等人 [2024]，我们采用最佳N和束搜索算法进行测试时计算。我们在预定义的标记生成预算中比较了多种测试时计算方案。在不同的聚合策略中，我们选择PRM-Last作为代表。LLM推理服务器使用FastChat（Zheng等人，2023）实现。

  **结果**。图4a比较了这些搜索和投票方法在推理过程中的表现。y轴表示在MATH500数据集上的测试准确率，而x轴显示了生成预算（每个问题的平均标记数），反映了每个问题的计算努力或标记使用量。图表表明，最佳N和束搜索方法显著优于多数投票，尤其是在生成预算增加时，显示出与之前发现（Snell等人，2024）相似的模式。在低测试时计算预算（$<2^{4}$）下，最佳N方法表现出比束搜索更好的性能，而束搜索在给定更高预算时可以匹配性能，甚至在使用PRM-Last策略时，在预算大于 $2^{5}$ 时超越最佳N。

  另一方面，图4b研究了不同PRM对测试时计算的影响。我们比较了使用不同PRM引导的最佳N方法的性能。图表显示，我们的PRM（Math-aps）在所有测试的计算预算中都能达到最高的测试准确率。这确实验证了我们的PRM训练管道可以提供有效的过程监督学习。

  ### LLM的在线策略学习

  **设置**。在策略学习实验中，我们使用Qwen2.5-1.5B-Math-Instruct模型作为训练的策略模型，Math-Shepherd模型（Wang等人，2024a）作为PRM，在RL期间提供反馈。除了MATH500数据集外，我们还测试了模型在特定数学问题上的表现：“196有多少个正整数的约数？”最终答案为“9”。

  **结果**。图5展示了使用过程奖励模型（PRM）的强化学习（RL）算法在单个数学问题上获得的奖励。随着时间的推移，奖励稳步增加，显示出持续改进，性能在大约6小时的训练后趋于稳定。这表明随着训练的进行，模型在解决特定问题方面变得更加准确。

  在MATH500数据集上，结果显示出更多的奖励波动。这表明，由于数据集中问题的多样性，使用PRM的PPO算法面临更复杂的挑战。尽管奖励随着时间的推移而增加，但其波动性突显了在更广泛问题上泛化的难度。这表明未来需要进一步改进算法，以增强其适应多样化问题集的能力。

  ### 案例研究

  **PRM的比较**。图6和图7比较了Math-psa PRM和Math-Shepherd PRM对给定推理步骤的响应。每个步骤的PRM分数表示为“[Math-psa（我们的）分数，Math-Shepherd PRM分数）”。图6显示，Math-psa（我们的）能够为推理步骤分配更合理的分数。特别是在步骤4和步骤5，Math-psa对输出的正确性表现出更高的信心，而Math-Shepherd PRM对给定的正确响应似乎不太确定。图7显示了导致错误答案的推理过程。它显示，Math-psa PRM为步骤3分配了较低的分数，因为水平位移确实会影响多项式根的总和。Math-psa PRM进一步阻止了步骤4，而Math-Shepherd PRM仍然保持高信心。

  **策略学习前后LLM的比较**。图8和图9分别展示了策略学习前后的模型案例。图8展示了策略学习前的结果，收集自Qwen2.5-Math-1.5B-Instruct的原始策略。在这个例子中，模型错误地将三角形的周长用作边长，导致错误答案。图9使用与图8相同的问题，展示了应用Math-psa PRM后的改进。Math-psa PRM能够纠正错误，表明使用设计良好的PRM进行RL训练过程可以提高推理准确性。

  **不同搜索方法的比较**。图10、11和12展示了不同测试时计算方法对推理输出的影响的示例。这三个问答会话显示，最佳N和束搜索推理都准确地解决了问题，彻底理解了问题并提供了正确的计算。相比之下，CoT推理误解了问题，导致从步骤2开始的计算错误。这表明，最佳N和束搜索推理受益于更大的搜索空间，使它们能够探索更多的推理路径并收敛到正确的解决方案。

  ## 5 结论

  在这项工作中，我们介绍了 _OpenR_，一个开源框架，旨在通过整合测试时计算、强化学习和过程监督来增强大语言模型（LLMs）的推理能力。我们的框架为在LLMs中进行推理任务实验提供了一个开放且易于访问的平台，展示了如何结合测试时计算、搜索算法和过程奖励模型（PRMs）来提高推理性能。

  我们相信 _OpenR_ 将成为研究社区的宝贵资源，提供一个全面的平台，以进一步探索LLMs中的推理。通过公开我们的模型、数据和代码，我们旨在加速人工智能推理的进展，促进该领域的合作与创新。在未来的工作中，我们计划扩展框架以支持更广泛的推理任务，并优化推理时计算，以实现更高效和可扩展的推理模型。

  ## 6 局限性

  **实验规模有限**：由于对大规模计算基础设施的访问受限，我们的评估是在相对较小的模型和数据集上进行的。

  **模型大小**：我们主要使用中等大小的模型作为基础LLM。虽然这些模型展示了显著的提升，但更大的模型可能会进一步增强推理能力。

  **过程监督数据有限**：虽然我们利用了PRM800K、Math-Shepherd和我们生成的MATH-APS数据集进行训练，但过程监督数据的规模和多样性仍然有限。

  未来的工作可以集中在扩大实验规模、扩展训练数据集以及在更广泛的模型和领域中进行测试，以解锁推理性能的进一步改进。

  ## 参考文献

  （此处省略参考文献部分，保持与原文一致）
