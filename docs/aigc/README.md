# AIGC

## ChatGPT

- [chatGPT](aigc/chatgpt/chatGPT.md)
- [Langchain](aigc/chatgpt/langchain.md)
- [LLM Challenges](aigc/chatgpt/llm_challenges.md)
- [Useful Chatgpt Prompt](aigc/chatgpt/useful_prompt.md)
- [文本大语言模型的聊天模板入门指南](aigc/chatgpt/chat_template.md)

## LLM Pretrain

- [T5 模型](aigc/llm_pretrain/T5model.md)
- [NLP 任务分类](aigc/llm_pretrain/nlptasks.md)
- [Transformer 中的位置编码](aigc/llm_pretrain/pe.md)
- [旋转式位置编码(Rotary Position Embedding)](aigc/llm_pretrain/rope.md)

## LLM Finetune

- [大型语言模型 (LLM) 微调方法](aigc/llm_finetune/finetune_llm.md)
- [Low-Rank Adaptation (LoRA)](aigc/llm_finetune/lora_llm.md)


## LLM Datasets

- [LLM Instruction 数据集](aigc/llm_dataset/instruction_dataset.md)
- [LLM Preference 数据集](aigc/llm_dataset/preference_dataset.md)
- [LLM 行业数据集](aigc/llm_dataset/prompt_dataset.md)


## LLM Inference

- [解码策略基础](aigc/llm_inference/解码策略基础.md)
- [解码策略高级方法](aigc/llm_inference/解码策略高级方法.md)
- [KVCaching机制详解](aigc/llm_inference/KVCaching机制详解.md)
- [PagedAttention 原理详解](aigc/llm_inference/PageAttention.md)


## LLM RLHF Framework

- [Ray 的核心概念](aigc/llm_rlhf/Ray核心概念.md)
- [RLHF 中的 PPO 代码拆解](aigc/llm_rlhf/rlhf_with_ppo.md)
- [RLHF 训练框架 OpenRLHF](aigc/llm_rlhf/OpenRLHF.md)
- [RLHF 训练框架 NeMo-Aligner](aigc/llm_rlhf/NeMo-Aligner.md)
- [RLHF 训练框架 HybridFlow](aigc/llm_rlhf/HybridFlow.md)
- [RLHF 训练框架 DeepSpeedChat](aigc/llm_rlhf/DeepSpeedChat.md)
- [RLHF 训练框架 OpenR](aigc/llm_rlhf/OpenR.md)
- [RLHF 训练框架 OpenRLHF 源码解读](aigc/llm_rlhf/OpenRLHF源码解读.md)
- [RLHF 训练框架 VeRL 源码解读](aigc/llm_rlhf/Verl框架.md)
- [RLHF 训练框架 VeRL 参数配置指南](aigc/llm_rlhf/Verl参数配置.md)
- [从 Ray 角度分析 OpenRLHF 和 Verl 的工程设计](aigc/llm_rlhf/Ray_OpenRLHF_Verl.md)


## LLM RLHF Algorithm and Paper

- [RLHF 中的 Policy Gradient Algorithms](aigc/llm_rlhf/rlhf_policy_gradient.md)
- [理解 RLHF](aigc/llm_rlhf/rlhf_advance.md)
- [Chip Huyen 对 RLHF 的分析](aigc/llm_rlhf/rlhf_chiphuyen.md)
- [RLHF 相关知识整理](aigc/llm_rlhf/rlhf_overview.md)
- [直接偏好优化 (DPO)](aigc/llm_rlhf/rlhf_dpo.md)
- [直接偏好优化 (DPO) 推导](aigc/llm_rlhf/rlhf_dpo_notes.md)
- [Kahneman-Tversky-Optimization (KTO)](aigc/llm_rlhf/rlhf_kto.md)
- [RLOO](aigc/llm_rlhf/RLOO.md)
- [DeepSeek-R1：通过强化学习激励 LLMs 的推理能力](aigc/llm_rlhf/DeepSeek-R1.md)
- [Kimi k1.5：使用 LLM 扩展强化学习](aigc/llm_rlhf/KimiK1.5.md)
- [DAPO: 一个开源的大规模 LLM 强化学习系统](aigc/llm_rlhf/DAPO.md)
- [深入理解 R1-Zero 类训练：一个批判性视角](aigc/llm_rlhf/DR.GRPO.md)
- [DeepScaleR：通过扩展强化学习超越 o1](aigc/llm_rlhf/deepscaler.md)
- [REINFORCE++：一种简单高效的大型语言模型对齐方法](aigc/llm_rlhf/REINFORCE++.md)
- [ChatGPT O1 Reasoning](aigc/llm_rlhf/chatgpt_O1.md)
- [KL 散度的近似计算](aigc/llm_rlhf/KL-Approximate.md)
- [过程奖励模型（Process Reward Model）](aigc/llm_rlhf/PRM.md)
- [数学推理中过程奖励模型的开发经验](aigc/llm_rlhf/PRM_Reasoning.md)
- [ReFT: 通过强化微调提升推理能力](aigc/llm_rlhf/ReFT.md)
- [拒绝采样（Reject Sampling）在 RLHF 中的应用](aigc/llm_rlhf/RejectSampling.md)
- [ReST-MCTS：通过过程奖励引导的树搜索实现 LLM 自训练](aigc/llm_rlhf/ReST-MCTS.md)
- [rStar-Math：小型语言模型通过自我进化的深度思考掌握数学推理](aigc/llm_rlhf/rStar-Math.md)

## LLM Agent

- [Autonomous AI Agents](aigc/llm_agent/llm_agent0.md)
- [LLM 赋能 Agent](aigc/llm_agent/llm_agent1.md)
- [AutoGPT 与 LLM Agent 解析](aigc/llm_agent/llm_agent2.md)
- [Agent 应用: MarkAgent](aigc/llm_agent/market_agent.md)
- [Agent 应用: SightPro](aigc/llm_agent/sightpro.md)
- [LLM Playing StarCraftII 1](aigc/llm_agent/llm_starcraft_1.md)
- [LLM Playing StarCraftII 2](aigc/llm_agent/llm_starcraft_2.md)
- [LLM Playing StarCraftII 3](aigc/llm_agent/llm_starcraft_3.md)

## LLM Evaluation

- [LLM 评测体系](aigc/llm_eval/LLM评测体系.md)
- [LLM 评估工具](aigc/llm_eval/LLM评估工具.md)
- [LLM 评估指标](aigc/llm_eval/LLM评估指标.md)

## 多模态

- [比 LLM 更重要的多模态学习](aigc/multimodal/overview.md)
- [多模态和多模态模型](aigc/multimodal/lmm.md)
- [声音生成文本](aigc/multimodal/video2text.md)
- [文生视频: 任务、挑战及现状](aigc/multimodal/text2video.md)
- [ALBEF](aigc/multimodal/albef.md)
- [BLIP](aigc/multimodal/blip.md)
- [BLIP-2](aigc/multimodal/blip2.md)
- [CoCa](aigc/multimodal/coca.md)
- [Flamingo](aigc/multimodal/flamingo.md)

## MMEngine

- [MMEngine](aigc/train_engine/engine.md)
- [Runner](aigc/train_engine/runner.md)
- [Model](aigc/train_engine/model.md)
- [Hooks](aigc/train_engine/hooks.md)
- [Logger](aigc/train_engine/logger.md)

## 分布式训练

- [大模型训练技术挑战](aigc/llm_recipes/llm_training.md)
- [大模型训练技巧](aigc/llm_recipes/recipes.md)
- [Pytorch 分布式使用教程](aigc/llm_recipes/pytorch_distributed.md)
- [DeepSpeed教程](aigc/llm_recipes/DeepSpeed教程.md)
- [ZeRO 技术原理 (上)](aigc/llm_recipes/zero-optimizer.md)
- [ZeRO 技术原理 (下)](aigc/llm_recipes/deepspeed_zero.md)
- [Pytorch FSDP 介绍](aigc/llm_recipes/fsdp.md)
- [TorchTitan 原生大模型训练](aigc/llm_recipes/torch_titan.md)

## 扩散模型

- [扩散模型理论第一课](aigc/diffusion/theory.md)
- [什么是扩散模型: A Review](aigc/diffusion/summary.md)
- [扩散模型的工作原理：从零开始的数学](aigc/diffusion/math101.md)
- [深入理解扩散模型](aigc/diffusion/deepdive.md)
- [扩散模型: 重参数化技巧](aigc/diffusion/reparameterization.md)
- [基于分数的生成模型](aigc/diffusion/score_model.md)
- [From Autoencoder to Beta-VAE](aigc/diffusion/vae_model.md)

## 量化压缩

- [8-bit 优化器中文解读](aigc/quantization/int8_opt.md)
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](aigc/quantization/llm_int8.md)
- [Transformer 中的 Bitsandbytes 集成](aigc/quantization/hf-bitsandbytes-integration.md)
- [QLora: Efficient Finetuning of Quantized LLMs](aigc/quantization/qlora.md)
- [QLora 源码分析](aigc/quantization/qlora_usage.md)

## AI News and Blogs

- [Top AI Blogs and Websites](aigc/ai-news.md)

## Tools

- [如何快速下载 Huggingface 模型](aigc/hf_download.md)

## 从 GPU 到华为 NPU

- [从 0 开始安装 Ascend Pytorch](aigc/ascend/从0开始安装AscendPytorch.md)
- [Ascend_Pytorch 适配方案介绍](aigc/ascend/Ascend_Pytorch适配方案介绍.md)
- [LLamaFactory NPU Docker 镜像配置](aigc/ascend/llamafactory_docker.md)
- [Mindspeed-RL框架使用指南](aigc/ascend/Mindspeed-RL框架使用指南.md)
