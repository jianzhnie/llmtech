# è§£ç ç­–ç•¥

è§£ç ç­–ç•¥å†³å®šäº†æ¨¡å‹å¦‚ä½•é€‰æ‹©ä¸‹ä¸€ä¸ªç”Ÿæˆçš„tokenã€‚å­˜åœ¨è®¸å¤šç±»å‹çš„è§£ç ç­–ç•¥ï¼Œé€‰æ‹©åˆé€‚çš„ç­–ç•¥å¯¹ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡æœ‰ç€æ˜¾è‘—å½±å“ã€‚

æœ¬æ–‡å°†å¸®åŠ©æ‚¨äº†è§£Transformersä¸­å¯ç”¨çš„ä¸åŒè§£ç ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•å’Œä½•æ—¶ä½¿ç”¨å®ƒä»¬ã€‚

## åŸºç¡€è§£ç æ–¹æ³•

è¿™äº›æ˜¯ç»è¿‡éªŒè¯çš„è§£ç æ–¹æ³•ï¼Œåº”ä½œä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„èµ·ç‚¹ã€‚

### è´ªå©ªæœç´¢ï¼ˆGreedy Searchï¼‰

è´ªå©ªæœç´¢æ˜¯é»˜è®¤çš„è§£ç ç­–ç•¥ã€‚å®ƒåœ¨æ¯ä¸€æ­¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªtokenã€‚é™¤éåœ¨`GenerationConfig` ä¸­æŒ‡å®šï¼Œå¦åˆ™æ­¤ç­–ç•¥æœ€å¤šç”Ÿæˆ20ä¸ªæ–°tokenã€‚

è´ªå©ªæœç´¢é€‚ç”¨äºè¾“å‡ºè¾ƒçŸ­ä¸”ä¸éœ€è¦åˆ›é€ åŠ›çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿæˆè¾ƒé•¿åºåˆ—æ—¶ï¼Œå®ƒå¾ˆå®¹æ˜“é™·å…¥é‡å¤ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# æ˜¾å¼è®¾ç½®ä¸ºé»˜è®¤é•¿åº¦ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦ä¸º4096
outputs = model.generate(**inputs, max_new_tokens=20)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```python
Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing
```

### é‡‡æ ·ï¼ˆSamplingï¼‰

é‡‡æ ·ï¼ˆæˆ–å¤šé¡¹å¼é‡‡æ ·ï¼‰æ ¹æ®æ•´ä¸ªæ¨¡å‹è¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©tokenï¼ˆè€Œä¸æ˜¯åƒè´ªå©ªæœç´¢é‚£æ ·é€‰æ‹©æœ€æœ‰å¯èƒ½çš„tokenï¼‰ã€‚è¿™æ„å‘³ç€ä»»ä½•å…·æœ‰éé›¶æ¦‚ç‡çš„tokenéƒ½æœ‰æœºä¼šè¢«é€‰ä¸­ã€‚é‡‡æ ·ç­–ç•¥å¯ä»¥å‡å°‘é‡å¤ï¼Œå¹¶ç”Ÿæˆæ›´å…·åˆ›é€ æ€§å’Œå¤šæ ·æ€§çš„è¾“å‡ºã€‚

é€šè¿‡è®¾ç½®`do_sample=True`å’Œ`num_beams=1`å¯ç”¨å¤šé¡¹å¼é‡‡æ ·ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦ä¸º4096
outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```
Hugging Face is an open-source company ğŸ¤—
We are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.
```

### æŸæœç´¢ï¼ˆBeam Searchï¼‰

æŸæœç´¢åœ¨æ¯ä¸ªæ—¶é—´æ­¥è·Ÿè¸ªå¤šä¸ªç”Ÿæˆåºåˆ—ï¼ˆæŸï¼‰ã€‚ç»è¿‡ä¸€å®šæ­¥æ•°åï¼Œå®ƒé€‰æ‹©æ•´ä½“æ¦‚ç‡æœ€é«˜çš„åºåˆ—ã€‚ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œè¿™ç§ç­–ç•¥å¯ä»¥â€œå‘å‰çœ‹â€ï¼Œå³ä½¿åˆå§‹tokençš„æ¦‚ç‡è¾ƒä½ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©æ•´ä½“æ¦‚ç‡æ›´é«˜çš„åºåˆ—ã€‚å®ƒæœ€é€‚åˆè¾“å…¥å¯¼å‘çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æè¿°å›¾åƒæˆ–è¯­éŸ³è¯†åˆ«ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨æŸæœç´¢ä¸­ä½¿ç”¨`do_sample=True`åœ¨æ¯ä¸€æ­¥è¿›è¡Œé‡‡æ ·ï¼Œä½†æŸæœç´¢ä»ä¼šåœ¨å„æ­¥ä¹‹é—´è´ªå©ªåœ°å‰”é™¤ä½æ¦‚ç‡åºåˆ—ã€‚

é€šè¿‡è®¾ç½®`num_beams`å‚æ•°ï¼ˆåº”å¤§äº1ï¼Œå¦åˆ™ç­‰åŒäºè´ªå©ªæœç´¢ï¼‰å¯ç”¨æŸæœç´¢ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦ä¸º4096
outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```python
['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']
```

## é«˜çº§è§£ç æ–¹æ³•

é«˜çº§è§£ç æ–¹æ³•æ—¨åœ¨è§£å†³ç‰¹å®šçš„ç”Ÿæˆè´¨é‡é—®é¢˜ï¼ˆä¾‹å¦‚é‡å¤ï¼‰æˆ–åœ¨æŸäº›æƒ…å†µä¸‹æé«˜ç”Ÿæˆååé‡ã€‚è¿™äº›æŠ€æœ¯è¾ƒä¸ºå¤æ‚ï¼Œå¯èƒ½å¹¶éé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ã€‚

### è¾…åŠ©è§£ç ï¼ˆSpeculative Decodingï¼‰

è¾…åŠ©è§£ç å¹¶ä¸æ˜¯ä¸€ç§æœç´¢æˆ–é‡‡æ ·ç­–ç•¥ã€‚ç›¸åï¼Œè¾…åŠ©è§£ç é€šè¿‡æ·»åŠ ä¸€ä¸ªè¾ƒå°çš„è¾…åŠ©æ¨¡å‹æ¥ç”Ÿæˆå€™é€‰tokenã€‚ä¸»æ¨¡å‹åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­éªŒè¯å€™é€‰tokenï¼Œä»è€ŒåŠ é€Ÿæ•´ä¸ªè§£ç è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå› ä¸ºç”Ÿæˆtokençš„æˆæœ¬è¾ƒé«˜ä¸”é€Ÿåº¦è¾ƒæ…¢ã€‚

ç›®å‰ï¼Œè¾…åŠ©è§£ç ä»…æ”¯æŒè´ªå©ªæœç´¢å’Œå¤šé¡¹å¼é‡‡æ ·ï¼Œä¸”ä¸æ”¯æŒæ‰¹é‡è¾“å…¥ã€‚

é€šè¿‡`assistant_model`å‚æ•°å¯ç”¨è¾…åŠ©è§£ç ã€‚å½“è¾…åŠ©æ¨¡å‹è¿œå°äºä¸»æ¨¡å‹æ—¶ï¼Œæ‚¨ä¼šå‘ç°é€Ÿåº¦æå‡æœ€ä¸ºæ˜¾è‘—ã€‚æ·»åŠ `do_sample=True`ä»¥å¯ç”¨å¸¦é‡é‡‡æ ·çš„tokenéªŒè¯ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
assistant_model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-135M")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt")

outputs = model.generate(**inputs, assistant_model=assistant_model)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```python
Hugging Face is an open-source company that provides a platform for developers to build and deploy machine
```

è¾…åŠ©è§£ç ä¹Ÿæ”¯æŒé€šè¿‡`assistant_model`å‚æ•°åœ¨Pipelineä¸­ä½¿ç”¨ã€‚

```python
from transformers import pipeline
import torch

pipe = pipeline(
    "text-generation",
    model="meta-llama/Llama-3.1-8B",
    assistant_model="meta-llama/Llama-3.2-1B",
    torch_dtype=torch.bfloat16
)
pipe_output = pipe("Once upon a time, ", max_new_tokens=50, do_sample=False)
pipe_output[0]["generated_text"]
```

### æç¤ºæŸ¥æ‰¾è§£ç ï¼ˆPrompt Lookup Decodingï¼‰

æç¤ºæŸ¥æ‰¾è§£ç æ˜¯è¾…åŠ©è§£ç çš„ä¸€ç§å˜ä½“ï¼Œå®ƒä½¿ç”¨é‡å çš„n-gramä½œä¸ºå€™é€‰tokenã€‚å®ƒé€‚ç”¨äºè¾“å…¥å¯¼å‘çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ‘˜è¦ã€‚

é€šè¿‡`prompt_lookup_num_tokens`å‚æ•°å¯ç”¨æç¤ºæŸ¥æ‰¾è§£ç ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-1.7B", torch_dtype=torch.float16).to("cuda")
assistant_model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-135M", torch_dtype=torch.float16).to("cuda")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=20, prompt_lookup_num_tokens=5)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```python
Hugging Face is an open-source company that provides a platform for developers to build and deploy machine learning models. It offers a variety of tools
```

### è‡ªæˆ‘è¾…åŠ©è§£ç ï¼ˆSelf-speculative Decodingï¼‰

ä½¿ç”¨language modeling heads çš„æ—©æœŸéšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œæœ‰æ•ˆåœ°è·³è¿‡ä¸€äº›å±‚ä»¥ç”Ÿæˆè´¨é‡è¾ƒä½çš„è¾“å‡ºã€‚è¯¥ä½è´¨é‡è¾“å‡ºç”¨ä½œè¾…åŠ©è¾“å‡ºï¼Œç„¶åé€šè¿‡å‰©ä½™å±‚åº”ç”¨è¾…åŠ©è§£ç æ¥ä¿®æ­£è¾“å‡ºã€‚è¿™ç§è‡ªæˆ‘è¾…åŠ©è§£ç æ–¹æ³•ç”Ÿæˆçš„æœ€ç»ˆç»“æœä¸åŸå§‹æ¨¡å‹ç”Ÿæˆçš„ç»“æœç›¸åŒï¼ˆæˆ–å…·æœ‰ç›¸åŒçš„åˆ†å¸ƒï¼‰ã€‚

è¾…åŠ©æ¨¡å‹ä¹Ÿæ˜¯ç›®æ ‡æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤å¯ä»¥å…±äº«ç¼“å­˜å’Œæƒé‡ï¼Œä»è€Œé™ä½å†…å­˜éœ€æ±‚ã€‚

å¯¹äºç»è¿‡æ—©æœŸé€€å‡ºè®­ç»ƒçš„æ¨¡å‹ï¼Œé€šè¿‡`generate()`ä¸­çš„`assistant_early_exit`å‚æ•°ä¼ é€’ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "Alice and Bob"
checkpoint = "facebook/layerskip-llama3.2-1B"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(checkpoint)
outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

### æ™®é€‚è¾…åŠ©è§£ç ï¼ˆUniversal Assisted Decodingï¼‰

æ™®é€‚è¾…åŠ©è§£ç ï¼ˆUADï¼‰å…è®¸ä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨ã€‚ä¸»æ¨¡å‹çš„è¾“å…¥tokenè¢«é‡æ–°ç¼–ç ä¸ºè¾…åŠ©æ¨¡å‹çš„tokenã€‚å€™é€‰tokenåœ¨è¾…åŠ©ç¼–ç ä¸­ç”Ÿæˆï¼Œç„¶åé‡æ–°ç¼–ç ä¸ºä¸»æ¨¡å‹çš„å€™é€‰tokenã€‚å€™é€‰tokençš„éªŒè¯æ–¹å¼ä¸è¾…åŠ©è§£ç ä¸­æ‰€è¿°ç›¸åŒã€‚

é‡æ–°ç¼–ç æ¶‰åŠå°†token IDè§£ç ä¸ºæ–‡æœ¬ï¼Œç„¶åä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚ä¸ºäº†é¿å…åœ¨é‡æ–°ç¼–ç è¿‡ç¨‹ä¸­å‡ºç°åˆ†è¯å·®å¼‚ï¼ŒUADä¼šæ‰¾åˆ°æºç¼–ç å’Œç›®æ ‡ç¼–ç ä¹‹é—´çš„æœ€é•¿å…¬å…±å­åºåˆ—ï¼Œä»¥ç¡®ä¿æ–°tokenåŒ…å«æ­£ç¡®çš„æç¤ºåç¼€ã€‚

é€šè¿‡åœ¨`generate()`ä¸­æ·»åŠ `tokenizer`å’Œ`assistant_tokenizer`å‚æ•°å¯ç”¨UADã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "Alice and Bob"

assistant_tokenizer = AutoTokenizer.from_pretrained("double7/vicuna-68m")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b")
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained("google/gemma-2-9b")
assistant_model = AutoModelForCausalLM.from_pretrained("double7/vicuna-68m")
outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```
['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']
```

### å¯¹æ¯”æœç´¢ï¼ˆContrastive Searchï¼‰

å¯¹æ¯”æœç´¢æ˜¯ä¸€ç§æ—¨åœ¨å‡å°‘é‡å¤çš„è§£ç ç­–ç•¥ï¼Œå³ä½¿åœ¨ç”Ÿæˆè¾ƒé•¿åºåˆ—æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¯¥ç­–ç•¥æ¯”è¾ƒç”Ÿæˆtokenä¸ä¹‹å‰tokençš„ç›¸ä¼¼åº¦ï¼Œå¦‚æœå®ƒä»¬æ›´ç›¸ä¼¼ï¼Œåˆ™ä¼šåº”ç”¨æƒ©ç½šã€‚

é€šè¿‡`penalty_alpha`å’Œ`top_k`å‚æ•°å¯ç”¨å¯¹æ¯”æœç´¢ã€‚`penalty_alpha`ç®¡ç†åº”ç”¨çš„æƒ©ç½šï¼Œ`top_k`æ˜¯è¿”å›çš„æœ€æœ‰å¯èƒ½çš„tokenæ•°é‡ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦ä¸º4096
outputs = model.generate(**inputs, max_new_tokens=100, penalty_alpha=0.6, top_k=4)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```python
Hugging Face is an open-source company that provides a platform for building and deploying AI models.
Hugging Face is an open-source company that provides a platform for building and deploying AI models. The platform allows developers to build and deploy AI models, as well as collaborate with other developers.
Hugging Face was founded in 2019 by Thibault Wittemberg and ClÃ©ment Delangue. The company is based in Paris, France.
Hugging Face has
```

### DoLaï¼ˆDecoding by Contrasting Layersï¼‰

DoLaæ˜¯ä¸€ç§å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜äº‹å®æ€§å¹¶å‡å°‘å¹»è§‰ã€‚è¯¥ç­–ç•¥é€šè¿‡å¯¹æ¯”æœ€ç»ˆå±‚ä¸æ—©æœŸå±‚ä¹‹é—´çš„logitå·®å¼‚æ¥å®ç°ã€‚å› æ­¤ï¼Œç‰¹å®šå±‚ä¸­çš„äº‹å®æ€§çŸ¥è¯†å¾—ä»¥å¢å¼ºã€‚ä¸å»ºè®®å°†DoLaç”¨äºè¾ƒå°çš„æ¨¡å‹ï¼Œä¾‹å¦‚GPT-2ã€‚

é€šè¿‡ä»¥ä¸‹å‚æ•°å¯ç”¨DoLaï¼š

- `dola_layers`ï¼šè¿™äº›æ˜¯ä¸æœ€ç»ˆå±‚è¿›è¡Œå¯¹æ¯”çš„å€™é€‰å±‚ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆ`low`æˆ–`high`ï¼‰ï¼Œç”¨äºå¯¹æ¯”å±‚çš„è¾ƒä½æˆ–è¾ƒé«˜éƒ¨åˆ†ã€‚å¯¹äºçŸ­ç­”æ¡ˆä»»åŠ¡ï¼ˆå¦‚TruthfulQAï¼‰ï¼Œå»ºè®®ä½¿ç”¨`high`ï¼›å¯¹äºé•¿ç­”æ¡ˆæ¨ç†ä»»åŠ¡ï¼ˆå¦‚GSM8Kã€StrategyQAã€FACTORå’ŒVicunaQAï¼‰ï¼Œå»ºè®®ä½¿ç”¨`low`ã€‚
- å¦‚æœæ¨¡å‹çš„è¯åµŒå…¥æ˜¯ç»‘å®šçš„ï¼Œåˆ™è·³è¿‡ç¬¬0å±‚ï¼Œä»ç¬¬2å±‚å¼€å§‹ã€‚
- å®ƒä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œè¡¨ç¤º0åˆ°æ€»å±‚æ•°ä¹‹é—´çš„å±‚ç´¢å¼•ã€‚ç¬¬0å±‚æ˜¯è¯åµŒå…¥ï¼Œç¬¬1å±‚æ˜¯ç¬¬ä¸€ä¸ªTransformerå±‚ï¼Œä¾æ­¤ç±»æ¨ã€‚æ ¹æ®æ¨¡å‹å±‚æ•°çš„ä¸åŒï¼Œå±‚ç´¢å¼•èŒƒå›´å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

| å±‚æ•°  | `low`               | `high`              |
| ----- | ------------------- | ------------------- |
| 40    | (0, 20, 2)          | (N - 20, N, 2)      |
| <= 40 | range(0, N // 2, 2) | range(N // 2, N, 2) |

- `repetition_penalty`ï¼šå‡å°‘é‡å¤ï¼Œå»ºè®®è®¾ç½®ä¸º1.2ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-1.7B", torch_dtype=torch.float16).to("cuda")
inputs = tokenizer("What is the highest peak in the world??", return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=50, dola_layers="high", do_sample=False)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```
Mount EverestMount Everest, called Himalaya in Nepali, is the world's highest peak, lying almost 9.5 kilometers above the sea level and the tallest mountain from 19,036.91 ft. The mountain was
```

### å¤šæ ·æ€§æŸæœç´¢ï¼ˆDiverse Beam Searchï¼‰

å¤šæ ·æ€§æŸæœç´¢æ˜¯æŸæœç´¢çš„ä¸€ç§å˜ä½“ï¼Œæ—¨åœ¨ç”Ÿæˆæ›´å…·å¤šæ ·æ€§çš„è¾“å‡ºå€™é€‰ç»“æœã€‚è¯¥ç­–ç•¥è¡¡é‡åºåˆ—ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œå¦‚æœåºåˆ—è¿‡äºç›¸ä¼¼ï¼Œåˆ™ä¼šåº”ç”¨æƒ©ç½šã€‚ä¸ºäº†é¿å…é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ï¼ŒæŸè¢«åˆ†æˆè‹¥å¹²ç»„ã€‚

é€šè¿‡`num_beams`ã€`num_beam_groups`å’Œ`diversity_penalty`å‚æ•°å¯ç”¨å¤šæ ·æ€§æŸæœç´¢ï¼ˆ`num_beams`åº”èƒ½è¢«`num_beam_groups`æ•´é™¤ï¼‰ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦ä¸º4096
outputs = model.generate(**inputs, max_new_tokens=50, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, do_sample=False)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

è¾“å‡ºï¼š

```
Hugging Face is an open-source company ğŸ¤—
We are an open-source company. Our mission is to democratize AI and make it accessible to everyone. We believe that AI should be used for the benefit of humanity, not for the benefit of a
```

## è‡ªå®šä¹‰è§£ç æ–¹æ³•

è‡ªå®šä¹‰è§£ç æ–¹æ³•å¯ä»¥å®ç°ç‰¹å®šçš„ç”Ÿæˆè¡Œä¸ºï¼Œä¾‹å¦‚ï¼š

- å¦‚æœæ¨¡å‹ä¸ç¡®å®šï¼Œåˆ™ç»§ç»­æ€è€ƒï¼›
- å¦‚æœæ¨¡å‹é™·å…¥å›°å¢ƒï¼Œåˆ™å›æ»šç”Ÿæˆï¼›
- ä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘å¤„ç†ç‰¹æ®Štokenï¼›
- ä¸ºé«˜çº§æ¨¡å‹å¢å¼ºè¾“å…¥å‡†å¤‡ã€‚

æˆ‘ä»¬é€šè¿‡æ¨¡å‹ä»“åº“å¯ç”¨è‡ªå®šä¹‰è§£ç æ–¹æ³•ï¼Œå‡è®¾å…¶å…·æœ‰ç‰¹å®šçš„æ¨¡å‹æ ‡ç­¾å’Œæ–‡ä»¶ç»“æ„ï¼ˆè§ä¸‹æ–‡å­èŠ‚ï¼‰ã€‚æ­¤åŠŸèƒ½æ˜¯è‡ªå®šä¹‰å»ºæ¨¡ä»£ç çš„æ‰©å±•ï¼ŒåŒæ ·éœ€è¦è®¾ç½®`trust_remote_code=True`ã€‚

å¦‚æœæ¨¡å‹ä»“åº“åŒ…å«è‡ªå®šä¹‰è§£ç æ–¹æ³•ï¼Œå°è¯•å®ƒçš„æœ€ç®€å•æ–¹æ³•æ˜¯åŠ è½½æ¨¡å‹å¹¶ä½¿ç”¨å®ƒè¿›è¡Œç”Ÿæˆï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# `transformers-community/custom_generate_example`æ˜¯`Qwen/Qwen2.5-0.5B-Instruct`çš„ä¸€ä¸ªå‰¯æœ¬ï¼Œ
# ä½†å®ƒå¸¦æœ‰è‡ªå®šä¹‰ç”Ÿæˆä»£ç  -> è°ƒç”¨`generate`å°†ä½¿ç”¨è‡ªå®šä¹‰è§£ç æ–¹æ³•ï¼
tokenizer = AutoTokenizer.from_pretrained("transformers-community/custom_generate_example")
model = AutoModelForCausalLM.from_pretrained(
    "transformers-community/custom_generate_example", device_map="auto", trust_remote_code=True
)

inputs = tokenizer(["The quick brown"], return_tensors="pt").to(model.device)
# è‡ªå®šä¹‰è§£ç æ–¹æ³•æ˜¯ä¸€ä¸ªæœ€å°åŒ–çš„è´ªå©ªè§£ç å®ç°ã€‚å®ƒè¿˜ä¼šåœ¨è¿è¡Œæ—¶æ‰“å°ä¸€æ¡è‡ªå®šä¹‰æ¶ˆæ¯ã€‚
gen_out = model.generate(**inputs)
# ç°åœ¨æ‚¨åº”è¯¥ä¼šçœ‹åˆ°å®ƒçš„è‡ªå®šä¹‰æ¶ˆæ¯ï¼š"âœ¨ using a custom generation method âœ¨"
print(tokenizer.batch_decode(gen_out, skip_special_tokens=True))
```

è¾“å‡ºï¼š

```
The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is
```

å¸¦æœ‰è‡ªå®šä¹‰è§£ç æ–¹æ³•çš„æ¨¡å‹ä»“åº“æœ‰ä¸€ä¸ªç‰¹æ®Šå±æ€§ï¼šå…¶è§£ç æ–¹æ³•å¯ä»¥é€šè¿‡`generate()`çš„`custom_generate`å‚æ•°ä»ä»»ä½•æ¨¡å‹ä¸­åŠ è½½ã€‚è¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥åˆ›å»ºå¹¶å…±äº«å…¶è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿä¸ä»»ä½•Transformersæ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼Œè€Œæ— éœ€ç”¨æˆ·å®‰è£…é¢å¤–çš„PythonåŒ…ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", device_map="auto")

inputs = tokenizer(["The quick brown"], return_tensors="pt").to(model.device)
# `custom_generate`ç”¨è‡ªå®šä¹‰è§£ç æ–¹æ³•æ›¿æ¢äº†åŸå§‹çš„`generate`
gen_out = model.generate(**inputs, custom_generate="transformers-community/custom_generate_example", trust_remote_code=True)
print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
```

è¾“å‡ºï¼š

```
The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is
```

æ‚¨åº”è¯¥é˜…è¯»åŒ…å«è‡ªå®šä¹‰ç”Ÿæˆç­–ç•¥çš„ä»“åº“çš„`README.md`æ–‡ä»¶ï¼Œä»¥äº†è§£æ˜¯å¦å­˜åœ¨æ–°çš„å‚æ•°å’Œè¾“å‡ºç±»å‹å·®å¼‚ã€‚å¦åˆ™ï¼Œæ‚¨å¯ä»¥å‡è®¾å®ƒçš„å·¥ä½œæ–¹å¼ä¸åŸºç¡€`generate()`æ–¹æ³•ç›¸åŒã€‚

æ‚¨å¯ä»¥é€šè¿‡æœç´¢å…¶è‡ªå®šä¹‰æ ‡ç­¾ï¼ˆ`custom_generate`ï¼‰æ¥æ‰¾åˆ°æ‰€æœ‰è‡ªå®šä¹‰è§£ç æ–¹æ³•ã€‚

ä»¥`transformers-community/custom_generate_example`ä»“åº“ä¸ºä¾‹ã€‚å…¶`README.md`æŒ‡å‡ºï¼Œå®ƒæœ‰ä¸€ä¸ªé¢å¤–çš„è¾“å…¥å‚æ•°`left_padding`ï¼Œç”¨äºåœ¨æç¤ºä¹‹å‰æ·»åŠ ä¸€å®šæ•°é‡çš„å¡«å……tokenã€‚

```python
gen_out = model.generate(
    **inputs, custom_generate="transformers-community/custom_generate_example", trust_remote_code=True, left_padding=5
)
print(tokenizer.batch_decode(gen_out)[0])
```

è¾“å‡ºï¼š

```python
<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The quick brown fox jumps over the lazy dog.

The sentence "The quick"
```

å¦‚æœè‡ªå®šä¹‰æ–¹æ³•çš„Pythonä¾èµ–é¡¹ä¸æ‚¨çš„ç¯å¢ƒä¸åŒ¹é…ï¼Œæ‚¨å°†æ”¶åˆ°æœ‰å…³ç¼ºå°‘ä¾èµ–é¡¹çš„å¼‚å¸¸ã€‚ä¾‹å¦‚ï¼Œ`transformers-community/custom_generate_bad_requirements`åœ¨å…¶`custom_generate/requirements.txt`æ–‡ä»¶ä¸­å®šä¹‰äº†ä¸€ç»„ä¸å¯èƒ½æ»¡è¶³çš„ä¾èµ–é¡¹ï¼Œå¦‚æœæ‚¨å°è¯•è¿è¡Œå®ƒï¼Œæ‚¨å°†çœ‹åˆ°ä»¥ä¸‹é”™è¯¯æ¶ˆæ¯ã€‚

```python
ImportError: Missing requirements in your local environment for `transformers-community/custom_generate_bad_requirements`:
foo (installed: None)
bar==0.0.0 (installed: None)
torch>=99.0 (installed: 2.6.0)
```

æ ¹æ®æç¤ºæ›´æ–°æ‚¨çš„Pythonä¾èµ–é¡¹å°†æ¶ˆé™¤æ­¤é”™è¯¯æ¶ˆæ¯ã€‚

## åˆ›å»ºè‡ªå®šä¹‰è§£ç æ–¹æ³•

è¦åˆ›å»ºæ–°çš„è§£ç æ–¹æ³•ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ä»“åº“ï¼Œå¹¶å°†ä¸€äº›æ–‡ä»¶æ¨é€åˆ°å…¶ä¸­ã€‚

- æ‚¨è®¾è®¡è§£ç æ–¹æ³•æ‰€ä½¿ç”¨çš„æ¨¡å‹ã€‚
- `custom_generate/generate.py`ï¼Œå…¶ä¸­åŒ…å«æ‚¨è‡ªå®šä¹‰è§£ç æ–¹æ³•çš„æ‰€æœ‰é€»è¾‘ã€‚
- `custom_generate/requirements.txt`ï¼Œç”¨äºå¯é€‰åœ°æ·»åŠ æ–°çš„Pythonä¾èµ–é¡¹å’Œ/æˆ–é”å®šç‰¹å®šç‰ˆæœ¬ï¼Œä»¥ä¾¿æ­£ç¡®ä½¿ç”¨æ‚¨çš„æ–¹æ³•ã€‚
- `README.md`ï¼Œæ‚¨åº”åœ¨æ­¤å¤„æ·»åŠ `custom_generate`æ ‡ç­¾ï¼Œå¹¶è®°å½•æ‚¨è‡ªå®šä¹‰æ–¹æ³•çš„ä»»ä½•æ–°å‚æ•°æˆ–è¾“å‡ºç±»å‹å·®å¼‚ã€‚

æ·»åŠ æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶åï¼Œæ‚¨çš„ä»“åº“åº”å¦‚ä¸‹æ‰€ç¤ºï¼š

```plaintext
your_repo/
â”œâ”€â”€ README.md          # åŒ…å«`custom_generate`æ ‡ç­¾
â”œâ”€â”€ config.json
â”œâ”€â”€ ...
â””â”€â”€ custom_generate/
    â”œâ”€â”€ generate.py
    â””â”€â”€ requirements.txt
```

### æ·»åŠ åŸºç¡€æ¨¡å‹

æ‚¨è‡ªå®šä¹‰è§£ç æ–¹æ³•çš„èµ·ç‚¹æ˜¯ä¸€ä¸ªæ™®é€šçš„æ¨¡å‹ä»“åº“ã€‚è¦æ·»åŠ åˆ°æ­¤ä»“åº“çš„æ¨¡å‹åº”è¯¥æ˜¯æ‚¨è®¾è®¡æ–¹æ³•æ—¶æ‰€ä½¿ç”¨çš„æ¨¡å‹ï¼Œå¹¶ä¸”å®ƒåº”è¯¥æ˜¯å·¥ä½œä¸­çš„ä¸€ä¸ªå®Œæ•´çš„è‡ªåŒ…å«æ¨¡å‹-ç”Ÿæˆå¯¹çš„ä¸€éƒ¨åˆ†ã€‚å½“åŠ è½½æ­¤ä»“åº“ä¸­çš„æ¨¡å‹æ—¶ï¼Œæ‚¨çš„è‡ªå®šä¹‰è§£ç æ–¹æ³•å°†è¦†ç›–`generate`ã€‚ä¸ç”¨æ‹…å¿ƒâ€”â€”æ‚¨çš„è§£ç æ–¹æ³•ä»ç„¶å¯ä»¥ä¸ä»»ä½•å…¶ä»–Transformersæ¨¡å‹ä¸€èµ·åŠ è½½ï¼Œå¦‚ä¸ŠèŠ‚æ‰€è¿°ã€‚

å¦‚æœæ‚¨åªæ˜¯æƒ³å¤åˆ¶ä¸€ä¸ªç°æœ‰çš„æ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("source/model_repo")
model = AutoModelForCausalLM.from_pretrained("source/model_repo")
tokenizer.save_pretrained("your/decoding_method", push_to_hub=True)
model.save_pretrained("your/decoding_method", push_to_hub=True)
```

### generate.py

è¿™æ˜¯æ‚¨è§£ç æ–¹æ³•çš„æ ¸å¿ƒã€‚å®ƒå¿…é¡»åŒ…å«ä¸€ä¸ªåä¸º`generate`çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ˜¯`model`ã€‚`model`æ˜¯æ¨¡å‹å®ä¾‹ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥è®¿é—®æ¨¡å‹ä¸­çš„æ‰€æœ‰å±æ€§å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨`GenerationMixin`ä¸­å®šä¹‰çš„æ–¹æ³•ï¼ˆä¾‹å¦‚åŸºç¡€`generate`æ–¹æ³•ï¼‰ã€‚

`generate.py`å¿…é¡»æ”¾åœ¨åä¸º`custom_generate`çš„æ–‡ä»¶å¤¹ä¸­ï¼Œè€Œä¸æ˜¯ä»“åº“çš„æ ¹ç›®å½•ä¸­ã€‚æ­¤åŠŸèƒ½çš„æ–‡ä»¶è·¯å¾„æ˜¯ç¡¬ç¼–ç çš„ã€‚

åœ¨å†…éƒ¨ï¼Œå½“åŸºç¡€`generate()`æ–¹æ³•è¢«è°ƒç”¨æ—¶ï¼Œå¦‚æœå¸¦æœ‰`custom_generate`å‚æ•°ï¼Œå®ƒå°†é¦–å…ˆæ£€æŸ¥å…¶Pythonä¾èµ–é¡¹ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œç„¶ååœ¨`generate.py`ä¸­å®šä½è‡ªå®šä¹‰`generate`æ–¹æ³•ï¼Œæœ€åè°ƒç”¨è‡ªå®šä¹‰`generate`ã€‚æ‰€æœ‰æ¥æ”¶åˆ°çš„å‚æ•°å’Œæ¨¡å‹éƒ½ä¼šè½¬å‘åˆ°æ‚¨çš„è‡ªå®šä¹‰`generate`æ–¹æ³•ä¸­ã€‚

è¿™æ„å‘³ç€æ‚¨çš„`generate`å¯ä»¥åŒ…å«åŸå§‹å’Œè‡ªå®šä¹‰å‚æ•°çš„æ··åˆï¼ˆä»¥åŠä¸åŒçš„è¾“å‡ºç±»å‹ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import torch

def generate(model, input_ids, generation_config=None, left_padding=None, **kwargs):
    generation_config = generation_config or model.generation_config  # é»˜è®¤ä½¿ç”¨æ¨¡å‹çš„ç”Ÿæˆé…ç½®
    cur_length = input_ids.shape[1]
    max_length = generation_config.max_length or cur_length + generation_config.max_new_tokens

    # ç¤ºä¾‹è‡ªå®šä¹‰å‚æ•°ï¼šåœ¨æç¤ºä¹‹å‰æ·»åŠ `left_padding`ï¼ˆæ•´æ•°ï¼‰ä¸ªå¡«å……token
    if left_padding is not None:
        if not isinstance(left_padding, int) or left_padding < 0:
            raise ValueError(f"left_paddingå¿…é¡»æ˜¯ä¸€ä¸ªå¤§äº0çš„æ•´æ•°ï¼Œä½†å½“å‰å€¼ä¸º{left_padding}")

        pad_token = kwargs.pop("pad_token", None) or generation_config.pad_token_id or model.config.pad_token_id
        if pad_token is None:
            raise ValueError("pad_tokenæœªå®šä¹‰")
        batch_size = input_ids.shape[0]
        pad_tensor = torch.full(size=(batch_size, left_padding), fill_value=pad_token).to(input_ids.device)
        input_ids = torch.cat((pad_tensor, input_ids), dim=1)
        cur_length = input_ids.shape[1]

    # ç®€å•çš„è´ªå©ªè§£ç å¾ªç¯
    while cur_length < max_length:
        logits = model(input_ids).logits
        next_token_logits = logits[:, -1, :]
        next_tokens = torch.argmax(next_token_logits, dim=-1)
        input_ids = torch.cat((input_ids, next_tokens[:, None]), dim=-1)
        cur_length += 1

    return input_ids
```

ä¸ºäº†ç¡®ä¿æ‚¨çš„è‡ªå®šä¹‰è§£ç æ–¹æ³•æŒ‰é¢„æœŸå·¥ä½œï¼Œè¯·éµå¾ªä»¥ä¸‹æ¨èå®è·µï¼š

- å°½é‡é‡ç”¨åŸºç¡€`generate()`ä¸­çš„éªŒè¯å’Œè¾“å…¥å‡†å¤‡é€»è¾‘ã€‚
- å¦‚æœåœ¨æ¨¡å‹ä¸­ä½¿ç”¨äº†ä»»ä½•ç§æœ‰æ–¹æ³•/å±æ€§ï¼Œè¯·åœ¨`requirements.txt`ä¸­å›ºå®š`transformers`ç‰ˆæœ¬ã€‚
- æ‚¨å¯ä»¥åœ¨`custom_generate`æ–‡ä»¶å¤¹ä¸­æ·»åŠ å…¶ä»–æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥ã€‚
- è€ƒè™‘æ·»åŠ æ¨¡å‹éªŒè¯ã€è¾“å…¥éªŒè¯ï¼Œç”šè‡³ä¸€ä¸ªå•ç‹¬çš„æµ‹è¯•æ–‡ä»¶ï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨ä»–ä»¬çš„ç¯å¢ƒä¸­éªŒè¯æ‚¨çš„ä»£ç ã€‚

### requirements.txt

æ‚¨å¯ä»¥åœ¨`custom_generate`æ–‡ä»¶å¤¹å†…çš„`requirements.txt`æ–‡ä»¶ä¸­å¯é€‰åœ°æŒ‡å®šé¢å¤–çš„Pythonä¾èµ–é¡¹ã€‚è¿™äº›ä¾èµ–é¡¹ä¼šåœ¨è¿è¡Œæ—¶è¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æœç¼ºå¤±ï¼Œå°†æŠ›å‡ºå¼‚å¸¸ï¼Œæç¤ºç”¨æˆ·æ›´æ–°ä»–ä»¬çš„ç¯å¢ƒã€‚

### README.md

æ¨¡å‹ä»“åº“æ ¹ç›®å½•ä¸­çš„`README.md`é€šå¸¸æè¿°è¯¥ä»“åº“ä¸­çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºä»“åº“çš„é‡ç‚¹æ˜¯è‡ªå®šä¹‰è§£ç æ–¹æ³•ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨å°†é‡ç‚¹è½¬ç§»åˆ°æè¿°è‡ªå®šä¹‰è§£ç æ–¹æ³•ä¸Šã€‚é™¤äº†å¯¹æ–¹æ³•çš„æè¿°å¤–ï¼Œæˆ‘ä»¬è¿˜å»ºè®®æ‚¨è®°å½•ä¸åŸå§‹`generate()`ç›¸æ¯”çš„è¾“å…¥å’Œ/æˆ–è¾“å‡ºå·®å¼‚ã€‚è¿™æ ·ï¼Œç”¨æˆ·å¯ä»¥ä¸“æ³¨äºæ–°å†…å®¹ï¼Œè€Œä¾èµ–Transformersæ–‡æ¡£æ¥äº†è§£é€šç”¨å®ç°ç»†èŠ‚ã€‚

ä¸ºäº†æé«˜å¯å‘ç°æ€§ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨ä¸ºä»“åº“æ·»åŠ `custom_generate`æ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œæ‚¨çš„`README.md`æ–‡ä»¶çš„é¡¶éƒ¨åº”å¦‚ä¸‹æ‰€ç¤ºã€‚æ¨é€æ–‡ä»¶åï¼Œæ‚¨åº”è¯¥ä¼šåœ¨ä»“åº“ä¸­çœ‹åˆ°è¯¥æ ‡ç­¾ï¼

```plaintext
library_name: transformers
tags:
  - custom_generate

(æ‚¨çš„Markdownå†…å®¹)
```

æ¨èå®è·µï¼š

- åœ¨`generate()`ä¸­è®°å½•è¾“å…¥å’Œè¾“å‡ºå·®å¼‚ã€‚
- æ·»åŠ è‡ªåŒ…å«çš„ç¤ºä¾‹ï¼Œä»¥ä¾¿å¿«é€Ÿè¿›è¡Œå®éªŒã€‚
- æè¿°è½¯æ€§è¦æ±‚ï¼Œä¾‹å¦‚è¯¥æ–¹æ³•ä»…é€‚ç”¨äºæŸä¸€ç±»æ¨¡å‹ã€‚
