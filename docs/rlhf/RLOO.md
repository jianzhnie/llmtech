# RLOO

## 摘要
强化学习从人类反馈（RLHF）越来越被视为提高大型语言模型（LLMs）性能的关键方法。近来的文献将 proximal policy optimization (PPO) 定位为 RLHF 中强化学习部分的典范方法。然而，它涉及高昂的计算成本和敏感的超参数调优。我们认为，导致 PPO 开发的大多数动机原则在 RLHF 中的实际应用中并不那么重要，并且提倡一种计算成本更低的方法，同时保持甚至提高性能。我们重新审视了在 RL 背景下从人类偏好中对齐的公式。以简洁性为指导原则，我们发现 PPO 的许多组件在 RLHF 背景下是不必要的，更简单的 REINFORCE 风格优化变体在性能上超越了 PPO 和新提出的“无 RL”方法，如 DPO 和 RAFT。我们的工作表明，仔细适应 LLMs 的对齐特性可以以低成本受益于在线 RL 优化。

## 1 引言
当前的大型语言模型（LLMs）通常在海量文本上进行预训练，这些文本包含数万亿个Token。这些训练语料通常包含许多复杂的偏好、关系和意图，可能并非所有都适合 LLM 展示。研究界和更广泛的从业者社区都非常关注如何使这些模型符合人类偏好？

尽管 RLHF 作为对齐方法受到广泛关注，但关于实现这一目标的最佳方法尚无共识。RLHF 直接借鉴了传统 RL 文献，并使用诸如 PPO 等技术来最大化由奖励模型产生的奖励分数，该奖励模型通常作为二元分类器在由人类注释者Token的Completion对上进行训练。虽然 PPO 已经通过其在 RLHF 的开创性文献中的使用而成为典范方法，但在实践中使其正常工作对于非 RL 专家来说并不容易，并且存在已知的问题：

1. 计算成本：PPO 通常需要同时加载多达 4 个模型：生成器、参考模型（用于 KL 估计）、评论家和奖励模型，其中生成和评论家模型的训练是交错进行的。现代 LLM 的大小通常以数十亿参数计，这进一步加剧了这一挑战。
2. 优化挑战：在线 RL 优化的不稳定性和 PPO 的相对算法复杂性需要特定领域的专业知识来调优。

最近的工作提出了“无 RL”方法，如 DPO、IPO 或迭代微调方法来对 LLM 进行偏好训练。然而，这些工作都没有质疑在 RLHF 中是否存在更简单的 RL 范式解决方案。相反，所有这些方法都试图通过剥离 RLHF 中的所有 RL 组件及其带来的困难来回答这个问题。

与这些方法不同，我们仍然坚持 RL 范式，但回归基础。我们在这项工作中要探讨的核心问题是：我们能否避免 PPO 的计算和优化复杂性，同时保持性能？我们隔离了传统深度 RL 设置和 LLM 人类偏好学习设置之间的几个关键差异。我们指出，PPO 作为一种方法，强调迭代过程中的稳定性，旨在通过小而稳定的更新来训练有效的策略。PPO 是为一个离策略梯度更新足够大以引入不稳定性的环境而设计的。这种环境在传统深度 RL 基准测试中占据主导地位。然而，在这项工作中，我们认为 RLHF 的设置，即微调预训练的 LLM，缺乏这些特性。

与传统深度 RL 设置不同，策略的初始化形式是预训练和监督微调（SFT）模型，远非随机参数化。虽然可想象的搜索空间是巨大的，但由于预训练和 SFT 阶段，只有更小的子集的Token可能被生成，因为概率质量集中在这些少数Token上。因此，虽然传统深度 RL 设置需要强大的正则化来减少梯度估计的高方差，但我们在 RLHF 中的经验观察到这在实际应用中并不是一个主要问题，并且激励了一种计算成本更低的方法，同时保持鲁棒性。

此外，我们重新审视了在 RL 背景下如何从人类偏好中学习，其中生成每个Token被视为一个动作，每个部分序列从提示开始被视为一个状态。在实践中，PPO 方法的这一建模假设通常是无效的。我们论证并展示了在奖励仅归因于完整生成的情况下，对部分序列的建模是不必要的，而在生成的任何中间Token上都没有真正的奖励。因此，将整个生成建模为一个单一动作，初始状态由提示确定，是更合适且高效的方法。

基于这些观察，以简洁性为指导原则，我们探索了 REINFORCE 估计器及其多样本扩展 REINFORCE Leave-One-Out (RLOO) 来优化序列级目标。我们将 PPO 拆解，并展示了最基本的策略梯度算法，即 Vanilla Policy Gradient REINFORCE 一致地优于 PPO。与 PPO 不同，我们可以使用 REINFORCE 直接优化完整轨迹（序列）回报，并结合无偏基线，而 Actor-Critic 算法（如 PPO）通过从中间状态值函数引导来减少方差，但以引入估计器的偏差为代价。

我们在包括 Llama、Pythia 等模型以及 Anthropic Helpful & Harmless 和 TL;DR Summarize 等数据集上得到了一致的结果：

1. PPO 不是 RLHF 中 RL 的正确工具。我们将 PPO 拆解，并展示了最基本的策略梯度算法，即 Vanilla Policy Gradient REINFORCE 一致地优于 PPO，在所有数据集和基础模型配对中，胜率提高了 3.2% 到 20.3%。
2. RLOO 超越了关键基线。基于 REINFORCE 构建的 RLOO 允许使用多个在线样本，我们实证展示了它一致地优于 PPO、DPO 以及 RAFT 等基线方法，在所有数据集和模型上。我们展示了 RLOO 比 RAFT 更好地利用了在线样本，同时对噪声和 KL 惩罚的敏感度更高。
3. 建模部分Completion是不必要的。我们有效地展示了对 LLM 偏好训练来说，建模部分序列是不必要的。相反，建模完整生成保留了性能，同时减少了 RL 阶段的复杂性，并显著加速了学习。
4. RLOO 对噪声和 KL 惩罚敏感度相对较高。我们还伴随着我们的结果进行了多维度分析，涉及语言流畅性、多样性和对噪声的鲁棒性。我们展示了 RLOO 相比 RAFT 在噪声和 KL 惩罚敏感度方面的鲁棒性。

## 2 背景
Ziegler 等人（2020）提出的 LLMs 的原始 RLHF 管道包括三个阶段：

1. SFT 阶段：预训练的语言模型使用由给定指令提示和（通常）人类编写的Completion组成的指令调优数据集进行训练。LM/策略使用交叉熵损失在Completion上进行训练。通常，SFT 模型 πsft 用于初始化奖励模型和 RLHF 策略。
2. 奖励模型阶段：RLHF 方法利用奖励模型 rϕ(x, y) 进行训练，使用偏好数据集 D = {(x, y+, y−)}N i=1，其中 y+ 和 y− 分别表示提示 x 的首选和非首选Completion。奖励模型作为二元分类器进行训练，损失函数为：LRM = − log σ(log(rϕ(x, y+) − rϕ(x, y−))。
3. RL 阶段：在这个阶段，奖励模型用于提供在线反馈以优化策略，目标为：
   $$
   \max πθ \mathbb{E}_{x \sim D, y \sim πθ(.|x)} [rϕ(x, y) − β D_{KL} πθ(.|x) || πref(.|x)]
   $$
   其中 β 用于在优化 rθ(x, y) 期间控制与初始策略 πref 的距离。KL 惩罚至关重要，因为无惩罚的奖励模型优化会导致模型连贯性下降。优化此目标等价于最大化以下 KL 形状的奖励的期望：
   $$
   R(x, y) = rϕ(x, y) − β \log \frac{πθ(y|x)}{πref(y|x)}
   $$

虽然强化学习方法共享上述组件，但奖励的公式有所不同。为了理解这些差异，我们将在以下部分介绍 PPO 以及不同的替代方案，如 REINFORCE 和 REINFORCE Leave-One-Out。

### 2.1 PPO
在 RL 阶段使用 PPO 时，初始状态由提示确定，每个生成的Token被建模为一个动作，部分序列被视为状态，折扣因子（γ ∈ [0, 1]）为 1。在这个框架中，只有生成 <EOS> Token时才会获得奖励，该奖励由奖励模型输出，并结合 KL 惩罚，而词汇表中其他Token的 KL 组件非零：
$$
R(x, y) = \sum_{t=1}^{T} R_t(x, y_t)
$$
其中 yt 表示 y 的第 t 个Token，T 是轨迹中的Token数量，Ri 是相应形状的奖励。

在实践中，PPO 使用以下Token级别的裁剪目标：
$$
\min \left\{ \frac{f(y_t|s_t)}{f_{old}(y_t|s_t)} \hat{A}_λ(y_t, s_t), \text{clip}_{1+\epsilon, 1−\epsilon} \left( \frac{f(y_t|s_t)}{f_{old}(y_t|s_t)} \right) \hat{A}_λ(y_t, s_t) \right\}
$$
其中 st = {y<t, x} 表示生成步骤 t 的状态，即生成Token y<t 的历史和给定提示 x，πold 是一个较旧的策略（与 πref 不同），而 ˆA(yt, st) 是生成Token（动作）yt 在部分Completion（状态）t−1 步的估计优势函数，ϵ 是裁剪比率。优势函数使用广义优势估计（GAE）进行估计。

### 2.2 REINFORCE

鉴于在 LLM 应用中，r(x, y) 仅在完整序列结束时获得，将整个生成建模为一个单一动作可能更合适，而不是每个Token。尽管尚未在 LLM 对齐的背景下探索，但将完整Completion建模为单一动作，如在 bandit 建模中，允许使用 REINFORCE 估计器。这允许通过离散动作（生成）空间进行反向传播，并直接优化整个序列的 KL 形状奖励目标：
$$
\mathbb{E}_{x \sim D, y \sim πθ(.|x)} [R(y, x) \nablaθ \log πθ(y|x)]
$$
为了提高学习效率，可以通过减去与随机梯度估计具有高协方差的基线 b 来减少估计器的方差，同时保持无偏性：
$$
\mathbb{E}_{x \sim D, y \sim πθ(.|x)} [(R(y, x) − b) \nablaθ \log πθ(y|x)]
$$
一个强大的无参数基线选择是训练过程中所有奖励的移动平均值：
$$
b_{MA} = \frac{1}{S} \sum_{s} R(x_s, y_s)
$$
其中 S 是训练步骤的数量，(xs, ys) 是第 s 步的提示-Completion对。

### 2.3 REINFORCE Leave-One-Out (RLOO)
第 8 节中的基线简单易实现且计算成本低。然而，如果可以访问多个在线样本，可以用于进一步的无偏方差减少，则可以改进它：

（1）每个样本的奖励可以作为其他样本的基线。

（2）策略更新可以基于每个样本的梯度估计的平均值，从而实现多样本蒙特卡洛（MC）估计。

这就是 REINFORCE Leave-One-Out (RLOO) 估计器的直觉，由 Kool 等人（2019）提出：
$$
\frac{1}{k} \sum_{i=1}^{k} \left[ R(y^{(i)}, x) − \frac{1}{k−1} \sum_{j \neq i} R(y^{(j)}, x) \right] \nabla \log π(y^{(i)}|x)
$$
其中 k 表示生成的在线样本数量，RLOO_k 考虑每个 y(i) 并使用剩余的 k−1 个样本创建无偏的期望回报估计，类似于每个训练步骤的参数免费值函数。

### 2.4 RL 在偏好训练中的替代方案
在 RLHF 的背景下，大量工作提出了“无 RL”方法，这些方法不涉及第三阶段。我们将对 PPO、REINFORCE 和 RLOO 等 RL 方法与这些替代方法（如“直接偏好优化（DPO）”和 RAFT）进行基准测试。

迭代微调方法使用训练的奖励模型对在线或离线采样的提示的Completion进行排名，然后迭代地在选定的子集上微调策略。我们注意到，使用强化学习/ bandit 学习的奖励与监督学习目标结合的技巧也被称为 bandit-to-supervised 转换，并且在 RLHF 之前的 NLP 问题中对大型动作空间有实证成功。

我们对奖励排名微调（RAFT; Dong 等人，2023）进行基准测试，该方法基于 R(x, y) 或 r(x, y) 对 k 个在线样本进行排名，然后在最佳排名的Completion上使用简单的交叉熵损失。我们注意到 RAFT 没有充分利用所有样本，因为它仅使用过滤后的顶级样本进行优化。相比之下，RLOO 充分利用了构建基线和多样本 MC 估计的策略梯度。

直接偏好优化（DPO）与传统 RLHF 管道中的其他方法不同，DPO 跳过了奖励建模阶段，并使用偏好对直接优化策略，损失函数为：
$$
− \log σ\left(β \log \frac{πθ(y^+|x)}{πref(y^+|x)} − β \log \frac{πθ(y^−|x)}{πref(y^−|x)}\right)
$$

## 3 从 PPO 到 REINFORCE

我们审视了 PPO 的各个组件，认为它们并不适合 RLHF。我们解释了理论起源，以 LLM RLHF 的实际条件为动机，并从初步实验中提供了实证支持。

### 3.1 重新审视低方差估计器的需求
Actor-Critic 算法（如 PPO）的动机是传统 RL 设置中观察到的高方差。PPO 利用从状态值函数引导的低方差估计器来改进学习。虽然引导减少了方差，但代价是引入了偏差，这可能会优化有偏的奖励。

相比之下，REINFORCE 使用轨迹回报的无偏蒙特卡洛估计器，理论上可能具有高方差，尤其是在仅用单个样本近似时，这在传统深度 RL 环境中并不常见。然而，我们注意到，这些发现基于从随机或弱初始化开始的训练场景，而不是从强大的预训练模型开始。

在这里，我们质疑这种实证证据在 RLHF 中是否成立。我们认为，在微调 LLM 时，由于策略的极强初始化（预训练 LLM），加上提示条件，导致每个生成步骤的概率质量集中在少数Token上，尽管理论上可能的动作数量是巨大的（有关条件影响的进一步讨论，请参见附录 A）。优化景观远不太可能出现破坏性的大和高方差梯度更新。因此，进一步减少方差以引入偏差是不值得的。

**实证支持** 为了验证这一假设，我们权衡了方差最小化和偏差引入的权重。在第 2.1 节中 PPO 的公式中，优势估计器 GAE 依赖于在 PPO 中平衡偏差和方差，以估计真实优势函数。GAE 在真实优势函数中引入了一个超参数 λ ∈ [0, 1]，平衡了构建估计器的偏差和方差。λ 越接近 1，观察到的方差越高。在高度随机的环境中，最小化方差以引入偏差是值得的。然而，在一个方差已经很低的稳定环境中，引入偏差是不必要的。

在图 1 中，我们展示了使用不同 λ 值的 GAE 对 PPO 的奖励评估结果。两个变体最小化偏差但引入高方差（λ = 1.00，即第 9 节中介绍的 Vanilla PG，和 λ = 0.95），以及两个变体过度关注最小化方差以引入偏差（λ = 0.0 和 λ = 0.5）。图 1 绘制了奖励并观察到，最极端的变体 Vanilla PG（无偏 Aλ=1.0）表现最佳，尽管存在高方差的风险。我们观察到随着 λ 的减小，奖励呈单调下降趋势。这支持了我们的假设，即在 RLHF 设置中，以引入偏差为代价减少方差是不必要的，因为环境的默认属性是稳定的。

### 3.2 在 RLHF 中裁剪很少是必要的
接下来，我们关注裁剪比率 ϵ（见第 5 节），它用于防止当 πθ/πold 偏离 1 太远时出现大的策略更新，即防止更新与当前策略相差太远。

在图 1 中，我们比较了独立 PPO 训练中使用和不使用裁剪的奖励曲线。请注意，对于这组实验，我们还关闭了价值网络的裁剪，因为在传统深度 RL 环境中，它对学习有显著影响。我们在 RLHF 设置中发现，平均而言，每批训练中裁剪损失的比例不到 5%，这表明学习制度接近“在线策略”，策略在每次迭代中变化缓慢。

为了进一步验证这一点，我们完全关闭裁剪，然后移除 πθ/πold 比率，同时 λ = 1，这将 PPO 损失简化为 Vanilla PG。如果有的话，移除裁剪会略微提升性能，验证了我们的假设，即在我们的优化制度中，大的离策略更新很少见，并且不会像在传统深度 RL 中那样对学习产生灾难性影响。

### 3.3 建模部分Completion是不必要的
如第 2 节所述，PPO 将每个Token建模为一个动作，而 REINFORCE 将整个生成建模为一个单一动作，而不是每个Token。在实践中，在 LLM RLHF 中，r(x, y) 仅归因于 <EOS> Token，而对于其他Token，只有 log π(yt|st)/πref(yt|st) 组成 Rt(x, y)，这没有实际意义。

从纯 RL 的角度来看，环境动力学是完全确定性的（PD({y<t+1,x}|st, yt) = 1），这意味着我们的环境（上下文）根据预测的新Token/动作确定性地变化。因此，问题可以简化为一个 bandit 问题，其中马尔可夫决策过程（MDP）仅由提示确定的初始状态和生成后始终达到的终止状态组成。REINFORCE 明确地将整个生成建模为单一动作，而迭代微调方法隐含地这样做，这些方法在使用奖励模型过滤之前生成完整Completion。

在第 5.1 节的结果中，我们将明确比较 REINFORCE 和 RLOO（都建模完整轨迹回报）与 PPO 和 Vanilla PG（都将部分Completion建模为动作）。问题是，在 RLHF 中，将整个生成建模为单一动作是否足以实现类似的或更好的性能？

## 4 实验设置
### 4.1 训练细节
**数据集** 我们在 TL;DR Summarize 和 Anthropic Helpful and Harmless Dialogue 数据集上报告结果。TL;DR Summarize 数据集的训练部分包含 116k 个人类编写的指令和 93k 个人类标注的偏好对。预处理后的 Anthropic-HH 数据集包含 112k 个训练偏好对。

**模型** 对于这两个数据集，我们使用 Pythia-6.9B 作为预训练基础模型。为了评估预训练模型质量对从人类偏好中学习的影响，我们还使用 Llama-7B 与 Anthropic-HH 数据集进行实验。

为了确保所有方法之间的公平比较，我们在监督微调和奖励模型训练期间都使用了 512 个Token的上下文长度。除非另有说明，我们使用相应的 SFT 检查点初始化奖励模型和策略。

**实验细节** 对于 TL;DR Summarize 数据集，我们使用了专用的 SFT 分割。由于原始 Anthropic-HH 数据集不包含单独的 SFT 分割，我们在 SFT 阶段使用了二元比较中的提示和首选响应，类似于先前的工作。在偏好训练阶段，我们使用 SFT 阶段的相同提示生成Completion。有关实验设置和超参数的进一步详细信息，请参见附录 C。

### 4.2 评估
**优化质量** 对于所有在线方法（除 DPO 外的所有方法），为了衡量方法优化内在目标的效果，我们在测试集的 1000 个样本上报告平均奖励（使用训练 RM）。为了衡量每个方法在优化模型与人类偏好对齐的外在目标方面的效果，在相同的测试样本上，我们报告了与 Alpacafarm 框架一致的模拟胜率，其中我们使用 GPT-4 作为人类评估的代理。对于 TL;DR 数据集，我们在评估时使用贪婪采样，除非另有说明。

**对齐税** RLHF 微调通常与多样性下降和语言流畅性下降相关，这被称为对齐税。因此，我们还报告了作为流畅性和多样性代理的指标，类似于先前的工作。为了衡量流畅性，我们使用测试集中首选Completion的困惑度，类似于 Dong 等人（2023）。最后，我们使用平均 n 元语法多样性来衡量Completion长度和多样性。

## 5 结果与讨论
### 5.1 奖励优化
RLOO、REINFORCE（带基线）、RAFT、PPO 和 Vanilla PG 的目标是最大化奖励分数，因此我们比较了每种方法的优化成功程度。在每个数据集和基础模型配对上，我们对所有方法使用相同的奖励模型，因此它们的测试奖励分数可以直接比较。

**建模部分Completion与完整生成** 如图 2 所示，我们发现不建模部分Completion的方法（如带基线的 REINFORCE 和 RLOO）一致地优于将每个Token建模为动作的方法（如 Vanilla PG 和 PPO）。此外，除了在奖励优化方面的优越性能外，这些方法还需要加载更少的模型副本，与 Vanilla PG 和 PPO 相比，后者需要训练一个学习基线和一个值网络。这表明在 RLHF 背景下，建模部分序列是不必要的。

**采样效率** 给定相同的采样预算（每个提示的 k 个在线样本），RLOO 在整个训练过程中一致地优于 RAFT，如图 3 所示。值得注意的是，尽管采样预算较小，RLOO_k=2 在所有数据集和模型上要么与 RAFT_k=4 匹敌，要么优于 RAFT_k=4。这证实了 RLOO 通过使用所有生成的样本更好地优化了目标，而 RAFT 仅使用顶级样本进行微调。图 4 绘制了基于训练期间生成的总样本数量的奖励，无论 k 值如何，进一步证实了这一发现。

### 5.2 模拟胜率
表 1 呈现了每种方法在 TL;DR Summarize 和 Anthropic-HH 数据集上的原始Completion的胜率。这里，我们还包括了 DPO。

**建模部分Completion是不必要的** 请回忆，本工作中提到的 Vanilla PG 和 REINFORCE 之间的关键区别在于，虽然 Vanilla PG 将每个Token视为一个动作，但 REINFORCE 作用于整个生成。如表 1 所示，带基线的 REINFORCE 在 TL;DR（70.7 对 70.4）和 HH（37.9 对 36.4）数据集上与 Vanilla PG 不相上下，当使用基于 Pythia 的模型时。此外，带基线的 REINFORCE 在使用基于 Llama 的模型的 HH 数据集上优于 Vanilla PG，实现了更高的胜率（55.3 对 52.3）。

这证实了仅建模整个生成而不建模部分Completion的有效性，即使在 RLHF 中不使用多个样本。

**胜率与测试奖励分数一致** RLOO 在 k = 4 时实现了最高的胜率，在 TL;DR、HH（Pythia）和 HH（Llama）上分别比 PPO 高出 10.3、14.5 和 32.1。

**RLOO 比 RAFT 更具采样效率** 在相同的采样预算 k 下，RLOO 在所有数据集和模型上一致地优于 RAFT。当平均三个数据集和模型配对时，RLOO 在 k = 2 和 k = 4 时分别实现了 61.3 和 61.9 的胜率，而 RAFT 分别为 56.1 和 59.5。值得注意的是，RLOO 相比 RAFT 的胜率提升最高，如在 HH 数据集上 k = 2 且使用基于 Pythia 的模型时，提升达到 9.9（见表 1 第二列）。

### 5.2.1 对齐税
表 2 显示了 Llama 基础模型在 Anthropic-HH 数据集上的各种内在评估指标。

**生成长度** 显著地，DPO 训练的模型倾向于过于冗长（平均生成长度为 104 个Token），而 PPO 训练的模型导致生成长度最短（平均 16 个Token）。我们在附录 E 中提供了示例响应。

**困惑度和多样性** 如表 2 所示，RLOO、RAFT 和带基线的 REINFORCE 的困惑度（PPL）得分相对接近，且均显著低于 PPO 和 Vanilla PG。

在多样性方面，RLOO、RAFT、带基线的 REINFORCE 和 Vanilla PG 的 Diversity-1 得分相似。Diversity-2 得分倾向于随着奖励优化的提高而略有下降，这并不令人惊讶，因为与其他方法相比，它们的生成长度差异显著。总体而言，RLOO 和带基线的 REINFORCE 在保持流畅性和多样性的同时，实现了更高的奖励分数和胜率。

**奖励方差** 在安全性和无害性等应用中，生成低奖励样本的风险很高，因此较低的奖励方差是可取的。表 2 的结果显示，在相同的 k 值下，RLOO 的生成奖励方差比 RAFT 略低，而 RAFT 是与 RLOO 在奖励优化方面最具竞争力的方法。最后，Vanilla PG 导致最高的奖励方差。带基线的 REINFORCE 在经验上导致 27% 的方差减少，尽管在奖励优化和胜率方面与 Vanilla PG 不相上下或优于 Vanilla PG。

### 5.2.2 鲁棒性
如前所述，RAFT 的一个主要缺点是它仅优化最高排名的样本并丢弃其余的在线样本。因此，导致最佳Completion排名不准确的因素也可能显著阻碍学习。我们通过展示在不同程度的奖励噪声下 RAFT 与 RLOO 的表现来证明这种脆弱性。

**KL 惩罚的不匹配** 在图 5 中，我们展示了在 HH 数据集上使用基于 Pythia 的模型进行训练时，RLOO 和 RAFT 的 KL 距离和测试奖励曲线，使用 k = 2。我们使用 β = {0.25, 0.5, 1} 变化 KL 正则化。较大的 KL 惩罚在 R(x, y) 中可能增加 k 个在线样本之间的排名不匹配。然而，β 的选择通常取决于多个因素，如数据分布和基础模型的输出 logits，这可能不允许即使使用早停也使用较低的 β 值。

我们发现 RAFT 对较高的 KL 正则化更敏感。在低正则化制度（β = {0.1}）下，RLOO 和 RAFT 收敛到与参考策略相同的 KL 距离，而 RLOO 实现了更高的奖励。然而，随着正则化的增加（β = {0.25, 0.5, 1.0}），RAFT 不仅在优化奖励方面更差，而且与参考策略的偏差更大。

**奖励噪声的不匹配** 由于人类偏好（Nguyen 等人，2017b；Kreutzer 等人，2018）的固有噪声性质，奖励模型本身是奖励信号的噪声代理。受贝叶斯深度学习中建模 Aleatoric 不确定性（Kendall & Gal, 2017；Collier 等人，2021）的文献启发，为了模拟不同程度的这种噪声，对于每个提示，我们向奖励添加噪声。具体来说，我们向二元分类器的输出 logits 添加噪声：
$$
rσ(x, y) = r(x, y) + ϵ \quad \text{其中} \quad ϵ \sim \mathcal{N}(0, σ^2)
$$
图 6 显示了在不同噪声水平 σ = {1.0, 3.0, 5.0} 下的奖励下降情况。如预期所示，RAFT 的训练奖励下降幅度远大于 RLOO。这可能是由于奖励噪声影响了相对排名，从而影响了训练奖励。相比之下，RLOO 在嘈杂的奖励信号下表现出相对鲁棒的奖励优化。

## 6 结论
总体而言，这项工作认为，RLHF 设置中的 LLM 微调具有策略的强初始化，加上提示条件，缓解了高方差和大型动作空间的历史担忧。我们通过实证结果支持这一观点，表明尽管在传统深度 RL 设置中很少使用且由于高方差而备受诟病，但 vanilla 策略梯度 REINFORCE（Vanilla PG）优于 PPO。此外，我们重新审视了从人类偏好中学习的问题建模，并实证展示了 REINFORCE 估计器尽管简单，但能够实现高质量的奖励优化。

最后，我们的实验表明，RLOO 作为 REINFORCE 的多样本扩展，在保留与迭代微调方法（如 RAFT）相比的高鲁棒性的同时，优于 RAFT、DPO 和 PPO，实现了两全其美的效果。

## 7 限制
我们工作的局限性之一是，我们没有研究奖励模型（RM）过度优化问题，即代理奖励的优化轨迹与“黄金”奖励目标偏离。这一方面尚未在迭代微调方法（如 RAFT）中进行研究，值得专门研究。我们将其留给未来的工作。

另一个局限性是，我们没有探索在单个Token动作框架中使用 LOO 基线，其中部分序列被建模且提供中间奖励。在这项工作中，我们表明在 RLHF 背景下，建模部分序列是不必要的，因为奖励仅归因于完整序列。

最后，我们仅限于使用 LLM 进行模拟胜率评估，而没有衡量与最终人类评估偏好的相关性。我们也没有探索使用其他 NLP 中的奖励，如 ROUGE、BLEU 或其他指标进行 RL 训练。

## 8 致谢
我们感谢 Ivan Zhang、Phil Blunsom、Florian Strub、Max Bartolo、Bharat Venkitesh、Roger Grosse 和 Keiran Paster 的有益讨论。我们特别感谢 Matthieu Geist 在这项工作的框架和最终手稿反馈方面的多次富有成果的讨论。我们还要感谢 Cohere 和 Cohere For AI 的同事们在整个项目过程中提供的持续支持。
