# ASPO：非对称重要性采样策略优化

在训练大型语言模型（LLMs）的主流强化学习算法中，如PPO和GRPO，都依赖于PPO-Clip组件。我们发现其原始设计中存在一个关键缺陷：**对具有正优势（Positive Advantage）的token存在权重失配**，这会导致模型过拟合、熵坍缩和重复输出，最终造成训练**过早收敛到局部最优**。为了解决这一问题，我们提出了**非对称双重裁剪（Asymmetric Dual-Clipping）**，这是一种重新设计的裁剪策略。实验表明，该策略能有效防止过早收敛，提高训练稳定性，并显著提升LLM的性能。

👨‍💻 [Github](https://github.com/wizard-III/Archer2.0), 🤗 [HF Model](https://huggingface.co/Fate-Zero/Archer2.0-Code-1.5B-Preview), 🤗 [HF Dataset](https://huggingface.co/datasets/Fate-Zero/Archer2.0-Code-1.5B), 🌐 [知乎](https://zhuanlan.zhihu.com/p/1950989602023244983), 📖 [论文](https://arxiv.org/abs/2510.06062)

## 深入理解LLM训练中强化学习的关键见解

- **PPO-Clip的核心设计原则：** 为确保训练稳定性，将那些在更新方向上**已取得显著优势**的token排除在当前的更新之外。
- 在应用于LLM的GRPO类算法中，PPO-Clip的关键作用在于**Token-Masking（token掩蔽）机制**，而**重要性采样带来的权重调整在实践中贡献甚微**。
- PPO-Clip的设计存在**正样本token上的权重失配**问题，这导致训练过程中模型对这些token产生过拟合。进而引发**熵坍缩（entropy collapse）**，最终使模型陷入局部最优。
- 流行的GRPO算法面临着固有的局限性——**搜索空间受限和奖励粒度粗糙**——这限制了模型的学习能力，使其无法超越基础模型（Base Model）的浅层能力。

# 1. 从PPO-Clip的设计原理说起

PPO-Clip技术由OpenAI研究人员在PPO算法的设计中引入。它通过对重要性采样比率进行裁剪（Clipping）来约束信任域（Trust Region），从而增强强化学习的稳定性。具体而言，它鼓励策略向有利的方向发展，同时严格限制每一次更新的最大步长。使用PPO-Clip的PPO损失函数表达式如下：

$$\mathcal{L}_{\mathrm{PPO}}(\theta) = \mathbb{E}_{\tau}\bigg[\min\Big( r_\theta(\tau) A(\tau),\mathrm{clip}\big(r_\theta(\tau), 1 - \varepsilon_{\mathrm{low}}, 1 +\varepsilon_{\mathrm{high}}\big) A(\tau) \Big)\bigg]$$

其中：

- $r_\theta(\tau) = \frac{\pi_\theta(\tau)}{\pi_{\text{old}}(\tau)}$

进一步分析，PPO-Clip的理论核心可以概括为两个方面：

- **Token-Masking（Token掩蔽）：** 将当前策略下的概率与旧策略偏差过大的Token屏蔽（Mask）掉。这确保了每一次更新的“步子不会迈得太大”，从而防止训练崩溃。
- **Importance-Sampling（重要性采样）：** 消除采样策略（旧策略）与当前策略之间的分布差异，确保对期望回报的准确估计。因此，模型可以在同一批采样的**数据上进行多次优化更新**，提高样本效率。

下面展示了PPO-Clip的核心实现（摘自Verl框架）。在讨论后续概念时，我们将时不时回顾这段代码以帮助理解。接下来，我们将详细探讨PPO-Clip是如何实现这两个功能的。

Python

```
negative_approx_kl = torch.clamp(log_prob - old_log_prob, min=-20.0, max=20.0)
ratio = torch.exp(negative_approx_kl)

pg_losses1 = -advantages * ratio
pg_losses2 = -advantages * torch.clamp(ratio, 1 - cliprange_low, 1 + cliprange_high)
clip_pg_losses1 = torch.maximum(pg_losses1, pg_losses2)
pg_clipfrac = verl_F.masked_mean(torch.gt(pg_losses2, pg_losses1).float(), response_mask)

pg_losses3 = -advantages * clip_ratio_c
clip_pg_losses2 = torch.min(pg_losses3, clip_pg_losses1)
pg_clipfrac_lower = verl_F.masked_mean(torch.gt(clip_pg_losses1, pg_losses3) * (advantages < 0).float(), response_mask)

pg_losses = torch.where(advantages < 0, clip_pg_losses2, clip_pg_losses1)
```

## 1.1 Token-Masking（Token掩蔽）

我们先来看看Token掩蔽，以及PPO-Clip究竟掩蔽了哪些token。这里我们只展示最终的掩蔽效果。关于PPO损失公式的详细解析，您可以参考[这篇指南](https://huggingface.co/blog/deep-rl-ppo)。结果如下图所示：

图 1. **标准 PPO 中的裁剪区域。**（来源：https://arxiv.org/abs/2507.15778）

- x轴代表旧策略下的**token概率**（`prob`）。
- y轴代表当前策略下的**token概率**。
- 坐标系中的每个点代表 $f(x, y) = y/x$，即公式中的**重要性比率 $r$**。

被掩蔽的token可以分为两种情况：

1. **当 Adv > 0 时：** **$r > 1.2$（$1 + \text{cliprange\_high}$）** 的token被掩蔽（对应左图中的蓝色区域C）。这些token被排除在梯度计算之外，不参与训练。其余两个区域，即绿色A和红色B中的token则不做处理——它们的梯度和重要性比率都被保留。其原理是：当 $Adv > 0$ 时，我们沿着左图中箭头的方向增加所有token的概率。但如果某些token的概率已经远高于旧策略（蓝色区域C），继续增加其概率可能会导致新训练的策略与旧策略偏差过大，从而造成训练不稳定。
2. **当 Adv < 0 时：** **$r < 0.8$（$1 - \text{cliprange\_low}$）** 的token被掩蔽（对应右图中的红色区域B）。这是因为当 $Adv < 0$ 时，我们沿着箭头的方向降低这些token的概率。同理，如果当前策略对某个token的概率已经远低于旧策略，进一步降低它可能会导致更新后的策略发生显著偏离，再次危害稳定性。

此外，当 $Adv<0$ 时，我们仅掩蔽图中下部的token（红色区域B）。然而，在蓝色区域C的最左侧部分，当 $x$ 很小而 $y$ 很大时，比率 $y/x$ 会变得极大。这种极端的重要性比率可能会主导回报估计，引起方差爆炸，进一步影响训练稳定性。为了解决这个问题，当 $Adv < 0$ 时，通常会应用**双重裁剪（dual-clip）**，对紫色区域D中的token进行掩蔽。在上面的代码中，`pg_losses3` 部分实现了这个操作（尽管实现上似乎有点问题，我们将在稍后讨论）。

最终，参与训练的token包括：

- 对于 $Adv>0$（左图）：绿色区域A + 红色区域B
- 对于 $Adv<0$（右图）：绿色区域A + 蓝色区域C

总而言之，我们可以推导出一个**PPO-Clip的核心设计原则：** **为了确保训练稳定性，将那些在更新方向上已取得显著优势的token排除在当前的训练更新之外。**

此外，我们回顾 `verl` 的代码实现。虽然代码对所有token的重要性比率 $r$ 都应用了上下界限制（`torch.clamp(ratio, 1 - cliprange_low, 1 + cliprange_high)`），但在实践中：

- 对于**正样本**（$adv > 0$），只有**上界** `1 + cliprange_high` 是有效的。
- 对于**负样本**（$adv < 0$），只有**下界** `1 - cliprange_low` 是有效的。

------



## 1.2 Importance-Sampling（重要性采样）



**重要性采样的原理**——当难以直接从目标概率分布 $p(x)$ 中采样时，先从更容易采样的替代提议分布 $q(x)$ 中抽取样本。然后，通过计算一个名为“重要性权重”（Importance weight）的比率 $p(x)/q(x)$ 来调整这些样本，以估计函数 $f(x)$ 在 $p(x)$ 下的期望值。

在RL训练中，引入重要性采样的初衷是**提高样本效率**。在标准的策略梯度方法中，通常每个数据样本只用于一次梯度更新。而PPO的不同之处在于，在收集了一批采样数据后，它对同一批数据执行**多次优化更新**。虽然这提高了样本利用率，但也引入了采样策略与当前策略分布之间的差异。“重要性权重”就是用来校正这种偏差的。

从理论上讲，引入重要性采样是完全合理的。然而，早在几年前我刚开始研究PPO时，我就有一个疑问：**在LLM场景中，重要性权重的作用究竟有多大？** 在LLM的RL训练中，采样策略与当前策略之间的步数通常很小，学习率也较低。此外，在KL和裁剪约束的附加限制下，采样策略与当前策略之间的参数差异通常很微小，这意味着它们的生成分布差异很小。

那么，重要性权重在实践中的效果究竟有多重要？如果它被证明是微不足道的，这个比率本身是否会引入其他问题？毕竟，除了调整策略分布差异外，重要性权重还有另一个明确——但常常被忽视——的效果：它会**影响每个token的训练权重**。我们将在后续章节中详细分析这一方面。

------



# 2. GRPO时代下的 PPO-Clip 变体



今年早些时候，随着 **DeepSeek-R1** 模型的发布，大语言模型（LLM）的强化学习范式逐渐从PPO时代过渡到了GRPO时代。在过去的六个月中，涌现了各种基于GRPO的算法，其中著名的例子包括 **Seed的DAPO**、**Minimax的CISPO** 和 **Qwen的GSPO**。

GRPO类算法与PPO的核心区别在于**优势值（Advantage）的计算方式。PPO使用Token-level Advantage（Token级别的优势值）**，其中每个token都分配有一个单独的优势值，通过价值模型和奖励模型分别计算。相比之下，GRPO类算法使用**Response-level Advantage（响应级别的优势值）**：对于给定的一个提示，采样多个响应，并使用以下公式计算这组响应的标准化得分。这个单一得分随后被分配为该响应内所有token的统一优势值。换句话说，**同一响应内的所有token共享相同的优势值**，没有token级别的差异化。

$$\hat{A}_{i,t} = \frac{R_i - \mathrm{mean}\left(\{R_j\}_{j=1}^G\right)}{\mathrm{std}\left(\{R_j\}_{j=1}^G\right)}$$

接下来，我们将从**Token-Masking**和**Importance-Sampling**这两个关键角度，系统回顾各种GRPO算法对PPO-Clip所做的改进。

------



## 2.1 Token-Masking（Token掩蔽）的相关改进



PPO-Clip引入的Token掩蔽机制无疑有助于训练的稳定性。如前所述，该机制可以防止在更新方向上已获得较大优势的token参与训练。通过这样做，它确保了更新后的策略不会与旧策略偏离过远，从而有效地维护了训练的稳定性。大量的实验验证逐渐强化了人们的认识：**该机制对于稳定强化学习训练至关重要**。它的设计简单、高效、真正优雅——一个绝妙的设计！

在过去的六个月中，出现了一些完善Token掩蔽细节的工作。这些方法都属于GRPO算法家族。根据其修改策略，大致可以归类如下：



### 2.1.1 修改裁剪范围



**Clip-Higher**

这类方法的代表是DAPO中提出的 *clip-higher* 策略。DAPO的研究人员指出，原始的上界裁剪阈值在一定程度上会限制低概率token的概率增长，可能限制模型生成的多样性。通过**提高上界裁剪限制**（即代码中的 `cliprange_high` 变量），该方法为提升低概率token的概率提供了更多空间。

需要注意的是，`cliprange_high` 仅对**正样本**——即具有正优势值的token——产生影响。换句话说，*clip-higher* 策略有效地**扩大了正样本的裁剪范围**，而负样本的裁剪范围保持不变。本质上，该方法沿着优势值维度对token进行了分类，并相应地调整了裁剪范围，如下图所示。

图 2. **DAPO Clip-Higher 策略中的裁剪区域。**

然而，“限制低概率token增长”的根本原因实际上在于token本身，并不局限于正样本。事实上，低概率token在正样本和负样本中都可能存在。这种改进没有考虑到负样本中的低概率token，同时却扩大了正样本中高概率token的概率增长。这可能会**加速策略熵的下降**，并增加模型偏离基线的风险。

尽管 *clip-higher* 仍然是一种不完美的改进策略，但它是第一个明确指出PPO-Clip裁剪范围局限性并提出针对性调整的方法，提供了重要的概念启示——绝对是一个值得注意的贡献。

**Dual-Token-Clip**

为了解决上述问题，最近的一项工作提出了在**Token级别**区分裁剪范围：低概率token使用较大的裁剪范围，而高概率token使用较小的范围。这种方法可以防止高概率token干扰整体更新范围，从而允许将裁剪范围设置得更大。例如，在DAPO中，*clip-higher* 值通常设置为 0.28，而在 Dual-Token-Clip 策略中，它可以放宽到 0.4 甚至更高。实验结果表明，该策略在训练稳定性和最终性能上均优于 *clip-higher*。

此外，如下图所示，Dual-Token-Clip 策略定义了两个独立的调整范围（E和F），分别对应正优势和负优势区域。由于计算限制，目前尚未完全探索每个范围对训练的具体影响和机制。（**TODO：** 在未来的更新中进行消融实验并提供进一步分析。）

图 3. **Dual-Token-Clip 策略中的裁剪区域。**

**Dynamic-Clip**

在 Dual-Token-Clip 工作的基础上，我们进一步思考：如果token可以被划分为两种类型并分别施加约束，我们是否可以走得更远，更精细地调整裁剪范围？这导致了为每个token分配一个独立的裁剪范围，实现**Token级动态裁剪**的想法。这项工作目前正在进行中，初步实验显示出积极的结果。然而，目前的设计在机制上还不够简单和优雅，尚未达到我们理想的状态。因此，目前不进行公开分享，可能会在随后的正式技术报告中包含。

最近的一项工作，*[DCPO: Dynamic Clipping Policy Optimization](https://arxiv.org/abs/2509.02333)* ，提出了一个相似的优化方向，也专注于动态调整裁剪机制，尽管具体设计有所不同。



### 2.1.2 Soft Clip 策略



在token裁剪过程中，有些方法保留了梯度，而只限制重要性权重的大小。一个代表性的例子是Minimax提出的**CISPO**。其损失函数定义如下：

$$\mathcal{J}_{\text{CISPO}}(\theta) = \mathbb{E}_{(q,a)\sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot \mid q)} \left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \text{sg}(\hat{r}_{i,t}(\theta)) \hat{A}_{i,t} \log \pi_\theta(o_{i,t} \mid q, o_{i,<t}) \right],$$

其中：

$$\hat{r}_{i,t}(\theta) = \text{clip}\!\left( r_{i,t}(\theta), \, 1-\epsilon^{IS}_{\text{low}}, \, 1+\epsilon^{IS}_{\text{high}} \right).$$

与PPO-Clip相比，CISPO有两个主要区别：首先，CISPO没有使用PPO的单边裁剪机制。相反，无论优势值是正还是负，它都对上界和下界进行裁剪。其次，在裁剪之后，CISPO没有掩蔽（Mask）token，而是**保留了其梯度**，使其继续参与训练，同时将超出范围的重要性权重裁剪到预设的边界（通常上界为1.2，下界为0.8）。我们将这种机制称为**Soft Clip策略**。

当CISPO首次发表时，我们受到启发，思考这种Soft Clip机制是否可以直接应用于PPO-Clip的单边裁剪。具体来说，对于 `advantage > 0` 且超出上界，以及 `advantage < 0` 且低于下界的token，我们保留梯度进行更新，而仅将重要性权重设置为 1.2 或 0.8。然而，仔细检查后，发现了两个问题：

1. **违反PPO-Clip的设计初衷：** PPO-Clip旨在掩蔽在更新方向上已具有显著优势的token，以防止策略更新步长过大，确保训练稳定性。使用Soft Clip允许这些token继续参与更新，这很容易导致训练崩溃。在一个 1.5B 模型上，在相对较大的离策略设置（`ppo_epochs=3`，`batch_size / mini_batch_size = 4`）的对比实验中，PPO-Clip保持稳定，而使用Soft Clip则在大约100步左右崩溃。
2. **重要性权重失配：** 裁剪后，token的重要性权重被统一设置为 0.8 或 1.2，这改变了原始的重要性采样比率。虽然理论上这构成了失配，但其在实践中的影响可能有限。

这两个问题也存在于标准的CISPO算法中。因此，我们得出结论，**不建议将Soft Clip直接应用于PPO-Clip的裁剪机制**。尽管如此，这种策略仍然可以应用于其他上下文，我们将在稍后讨论。与DAPO类似，尽管存在这些问题，Minimax仍值得认可，因为它是第一个将Soft Clip机制引入PPO-Clip的工作。



### 2.1.3 Response-level Clip（响应级别裁剪）



目前，只有**GSPO**采用了这种裁剪策略。与其他进行Token级别裁剪的方法不同，GSPO在**响应级别**进行裁剪：它首先计算一个响应中所有token的平均重要性权重，作为该响应的代表性权重。如果这个平均值与预设范围显著偏离，则**整个响应被丢弃**。

当我第一次接触这个策略时，我立即质疑其合理性。

在传统的Token级别裁剪中，被裁剪的token比例通常非常低。例如，在我对 1.5B 模型的实验中，裁剪比率通常低于 0.001。换句话说，在一个包含约 20,000 个token的响应中，通常只有大约 20 个token——那些具有极端偏差的token（正样本大于 1.2 或负样本小于 0.8）——被裁剪，而绝大多数token仍在合理范围内。

GSPO采用的响应级别裁剪策略可能存在以下问题：

首先，使用整个响应的平均重要性权重作为裁剪标准意味着少数**极端值很容易夸大或缩小平均值**，导致整个响应被错误地丢弃。在许多情况下，裁剪少量异常token就足以纠正响应。然而，GSPO的方法是**完全丢弃所有信息**，降低了采样数据的利用率，造成浪费。

其次，即使在极端值较少、整体权重相对稳定的响应中，GSPO仍然保留所有token进行训练。这显然违反了PPO-Clip引入Token掩蔽的初衷——即掩蔽在**优势更新方向上显著偏离原始策略**的token。继续使用这些token可能反而会破坏模型训练的稳定性。

此外，论文提到GSPO裁剪的token数量比GRPO多约100倍。这并不奇怪：Token级别的裁剪比率极低（约 0.1%），而在响应级别方法中，只要一组中有一个响应被完全丢弃，裁剪比率就会急剧增加。具体来说，如果一个提示对应 G 个响应，裁剪比率将扩大约 1k / G 倍。

基于此分析，我们认为GSPO粗粒度的**响应级别裁剪机制是不合理的**。

至于GSPO的另一个修改——在响应级别计算重要性权重以与奖励信号的粒度对齐——我们暂不讨论其有效性。假设该机制是有效的，那么对标准GSPO更合理的改进将是：

- 保持**Token级别裁剪**，以精细控制策略更新；
- 仅基于裁剪后剩余的token计算**响应级别的重要性权重**，并将其用于平均值计算。

------



## 2.2 Importance-Sampling（重要性采样）的改进



到目前为止，唯一修改重要性采样的似乎只有GSPO。该论文首先指出了GRPO算法中的一个**根本性问题**：**奖励信号的粒度与重要性采样的粒度不匹配**。如前所述，GRPO类算法中的优势值是在**响应级别**计算的，这意味着同一响应内的所有token共享相同的优势值。然而，重要性采样却是在**Token级别**执行的，每个token都乘以其对应的重要性权重来校正分布。

GSPO的作者认为，这种Token级别的重要性采样会引入误差，随着响应长度的增加而累积，最终导致训练不稳定甚至模型崩溃。原始论文中指出：

为了解决这个问题，GSPO修改了GRPO中的重要性采样机制，使其与**响应级别奖励**对齐。具体来说，它平均了同一响应中所有token的重要性权重，以获得一个代表整个响应的单一权重。修改后的GSPO损失函数定义如下：

但是，切换到响应级别权重是否合理？我们在这里不急于下结论，将在后续章节中更详细地探讨。现在，我们首先比较实验结果。

为了清楚地评估响应级别权重的影响，我们进行了对照实验，同时保留了原始PPO-Clip中的Token掩蔽机制。我们设置了两个实验组：一个使用GRPO的**Token级别重要性权重**，另一个使用GSPO的**响应级别权重**，所有其他设置都相同。大多数训练指标显示两者趋势相似，仅在熵上略有差异——使用GSPO权重的模型表现出**略快的熵衰减**。结果如下：

图 4. 训练过程中的 Actor/熵：GSPO vs. GRPO。

图 5. 训练过程中 LiveCodeBench 上的性能：GSPO vs. GRPO。

从结果来看，使用GSPO的响应级别重要性权重确实**加速了收敛**，测试集性能上升更快，这与论文中的说法一致。

然而，在快速达到峰值性能后，GSPO也**更早开始下降**，最终收敛结果与GRPO大致相当。在峰值性能方面，GSPO没有表现出显著优势。事实上，GSPO论文中报告的实验主要显示了更快的训练速度，但没有提供证据证明最终性能超越了GRPO。

我们试图分析响应级别权重可能加速学习的原因：在该机制下，响应中的所有token不仅共享相同的优势值，还共享相同的重要性权重。这意味着在更新过程中，一个token的权重完全由其概率决定——**低概率token具有更大的梯度，从而主导优化**。本质上，这为低概率（高熵）token分配了更高的优化权重。一些研究表明，RL训练主要优化高熵token，因此GSPO的机制可能确实通过关注它们来加速学习。然而，这种加速不一定能转化为更好的最终性能，且收敛结果不能保证优于原始GRPO方法。

------



# 3. Importance Sampling is not Important（重要性采样并不重要）





## 3.1 动机



重要性采样（IS）是估计一个概率分布下期望值，同时从另一个分布采样的基础技术。在RL中，这种方法很有价值，因为当前策略的轨迹分布通常与旧策略不同。通过使用重要性比率重新加权回报，可以重用旧策略下收集的数据，从而以更高的样本效率支持离策略学习（Off-Policy Learning）。

在LLM中基于结果的RL（如GRPO）的背景下，同一响应内的所有token共享相同的优势值。然而，如果我们考察单个token，并考虑其对最终正确性的实际贡献，GRPO分配的优势值可能存在两个问题：（1）它在数值上可能不准确，因为不同token对最终答案正确性的贡献可能不均等；（2）更新方向可能产生误导，因为一个最终答案正确的响应仍可能包含不正确的中间步骤。

引入IS的初衷是通过校正分布失配来更好地估计期望的Token级别回报。然而，这引出了以下问题：

- **如果由于基于结果的优势值估计，分配给每个token的奖励已经不准确，那么使用IS权重进一步调整分布的重要性有多大？**

与GSPO将重要性权重统一到响应级别的方法不同，我一直在思考一个更根本的问题：在LLM的RL背景下——或者更谨慎地说，在将GRPO类算法应用于LLM训练时——**重要性采样到底有多重要？** 这种分布校正机制在实践中究竟贡献了多少？

------



## 3.2 初步实验



为了研究IS权重在基于结果的RL中的实际效果，我们设计了两种方法进行比较。两种方法使用相同的Token掩蔽策略，但在IS权重上有所不同：

- **方法 1：** 带有原始IS的GRPO，即标准PPO-Clip。
- **方法 2：** 不带IS的GRPO（所有IS权重固定为 1.0）。

为了更清楚地观察IS的效果，实验采用了相对较高的离策略更新程度：`ppo_epochs = 3`，`batch_size / mini_batch_size = 4`。所有后续实验均遵循此配置。

图 6. **标准 GRPO（蓝色）与不带 IS 的 GRPO（橙色）在测试准确度和训练动态方面的比较。**

如上图所示，两种方法在测试集上的总体性能差异很小。然而，**训练指标**（如熵、重复率、截断率和KL散度）在**没有IS的情况下变化更平稳**，尤其是在训练的后期阶段。仔细观察可以发现，在训练接近尾声时，两种方法在测试集上的指标也略有分歧。标准GRPO更早达到峰值，然后开始下降，而另一种方法收敛更平稳。它维持在接近峰值的值附近，没有出现下降趋势。尽管GRPO的峰值略高于不带IS的GRPO，但差异仅约为 0.4 个百分点。这在LLM训练和评估的随机波动范围内。

接下来，我们分析造成这种差异的原因。

图 7. **GRPO 训练过程中响应级别裁剪比率的演变。**

首先，需要注意关于GRPO训练的一些普遍观察结果：

1. 当使用GRPO类算法训练LLM时，**一个响应的平均重要性采样权重通常略大于 1.0**。例如，在我对 1.5B 模型的实验中，它通常在 1.0005 左右。
2. 此外，**正样本的平均权重略高于负样本**，如上图所示。尽管这种差异看起来很微小，但当对数万个token求平均时，它的实际影响是显著的。正负样本权重之间的这种微妙差异导致GRPO训练**更倾向于拟合正样本，而不是抑制负样本**。在训练过程中，这种差距逐渐扩大，加速了正样本的拟合，从而加速了熵衰减的速度。（这与GRPO算法——或更广泛地说，PPO-Clip设计——中固有的局限性有关，我们将在后续文章中详细讨论。）

为了进一步阐明熵更快衰减与正样本加权之间的关系，有必要了解**熵与正/负样本之间的关系**：在GRPO训练中，采样更多负例（例如，来自更难的提示）或手动增加负样本权重会**减缓熵衰减**，甚至可能导致熵暂时上升——尽管模型在测试集上的性能通常保持不变或略微恶化。相反，**增加正样本的权重会加速熵衰减**，通常导致最终性能略差。将这一观察结果与先前关于熵机制的研究（例如，来自上海AI Lab的研究）相结合，我们可以进一步得出结论：在GRPO训练中，观察到的训练熵变化可能主要由**低熵token**驱动，这种熵可能无法准确反映高熵token上的探索。

最后，为什么**正样本的平均权重高于负样本**？这与模型预期的学习行为一致：经过几轮训练后，当前策略为正样本分配的生成概率高于旧策略，导致裁剪比率分布向**左上方**区域移动。相反，负样本低于旧策略，将裁剪比率移向**右下方**区域。这表明模型正朝着正确的方向学习，逐渐区分正负样本的预测概率。
