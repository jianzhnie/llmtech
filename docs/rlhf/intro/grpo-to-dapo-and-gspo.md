# 浅谈 GRPO及其改进算法

在大型语言模型的强化学习阶段，PPO 曾经是主流方法。然而，它依赖于价值模型，在处理长文本输出和复杂任务时显示出局限性。GRPO 消除了对价值模型的依赖，显著提高了可扩展性，但在效率和稳定性方面仍有优化空间。这促使了 DAPO 的出现，它细化了采样、裁剪和梯度计算等细节。然而，在动态激活专家的 MoE（混合专家模型）架构中，GRPO 框架下的Token-Level优化仍然难以稳定收敛。GSPO 进一步将优化粒度转移到 Sequence-Level，从根本上减少了高方差和结构噪声。本文遵循这一进化路径：从 GRPO 开始，逐步揭示 DAPO 和 GSPO 背后的设计动机和实现细节。

在接下来的文章中，您将发现：

1. 为什么 GRPO 摆脱了 PPO 对价值模型的依赖，但在某些情况下仍然可能“崩溃”。
2. Clip-Higher 如何修复优质Token过早被截断的隐患
3. 动态采样如何防止无效样本造成大量计算浪费。
4. Token-Level梯度损失如何确保长响应不再稀释有价值的梯度信号。
5. 为什么 GRPO 的逐Token重要性采样在 MoE 架构中产生巨大方差。
6. GSPO 如何用Sequence-Level优化替换Token-Level优化，从根本上提高稳定性和效率。



## PPO 回顾

PPO [Schulman et al., 2017] 是目前 RL 领域，尤其是 RLHF（Reinforcement Learning from Human Feedback）中应用最广泛的算法之一。它属于“行动者-评论家（Actor-Critic）”框架，通过优化一个“裁剪后（clipped）”的替代目标函数来提升策略的稳定性。其核心思想是在鼓励策略向好的方向更新的同时，通过一个裁剪超参数 $ \epsilon $ 来限制新旧策略之间的差距，防止因单步更新过大而导致训练崩溃。

PPO 的目标函数可以表示为：
$$
 J^{PPO}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)\hat{A}_t \right) \right]
$$
其中：

- $ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} $ 是新旧策略在状态 $ s_t $ 下采取动作 $ a_t $ 的概率比。
- $ \hat{A}_t $ 是在时间步 $ t $ 的优势函数估计值，通常使用广义优势估计（GAE, Generalized Advantage Estimation）[Schulman et al., 2018] 计算得出。它表示在当前状态下，采取某个动作比平均水平好多少。
- $ \text{clip} $ 函数将概率比限制在 $ [1 - \epsilon, 1 + \epsilon] $ 区间内。

这个 $ \min $ 操作是 PPO 的精髓：

- 当优势 $ \hat{A}_t > 0 $（即这是一个好动作）时，目标函数变为 $ \min(r_t(\theta)\hat{A}_t, (1 + \epsilon)\hat{A}_t) $。这鼓励我们增大 $ r_t(\theta) $，但增幅被 $ 1 + \epsilon $ 限制，防止策略更新过于激进。
- 当优势 $ \hat{A}_t < 0 $（即这是一个坏动作）时，目标函数变为 $ \max(r_t(\theta)\hat{A}_t, (1 - \epsilon)\hat{A}_t) $（因为 $ \hat{A}_t $ 为负，所以 min 变成了 max）。这鼓励我们减小 $ r_t(\theta) $，但减幅被 $ 1 - \epsilon $ 限制，同样是为了稳定。



## GRPO 回顾

GRPO [Shao et al., 2024] 是DeepSeek在训练其数学大模型DeepSeekMath时提出的一种方法。它对PPO进行了简化和改进，核心特点是**去掉了价值网络（Critic）**，直接通过对同一提示（prompt）产生的多个输出（responses）的奖励进行归一化来估计优势。

GRPO 的训练目标是：
$$
J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O \mid q)} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left( \min \left( r_{i,t}(\theta) A_i, \text{clip}(r_{i,t}(\theta), 1-\varepsilon, 1+\varepsilon) A_i \right) - \beta \, D_{\text{KL}}(\pi_{\theta} \parallel \pi_{\text{ref}}) \right) \right]
$$

其中

$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})}
$$

$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}
$$

- $r_i$ 是回答 $o_i$ 通过奖励模型（或验证器）获得的标量奖励。
- $\text{mean}(\{r_1, r_2, \ldots, r_G\}) $ 是组内所有回答奖励的平均值。
- ${\text{std}(\{r_1, r_2, \ldots, r_G\})}$ 是组内所有回答奖励的标准差。

这个公式的直观解释是：

- 一个回答的优势不再是与一个学习到的价值函数进行比较，而是与其“同伴”（在同一次采样中生成的其他回答）进行比较。
- 如果一个回答的奖励高于组内平均水平，它就获得正优势；反之则获得负优势。
- 通过标准差进行归一化，可以使优势值保持在稳定范围内。这种方法巧妙地利用了奖励模型的比较天性，因为奖励模型本身通常就是通过比较成对的输出来训练的。

在理解 GRPO 目标之后，我们首先需要澄清重要性采样的作用和局限性，这不仅对理解 GRPO 至关重要，也是 DAPO 和 GSPO 引入改进的切入点。

### 重要性比率扮演什么角色？

重要性采样的本质是，我们希望在新分布下计算期望，但我们的数据是从旧分布中抽取的。因此，我们使用新旧策略下相同动作的概率比率作为校正权重：

$$
\mathbb{E}_{\text{pnew}}[f(x)] = \mathbb{E}_{\text{pold}}\left[ \frac{p_\text{pnew}(x)}{p_\text{old}(x)} f(x) \right]
$$

这使我们能够使用旧策略的离线数据评估新策略下的期望值，避免了每次更新后重新采样的需要（从而降低成本）。然而，如果新旧策略之间的差距过大，权重的方差可能变得非常高，导致训练不稳定。

在 RL 训练里，引入重要性采样的初衷其实是为了提高样本的使用效率 —— 在标准的策略梯度方法中，每个数据样本只进行一次梯度更新不同，PPO 与其不同，是在得到一批采样数据后，进行多次的优化更新，这样固然可以提高样本的利用率，但是同时也带来了采样策略和当前策略分布差异的问题，“重要性权重” 的引入就是为了解决这种偏差。

重要性采样的目的是在我们只有行为分布样本的情况下，估计目标分布下的期望。在 PPO/GRPO 中，我们并不直接从新策略中采样数据；相反，我们首先使用旧策略生成数据（因为采样成本高），这个过程称为 **rollout**。在更新时，我们必须修正分布不匹配的问题，这就是重要性采样的用武之地。在采样后为每个Token定义重要性比率为：

$$
r_t = \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}
$$

PPO/GRPO 目标可以写成：

$$
L(\theta) = \mathbb{E}_t \left[ \min(r_t A_t, \text{CLIP}(r_t, 1-\varepsilon, 1+\varepsilon) A_t) \right]
$$

这里，

$A_t$是计算得到的优势，而裁剪操作通过限制更新幅度来防止策略与旧策略产生过大偏离。

有了这种重要性采样的直观理解，我们可以进一步考虑它在 PPO/GRPO 中的实际效果：优势函数 $A_t$的符号和比率 $r_t$一起决定了策略更新的方向和幅度。

### $A_t$和 $r_t$的符号如何影响训练？

让我们分析这些情况。

- 假设 $A_t > 0$（动作比预期更好）

  - 我们希望增加这个动作的概率。如果我们在裁剪中设置 $\varepsilon = 0.2$，那么当 $r_t > 1.2$时，`min` 和 `clip` 操作将其限制在 1.2。当 $r_t < 0.8$时，由于 `min` 操作，不发生裁剪，因此正优势的变化受到限制。

- 当 $A_t < 0$（动作比预期更差）

  - 我们应该减少这个动作的概率。如果 $r_t < 0.8$，`min` 操作进一步限制它，限制在 0.8$A_t$；但当 $r_t > 1.2$时，`min` 操作不施加限制（它可以上升到 +∞，带负号变为 -∞）。因此，负优势的向下调整也受到限制。

- $A_t$ 衡量当前动作/轨迹是否优于或劣于平均水平

  - 如果 $A_t$为正，我们鼓励它；

  - 如果是负的，我们惩罚它，以便它在未来出现的更少。

- 重要性比率 $r_t$ 反映了新策略选择这个动作的概率相较于旧策略的增加（或减少）程度。

  - 如果 $r_t > 1$，新模型更倾向于该动作；

  - 如果 $r_t < 1$，倾向性降低。

- 在 $A_t$和 $r_t$的四种可能符号组合中，我们只希望有两种：

  - 当它们符号相同时，正 $A_t$ 与 $r_t > 1$（加强）
  - 负 $A_t$ 与 $r_t < 1$（修正错误）。

然而，匹配 $A_t$ 和 $r_t$ 的符号还不够。在 PPO/GRPO 中，**裁剪操作** 对于稳定训练同样至关重要，因为它决定了哪些Token的梯度真正有助于更新。

### 裁剪对梯度和Token效率的影响

对于 $A_t > 0$，当 $r_t > 1+\varepsilon$ 时，即增长达到上限时，我们应用裁剪操作，此时梯度归零。这实际上取消了该Token对训练的贡献。同样，对于 $A_t < 0$，如果 $r_t < 1-\varepsilon$，即下降幅度超出限制，裁剪裁剪同样会使梯度归零。一个常见的误解是裁剪使用直通估计器将裁剪值的梯度传递回未裁剪值；实际上，这并没有发生：裁剪前的梯度直接设置为零。

此时，我们对 GRPO 的机制、优势和局限性有了相对完整的理解。接下来，我们将看到 DAPO 如何在保留 GRPO 基本框架的同时，引入更细粒度的改进来解决效率和稳定性挑战。

## Dr.GRPO 修正固有偏差

Dr.GRPO对GRPO的深入分析揭示了其目标函数中存在的两种系统性偏差：

- Response-level length bias:

源于目标函数中的 $\frac{1}{|o_i|}$ 项。当优势为正时，这个因子会给予较短的正确回答更大的梯度更新，鼓励模型生成简短的正确答案。然而，当优势为负时，它会使得较长的错误回答受到更小的惩罚，从而无意中鼓励模型在犯错时“胡言乱语”，生成更长的错误答案。

- Question-level difficulty bias:

源于优势计算中的 $\frac{1} {std({R_{j}})} $ 项。对于那些所有回答都正确（非常简单）或都错误（非常困难）的问题，组内奖励的标准差会非常小，接近于零。这会导致优势值被极大地放大，使得这些简单或困难的问题在梯度更新中占据过大的权重，从而产生对特定难度的偏好。

- 解决方案:

Dr. GRPO 的解决方案非常直接：移除这两个导致偏差的归一化项。即，在计算策略梯度时不除以回答长度 ，在计算优势时不除以奖励的标准差。修正后的无偏策略梯度更接近于标准的PPO目标，其优势项简化为中心化的回报：

$$
A_i = {r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}
$$

### 关于**响应长度偏差**的讨论

Dr.GRPO 在计算每个 Response 的 Loss 时，将除以的 |oi| 也就是 response length 替换为一个固定值 MAX_TOKENS，即训练时设置的 Response 的最大长度；

```python
# GRPO
def masked_mean ( tensor , mask , dim ) :
    return ( tensor * mask ).sum ( axis = dim ) / mask .sum( axis = dim)

# Dr.Grpo
def masked_mean ( tensor , mask , dim ) :
    return ( tensor * mask ).sum ( axis = -1) / MAX_TOKENS
```

关于 response-level length bias 这一问题，在DAPO 中也有基本相同的陈述，但是两者在改进方式上略有差异，区别就在于在处理不同的 group 时，Dr.GRPO 始终使用的是同一个除数 G * max_length, 而 DAPO 是使用的各个group 内部的总 token 数，那么这两种方式孰优孰劣呢？

我倾向于 Dr.GRPO 的做法，不同的group，也就是不同 prompt 产出的 response 的长度天然是有差异的，在 group 之间同样存在长度偏差的问题（平均Response长度较短的 Prompt，在总体损失中的贡献要更大，但很明显，这种学习趋势是不符合我们的预期的） DAPO的做法则忽略了这种差异。

### **关于移除方差项的讨论**:

在强化学习中，通过奖励的标准差来归一化优势（即优势标准化）是一种常见的稳定化技术。它将优势值缩放到一个固定的范围内（通常是均值为0，方差为1），这有助于稳定梯度更新，防止因奖励尺度变化而导致的训练不稳定。然而，Dr. GRPO的研究者发现，在LLM推理任务的特定背景下，这种稳定化措施带来了一个副作用，即“难度偏差”。对于那些模型能轻易解决或完全无法解决的问题，组内奖励的方差趋近于零，导致优势被放大到极端值，从而主导了梯度更新。

因此，Dr. GRPO选择移除方差项，这是一种设计上的权衡。它放弃了优势标准化带来的梯度稳定性，以换取对所有难度问题一视同仁的无偏学习信号。这表明，虽然方差归一化在通用RL中是一个有用的工具，但在特定应用场景下，它可能引入不期望的归纳偏置。后续算法（如LitePPO）则采取了折衷方案，在更大的批次（batch）级别上计算方差，以获得更鲁棒的估计，这反映了社区对这一问题仍在不断探索和优化。

- **影响**: 通过消除这些偏差，Dr. GRPO在保持与GRPO相当的推理性能的同时，显著提升了token效率。它有效抑制了模型在训练过程中生成越来越长的错误回答的趋势，使得训练更加稳健和高效。

## 从 GRPO 到 DAPO

DAPO旨在解决几个在规模化训练中常见的棘手问题：

- **熵崩溃 (Entropy Collapse)**: 在RL初始阶段，策略的熵（随机性）可能会迅速下降，导致模型过早地变得确定化，从而限制了探索，陷入局部最优。
- **奖励噪声 (Reward Noise)**:  对于因超出长度限制而被截断的回答，简单的奖励设计可能会惩罚一个本应正确的推理过程，从而引入噪声，影响训练稳定性。
- **梯度消失问题 (Gradient-decreasing Problem)**: 当一个采样批次（batch）中的所有回答都正确（准确率100%）或都错误（准确率0%）时，组内所有回答的奖励都相同，导致计算出的优势（advantage）全为零。这样的批次对策略更新没有任何贡献，浪费了计算资源，降低了训练效率。

DAPO通过四个关键技术来应对上述挑战，这些技术共同构成了其优化目标：

$$
J_{\text{DAPO}}(\theta) = \mathbb{E}_{(q,a) \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O \mid q)} \left[ \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min(r_{i,t}(\theta) A_i, \text{clip}(r_{i,t}(\theta), 1-\varepsilon_{\text{low}}, 1+\varepsilon_{\text{high}}) A_i) \right]
$$

$$
s.t.,  0 < \left| \{o_i \mid \text{is\_equivalent}(a, o_i)\} \right| < G
$$

### DAPO - Clip-Higher

- 为什么 DAPO 提高上限 $1+\varepsilon_{\text{high}}$同时保持 $1-\varepsilon_{\text{low}}$固定？

作者观察到，选择一个小的 $\varepsilon$ 作为裁剪上限可能导致以下问题：如果旧策略为采样Token分配了非常低的概率，但其优势是正的（意味着旧模型采样了优质内容），当前策略几乎没有空间增加其概率，而这恰恰是我们期望实现的目标。

例如，如果旧策略的概率是 0.9 且 $\varepsilon = 0.2$，上限是 0.9×1.2=1.08，已经超过了最大概率 1.0，所以它永远不会被裁剪。但如果旧策略的概率是 0.2，上限变为 0.2×1.2=0.24。在这种情况下，即使当前策略将概率提高到 0.4（一个很好的改进），过小的 $\varepsilon$ 导致它被裁剪，有效地丢弃了该Token。这就是为什么 DAPO 采用 **Clip-Higher**，提高上限提高了Token效率。它将截断区间的上下界解耦，即通过设置一个比 $\varepsilon_{\text{low}} $更大的 $ \varepsilon_{\text{high}}$，该策略允许那些原本概率较低的token有更大的空间来提升其概率，从而增强了策略的熵，促进了生成样本的多样性和探索。

这本质上就是我们所说的“马太效应”：*富者愈富，穷者难以改善*。如果旧策略几乎以非常低的概率采样了一个关键Token，比如说 `"Wait"`，但当前模型显著增加了该概率，它仍然可能被裁剪掉，剥夺了模型“扭转局面”的机会。

Clip-Higher 解决了“好Token过早被限制”的问题，但它并没有解决另一个常见的浪费来源：缺乏采样多样性。为了解决这个问题，DAPO 引入了 **动态采样**。

### DAPO - 动态采样

DAPO 的第二个创新是 **动态采样**。动机如下：假设对于给定的 Prompt 我们采样了 10 个响应，所有 10 个响应要么非常好要么非常差，始终获得最大奖励或零奖励。由于 GRPO 的计算方法，所有 10 个样本的优势为零，因此贡献的梯度为零。

这意味着实际贡献梯度的有效样本数量远低于名义采样量，从而导致高方差、训练不稳定及样本浪费。这种效应在训练初期（模型性能较差时）和后期（模型已能频繁生成完美回答时）表现得尤为明显。

为应对此问题，DAPO 实施了一项附加抽样规则：对于每个查询，采样得到的响应集合不得全为 0 分或全为 1 分。若所有样本均为 0 分或均为 1 分，则需持续补充抽样直至打破该条件。该约束条件具体表述为：

$$
s.t., 0 < \left| \{o_i \mid \text{is\_equivalent}(a, o_i)\} \right| < G
$$

这确保了每个用于梯度计算的批次都包含混合了正确和错误回答的样本，从而保证了非零优势的存在，使得每个样本都能提供有效的学习信号。

除了采样多样性，GRPO 对于长响应还有另一个隐藏缺陷：**随着响应长度的增加，Token梯度被稀释**。DAPO 的第三个改进通过 **Token-Level梯度损失** 来解决这个问题。

### DAPO - Token-Level梯度损失

DAPO 的第三项创新解决了 GRPO 中存在的梯度权重问题：即每个Token的梯度权重随着采样响应长度的增加而减少。

为什么会这样？假设我们采样两次：一个响应有 200 个Token，另一个有 10 个Token。在 GRPO 的公式中，我们首先在每个样本内平均梯度，然后在批次中平均。这使得第一个响应中的每个Token的权重为 (1/200)×(1/2)，而第二个响应中的每个Token获得 (1/10)×(1/2)。因此，较短响应的Token影响更大。

这种机制的缺陷显而易见：

- 对于复杂问题，长回答本属常态。若这些长回答质量高，其宝贵的梯度信号会被稀释；
- 若因重复累赘导致回答冗长且质量低下，修正信号同样会减弱。
- 在长链思维（Long-CoT）场景中，这既可能妨碍模型学习高质量长序列中的关键推理模式，也可能无法有效惩罚过长回答中的无意义内容。

DAPO 的解决方案：在计算梯度时，对所有样本生成的总Token数进行平均。在我们的示例中，长响应和短响应都给每个Token一个权重 1/(200+10)。这种方法平等对待所有Token，提高了长样本训练的效率。

这对应于从 GRPO 的损失聚合：

$$
\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|}
$$

到 DAPO 的：

$$
\frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|}
$$

经验上，Token-Level损失能带来更稳定的训练过程，防止熵变得过高（导致策略随机行动），并避免熵过低时的探索崩溃（Clip-Higher 也有助于解决）。通过从样本级损失转变为Token-Level损失，DAPO 确保长文本响应能按比例影响最终梯度：每个Token 都会直接影响整体梯度，且不受样本长度制约。

最后一个改进也涉及响应长度，但采取了不同视角：**过长响应对整体奖励的负面影响**。

### DAPO - 过长奖励塑形

为了处理截断引入的奖励噪声，DAPO首先采用超长过滤（Overlong Filtering）策略，即在训练中屏蔽掉被截断样本的损失。此外，DAPO 使引入了软性超长惩罚（Soft Overlong Punishment） 机制调整过长响应的奖励。具体来说，一旦生成的序列超过预定义的第一个长度阈值，就会对Token进行惩罚，随着长度的增加，惩罚线性增加。这个惩罚项被加到基于规则的正确性奖励之上，如果长度超过第二个阈值，惩罚力度将足以抵消正确答案带来的原始奖励，引导模型避免生成不必要的冗长回答。

通过 Clip-Higher、动态采样、Token-Level梯度损失和过长奖励塑形，DAPO 提供了 GRPO 的细粒度改进，显著提高了训练效率和稳定性。然而，在某些架构中，特别是 MoE 中，GRPO 仍然存在 DAPO 无法完全解决的结构问题，这引导我们走向 GSPO。

## GSPO：解决 MoE 训练中的 GRPO 不稳定性

若将 DAPO 视为 GRPO 框架内的“微调与精炼”，那么 GSPO 则迈出了更为根本的一步：它将优化粒度从Token-Level转变为Sequence-Level。这一转变背后的动机是，在 MoE 架构训练期间，GRPO 的重要性采样会引入较大方差与不稳定性。GSPO 的核心思想是在奖励处理过程中减少对逐Token优化的依赖，转而更注重整体序列结果。接下来我们将阐述 GSPO 背后的核心概念。

> **TL;DR：** 传统的算法如 PPO 和 GRPO 通常单独优化模型输出中的每个Token，给一些Token更高的权重，给其他Token更低的权重。虽然这旨在进行细粒度优化，但在长文本、大型模型场景中，它反而可能引入噪声和奖励偏差，导致模型失去方向，甚至突然崩溃。问题的根源在于我们根据完整响应评估模型，但逐个Token进行训练，导致奖励粒度和优化目标之间的不匹配。GSPO 通过从每个Token评分转向Sequence-Level优化来对齐奖励和优化目标。这一转变提供了两个主要好处：

> 1. **稳定性** – GSPO 优化整个序列，减少了Token-Level波动的训练噪声。
>
> 2. **效率** – GSPO 过滤并仅保留高质量样本进行优化，加速收敛并提高结果。
>
>    在 MoE 架构中，好处更大：由于每次推理只激活一小部分专家模块，路由路径是动态的且难以控制。传统方法通常依赖于 **Routing Replay**，在推理期间记录专家激活并在训练期间强制使用相同的路由路径，以确保一致性。虽然有效，但这大大增加了工程成本并限制了性能。GSPO 的Sequence-Level逻辑自然避免了对 Routing Replay 的需求，使 MoE 训练更轻量且更稳定。对于越来越多的大型 MoE 模型，这是一个有价值的突破。例如，QWen3 系列已经采用了 GSPO。从 PPO → GRPO → GSPO，我们看到 LLM 的 RL 优化目标应与任务的性质紧密对齐，同时保持训练逻辑简单、可扩展和可部署。进步通常不是由复杂的技巧驱动的，而是通过对核心问题的洞察。

PPO 在长文本和复杂任务中挣扎的主要原因是它依赖于价值模型：当策略模型输出长序列时，价值估计变得不准确，使得从简单任务泛化到复杂任务变得困难。GRPO 消除了这种依赖，打破了价值模型的瓶颈。然而，GRPO 在 MoE 训练或长时间训练运行中仍然面临稳定性问题：在某个时刻，模型可能会突然崩溃，即使恢复训练或调整参数也常常无法恢复。接下来，让我们分析可能的原因和解决方案。

### 重要性比率扮演什么角色，为什么在 GRPO 中存在问题？

重要性采样允许我们在只有行为分布样本的情况下，估计目标分布下的期望。我们通过在目标策略和行为策略之间的概率比率对样本进行加权来实现这一点。

然而，重要性采样的有效性依赖于对多个样本的平均来修正分布不匹配。在大型模型训练中的问题是在 **每个Token** 上执行重要性采样，单个Token的比率无法有意义地执行分布校正。在Token Level，每个下一步Token的分布只有一个采样点，这使得重要性权重充满了高方差的噪声，特别是在不稳定的 MoE 设置中。这表明 GRPO 的Token-Level计算可能在本质上是次优的。

另一个不匹配：我们的奖励是针对 **整个响应**（Sequence-Level）给出的，但在Token-Level重要性采样中，我们将此奖励均匀地分配给Token（奖励塑形）并尝试单独调整它们。这在奖励信号和优化目标之间创建了粒度不匹配。鉴于我们已经拥有Sequence-Level奖励，为什么不也让 GRPO 的优化成为Sequence-Level？

重要性采样的有效性依赖于对多个样本的平均来修正分布不匹配，但在Token-Level，每个下一步token的分布只有一个采样点，这使得重要性权重充满了高方差的噪声。这种噪声在长序列中会累积，并被截断机制放大，最终导致训练崩溃

### 为什么 GRPO 在 MoE 架构中难以收敛？

**专家激活波动：** 新旧策略可能激活不同的专家，引入结构偏差和噪声。当 $\pi_{\theta_{\text{old}}}$ 更新时，路由机制也可能改变，因此两个策略可能激活完全不同的专家集，即使只过了一个训练步骤。这导致输出概率大幅波动，异常频繁地触发裁剪。被裁剪的Token不贡献梯度，而那些保留的Token通常包含噪声。

理论上，重要性比率应该反映在 **相同** 结构下由参数更新引起的 **概率变化**。但专家变化导致不可预测的、与优化方向无关的高方差波动。这种方差扭曲了策略梯度估计，使训练不稳定，甚至导致崩溃。

### GSPO 之前的 Routing Replay

Routing Replay 在从 $\pi_{\theta_{\text{old}}}$ 采样期间记录专家激活，并强制 $\pi_{\theta}$ 在训练期间使用相同的路由路径。缺点是：高工程和基础设施成本，以及效率低下，$\pi_{\theta}$ 可能找到了更好的路由路径，但被迫遵循旧的。

虽然传统方法使用 Routing Replay 来减轻专家激活不匹配，但 GSPO 完全绕过了这种依赖，从根本上减少了结构方差。

### GSPO Sequence-Level 的优化目标

设$ x $为查询，$ \pi_{\theta_{\text{old}}} $ 为用于采样回复的策略，$ o_{i\_i} = 1^G $为采样得到的回复组，$ \hat{A}_i $为各个回复的组内相对优势，$ \pi_{\theta} $为需优化的当前策略。GSPO 采用以下优化目标：

$$
\mathcal{J}_{\text{GSPO}}(\theta) = \mathbb{E}_{x \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot \mid x)} \left[ \frac{1}{G} \sum_{i=1}^G \min \left( s_i(\theta) \hat{A}_i, \text{clip} \left( s_i(\theta), 1-\varepsilon, 1+\varepsilon \right) \hat{A}_i \right) \right],
$$

其中

$$
s_i(\theta) = \left( \frac{\pi_{\theta}(y_i \mid x)}{\pi_{\theta_{\text{old}}}(o_i \mid x)} \right)^{\frac{1}{|o_i|}} = \exp \left( \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \log \frac{\pi_{\theta}(o_{i,t} \mid x, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid x, o_{i,<t})} \right).
$$

这里的$ s_i(\theta) $即为 GSPO 基于序列似然定义的重要性比率，其中我们进行了长度归一化以降低方差并统一$ s_i(\theta) $的数值范围。

> 如果奖励是Sequence-Level的，重要性比率也应该是Sequence-Level的

从上述内容中，GSPO 使用Sequence-Level比率 $s_i(\theta)$ 替换了 GRPO 逐Token比率 $r_{i,t}(\theta)$，它不再与步骤索引 $t$ 绑定。这个想法是放弃Token-Level目标，转而采用Sequence-Level缩放。这自然导致了 GSPO 的新优化目标：用Sequence-Level的重要性比率替换Token-Level的。

Sequence-Level比率是经过**长度归一化** 的，以减少方差并保持数值在统一量级。如果没有归一化，不同长度的答案会使比率对长度非常敏感。由于同一序列中的所有Token共享相同的重要性比率，如果触发裁剪，将裁剪 **整个序列**，而不仅仅是某些Token。归一化因子 $\frac{1}{|o_i|}$ 也防止了长序列中的少数波动Token导致比率爆炸。

**为什么使用指数而不是直接使用对数似然差异？**

指数是必要的，因为重要性采样的核心公式是：

$$
\mathbb{E}_{z \sim \pi_{\text{tar}}}[f(z)] = \mathbb{E}_{z \sim \pi_{\text{beh}}}\left[ \frac{\pi_{\text{tar}}(z)}{\pi_{\text{beh}}(z)} f(z) \right]
$$

这里，权重必须是 **概率比率**（$\geq 0$），而不是对数概率差异。如果我们直接使用 $\Delta \log p$，它将等同于：

$$
\mathbb{E}[\Delta \log p \cdot A]
$$

这不再是一个无偏的重要性采样校正。

GSPO 在对数空间中通过 $\frac{1}{|o_i|}$ 归一化，然后指数化：

$$
s_i(\theta) = \exp \left( \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log \frac{\pi_{\theta}(y_{i,t} \mid x, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t} \mid x, y_{i,<t})} \right).
$$

这确保了不同序列长度的重要性比率的一致缩放，避免了长序列中少数Token概率变化导致的极端值。停留在对数空间而不进行指数化将使比率对长度敏感，需要调整裁剪范围，并破坏与 PPO/GRPO 中使用的 KL 正则化的兼容性。

### 理论梯度分析：GSPO 与 GRPO

从客观定义来看，关键差异在于重要性比率在梯度计算中的定义与使用方式。

在不进行裁剪的情况下，区别在于是否在同一响应内对Token进行不同的加权。GRPO 根据 $r_{i,t}(\theta)$ 为每个Token分配独立的权重，而 GSPO 对序列中的所有Token应用相同的 $s_i(\theta)$。

GSPO 的梯度：

$$
\nabla_{\theta} J_{\text{GSPO}}(\theta) = \mathbb{E} \left[ \frac{1}{G} \sum_{i=1}^G s_i(\theta) \frac{A_i}{|o_i|} \sum_{t=1}^{|o_i|} \nabla_{\theta} \log \pi_{\theta}(o_{i,t} \mid q, o_{i,<t}) \right]
$$

这里，响应中的所有Token共享相同的权重 $\frac{s_i(\theta) A_i}{|o_i|}$，确保了序列内梯度的一致性。

GRPO 的梯度：

$$
\nabla_{\theta} J_{\text{GRPO}}(\theta) = \mathbb{E} \left[ \frac{1}{G} \sum_{i=1}^G A_i \frac{|o_i|}{|o_i|} \sum_{t=1}^{|o_i|} r_{i,t}(\theta) \nabla_{\theta} \log \pi_{\theta}(o_{i,t} \mid q, o_{i,<t}) \right]
$$

这里，权重 $\frac{r_{i,t}(\theta) A_i}{|o_i|}$ 随Token位置和上下文变化，导致更高的方差，特别是在长序列或 MoE 模型中。

另一个区别在于剪裁处理与这些比率之间的交互方式。对于正优势样本，GRPO 的比率范围大约是 [0, 1.x]；对于负优势样本，它可以是 [0.x, ∞)，范围更广。在长序列中，这种不对称性的噪声可能会累积，导致 GRPO 在 MoE 下的不稳定性。

奖励指标也在检测模型漂移方面滞后，当问题出现时，模型可能已经偏离了一段时间。实验表明，GSPO 以较少的有效Token（由于更积极的裁剪）训练，但实现了更高的训练效率。

总之，GSPO 实现了序列内一致的梯度权重，减少了Token间的方差，特别适合长序列和 MoE 场景下的稳定训练。它的引入标志着从 PPO → GRPO → GSPO 的转变，从依赖价值模型的Token-Level优化转向与任务性质对齐的Sequence-Level优化。

## GMPO：利用几何平均值增强对异常值的鲁棒性

- **问题诊断**: GRPO的目标函数本质上是在优化Token-Level重要性加权奖励的**算术平均值**。算术平均值的一个众所周知的弱点是它对异常值（outliers）非常敏感。在RL训练中，某些token可能会因为策略的剧烈波动而产生极端的重要性采样比率，这些异常值会主导整个梯度更新，导致策略更新不稳定。

- **解决方案**: GMPO (Geometric-Mean Policy Optimization) 提出用**几何平均值**来替代算术平均值。几何平均值在数学上对极端大值不那么敏感。虽然相关论文摘要中未提供GMPO目标函数的精确数学形式，但其核心思想可以进行概念性对比：

  - GRPO (算术平均): $ J_{\text{GRPO}} \propto \frac{1}{T} \sum_{t=1}^{T} r_t(\theta) \hat{A} $

  - GMPO (几何平均, 推断): $ J_{\text{GMPO}} \propto \left( \prod_{t=1}^{T} r_t(\theta) \hat{A} \right)^{1/T} $

由于几何平均值涉及乘积和开方，单个极大的项对最终结果的影响会被其他项“拉平”，从而使得整体目标函数对异常的token奖励更具鲁棒性。

- **影响**: GMPO 使得重要性采样比率在训练过程中保持在一个更稳定的范围内，减少了极端值的出现。这不仅提高了训练稳定性，还允许使用更大的截断范围（$\epsilon$），从而促进了更充分的策略探索。实验表明，GMPO 在多个数学和多模态推理基准上均优于 GRPO。

## GFPO：面向简洁高效推理的组过滤策略优化

在RLVR的实践中，研究者观察到一个普遍现象：模型为了追求更高的正确率，倾向于生成越来越长的回答，即“长度膨胀（length inflation）”。这些冗长的回答中包含了大量与核心推理无关的“填充”文本，严重影响了推理效率。GFPO (Group Filtered Policy Optimization) 的提出，标志着优化目标的又一次演进：从单纯追求结果的正确性，转向同时优化答案的简洁性和效率。

### “多采样，少思考”的哲学

GFPO的核心理念是“多采样，少思考”（Sample More to Think Less）。它通过在训练阶段投入更多的计算资源，来换取推理阶段的更高效率。具体而言，训练时让模型为每个问题生成一个更大的候选回答组，然后根据某个期望的属性（如简洁性）从中筛选出一小部分最优质的回答，并只用这个精英子集来更新模型。通过反复接触和学习这些简洁而正确的范例，模型逐渐内化了这种高效的推理模式，从而在部署和推理时能够“少思考”，即用更短的步骤得出答案。

### 拒绝采样机制与过滤优势

GFPO 通过一个拒绝采样（Rejection Sampling）算法来实现上述思想：

1. **采样**: 对于一个问题 $ q $，从旧策略 $ \pi_{\theta_{\text{old}}} $ 采样一个大的候选组 $ G = {o_1, \ldots, o_G} $。

2. **评分**: 使用一个预定义的 $ \text{metric}(\cdot) $ 函数为组中的每个回答 $ o_i $ 打分。这个指标可以是回答长度（鼓励简洁）、token 效率（$ \text{reward}/\text{length} $，鼓励高效）等。

3. **筛选**: 根据分数对所有回答进行排序，并选出最符合要求（比如最短）的 $ k $ 个回答，形成保留子集 $ S $。

4. **掩码**: 创建一个二进制掩码 $ m $，如果回答 $ o_i $ 被选中（即 $ i \in S $），则 $ m_i = 1 $，否则 $ m_i = 0 $。

这个筛选过程被整合到优势函数的计算中，形成了**过滤优势**（Filtered Advantage）：

$$
A_i^{(m)} = (R(q, o_i) - \mu_S) \cdot m_i
$$

这里的关键在于：

- **基线来自子集**: 优势计算中的均值 $ \mu_S $ 只在被选中的子集 $ S $ 内部计算。这使得模型在已经满足简洁性等要求的回答中，进一步学习区分好坏。

- **掩码的作用**: 对于被拒绝的回答（$ m_i = 0 $），其优势值直接被置为零。这意味着这些不符合期望行为（如过于冗长）的回答完全不参与策略梯度的计算，从而不会对模型更新产生影响。

### 权衡分析

GFPO 的代价是训练时更高的采样开销，因为需要生成和评估比实际用于学习的样本多得多的候选回答。但这一代价换来的是显著的推理时效率提升。在 AIME、GPQA 等多个基准测试中，GFPO 在保持甚至略微提升准确率的同时，能够将 GRPO 等方法造成的长度膨胀减少 40% 到 80% 以上。这种能力使其成为在对推理延迟和计算成本敏感的应用场景中，训练高效推理模型的有力工具。

## LitePPO

LitePPO并非一个全新的算法，而是对现有PPO框架中各种技术进行严格解构和评估后的产物。其研究方法论的核心是，在一个统一的开源框架下，独立地评估每种技术（如不同的归一化策略、截断方法、损失聚合粒度等）的实际影响，从而避免了因实验设置、数据分布或模型初始化的不一致而导致的混淆结论。这项工作揭示了大多数RL技术对实验设置（如模型类型、数据难度、奖励机制）具有明显的偏好和敏感性，从而为从业者提供了清晰的应用指南。

### 极简组合

研究最终发现，一个仅由两种关键技术组成的极简组合，应用于一个无评论家的、使用vanilla PPO损失的框架，就能够稳定地提升性能，甚至超越GRPO和DAPO等更复杂的策略。这两个核心技术是：

1. **鲁棒的优势归一化 (Robust Advantage Normalization)**: 该技术结合了组级别和批次级别的统计量。具体来说，优势的**均值在组（group）级别计算**，而**标准差在整个批次（batch）级别计算**。

   - **原理**: 在组级别计算均值（即减去同一问题下其他回答的平均奖励）可以提供稳定的指导信号，因为它直接反映了相对好坏。而在批次级别计算标准差，由于批次大小远大于组大小，可以提供更强的正则化效果，避免因某个组内奖励分布极端（例如，标准差接近零）而导致的梯度爆炸问题。这种混合方法兼顾了指导的稳定性和正则化的鲁棒性。

2. **token级损失聚合 (Token-Level Loss Aggregation)**: 与DAPO中采用的技术类似，LitePPO主张在计算总损失时，对批次内所有token的损失进行求和，然后除以token总数，而不是在序列之间取平均。

   - **原理**: 这种聚合方式确保了每个token对总损失的贡献是均等的。这对于训练基础模型（base models）尤为重要，因为基础模型需要从长而复杂的正确推理链中充分学习知识，而token级聚合可以克服序列级聚合中存在的“长度偏差”问题（即长序列中单个token的信号被稀释）。

### 简化框架的性能

LitePPO的价值在于其“少即是多”的哲学。实验结果表明，这个仅包含两种精心选择的技术的简化框架，在多种模型和数据集上，其性能持续优于集成了多种复杂技术的GRPO和DAPO。这一发现挑战了当前RL流程过度工程化的趋势，并强调了根据具体环境（如模型是否对齐、数据难度等）自适应地选择技术的重要性，而不是盲目堆砌所有看似有用的“技巧”。

## CISPO

##

## 策略优化算法的比较分析

### 算法对比

为了系统地总结和对比本报告中讨论的九种算法，下表从多个维度对它们进行了梳理，旨在为研究人员和从业者提供一个清晰的参考框架。

| 算法           | 核心思想                   | Critic 模型 | 主要创新点                 | 关键优势             | 目标问题            |
| ------------ | ---------------------- | --------- | --------------------- | ---------------- | --------------- |
| **PPO**      | 将策略更新限制在信任区域内以稳定学习。    | 是         | 截断代理目标函数。             | 稳定、鲁棒。           | 通用RL对齐(RLHF)。   |
| **DPO**      | 通过分类损失直接在偏好对上优化策略。     | 否         | 将奖励重参数化为最优策略的函数。      | 简洁、稳定、无RM/RL循环。  | 通用RL对齐(RLHF)。   |
| **GRPO**     | 使用一组样本的奖励统计量来估计优势。     | 否         | 基于组的优势估计。             | 内存/计算效率高。        | 资源密集型的推理任务。     |
| **DAPO**     | 系统性地应用一套技术来解决大规模RL问题。  | 否         | Clip-Higher、动态采样等的组合。 | 解决特定的训练病理问题。     | 规模化训练的可扩展性和稳定性。 |
| **Dr. GRPO** | 从GRPO目标中移除已识别的长度和难度偏差。 | 否         | 无偏的损失函数形式。            | token效率更高，偏差更小。  | GRPO中的长度/难度偏差。  |
| **GSPO**     | 在序列级别执行重要性采样和截断。       | 否         | 序列级重要性采样。             | 极高的稳定性，尤其对MoE模型。 | token级更新的不稳定性。  |
| **GMPO**     | 使用奖励的几何平均值以对异常值保持鲁棒。   | 否         | 目标函数中使用几何平均。          | 对异常奖励值的鲁棒性。      | 奖励异常值导致的不稳定。    |
| **GFPO**     | 在更新前根据行为指标过滤采样的轨迹。     | 否         | 对轨迹进行拒绝采样。            | 生成简洁、高效的回答。      | 回答长度膨胀问题。       |
| **LitePPO**  | 组合归一化和损失聚合的最佳实践。       | 否         | 对现有技术的有原则配置。          | 以最小的复杂性实现高性能。    | RL流程中的过度工程化。    |

### 识别总体趋势

纵观这些算法的演进，可以识别出几个清晰的宏观趋势：

1. **向无评论家方法的转变及其后果**: 从PPO到GRPO的转变，标志着社区为了解决计算和内存瓶颈，普遍接受了移除评论家模型的设计。然而，这一简化并非一劳而逸，它催生了对优势估计稳定性和无偏性的新一轮深入研究，并直接导致了Dr. GRPO、GSPO和GMPO等一系列“修复性”算法的诞生。
2. **复杂性与简约性的钟摆效应**: 算法的发展路径呈现出一种从复杂（PPO）到简化（GRPO），再到为解决实际问题而增加工程复杂性（DAPO），最终回归到基于深刻理解的“有原则的简约”（LitePPO）的动态过程。这表明该领域正在从“发明新公式”走向“提炼基本原理”。
3. **从结果优化到行为塑造的演进**: 早期的算法主要关注最大化一个代表“好坏”的标量奖励。而以GFPO为代表的新方法，则开始关注优化生成过程本身的属性（如简洁性）。通过数据过滤而非奖励工程来塑造期望行为，这为LLM对齐开辟了新的、更具模块化的技术路径。

### **对算法发展轨迹的总结性思考**

LLM策略优化的未来方向可能将更加关注数据本身和对核心机制的深刻理解。像GFPO那样的数据中心方法（data-centric methods），即通过智能地筛选和塑造训练数据分布来引导模型行为，可能会变得越来越重要。同时，像LitePPO那样通过严谨的实证分析来“蒸馏”出最关键、最有效的组件，将有助于构建更简单、更鲁棒、更易于理解和复现的对齐流程。未来的突破可能不再仅仅来自于设计更复杂的损失函数，而更多地来自于对学习过程本身的洞察，以及如何通过数据和最少的机制来有效地引导这一过程。

## Reference：

- https://huggingface.co/blog/NormalUhr/grpo-to-dapo-and-gspo
- https://rogue-canopy-54a.notion.site/ASPO-Asymmetric-Importance-Sampling-Policy-Optimization-2650e4c8c16a8034a5d3dfec358c9021
