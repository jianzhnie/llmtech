# ReFT: 通过强化微调提升推理能力

代码: https://github.com/lqtrung1998/mwp_ReFT

###### 摘要

提升大型语言模型（LLMs）推理能力的一种常见方法是使用链式思维（CoT）注释进行监督微调（SFT）。然而，这种方法在泛化能力上表现有限，因为训练仅依赖于单一的CoT数据。例如，在数学问题求解中，每个问题通常只有一个注释的推理路径。显然，模型从多个注释的推理路径中学习效果会更好。为此，我们提出了一种简单而有效的方法——**强化微调**（ReFT），以增强LLMs在推理中的泛化能力，并以数学问题求解为例进行验证。ReFT首先通过SFT进行预热，然后利用在线强化学习（RL），特别是PPO算法，进一步微调模型。在此过程中，模型自动采样大量推理路径，并根据真实答案自然推导出奖励。在GSM8K、MathQA和SVAMP数据集上的实验表明，ReFT显著优于SFT，并且通过结合推理时策略（如多数投票和重新排序）可以进一步提升性能。值得注意的是，ReFT仅通过与SFT相同的训练问题进行学习，无需依赖额外或增强的训练数据，展示了其优越的泛化能力。

## 1 引言

当前解决数学问题的前沿方法（Luo et al., 2023; Wang et al., ）主要采用监督微调（SFT）来训练模型，使用链式思维（CoT）注释（Wei et al., 2022）。如图1所示，CoT注释概述了解决数学问题的中间推理步骤。

通常，训练数据中每个问题只有一个CoT注释，即一个正确的推理路径，用于SFT。然而，我们发现这可能导致SFT模型的泛化能力较弱。事实上，同一个问题往往存在多个有效的CoT注释（Cobbe et al., ; Zhang et al., 2023），这表明需要一种更强大的微调方法。为此，我们提出了**强化微调**（ReFT）（图1底部）。

ReFT从预热阶段开始，进行一到两个epoch的监督微调（SFT）（图1，阴影框）。这一阶段使模型具备生成正确数学问题响应的基本能力（Cobbe et al., ）。随后，ReFT通过在线强化学习（RL）算法（Schulman et al., 2017），特别是PPO算法，进一步优化模型。通过这种方式，ReFT能够采样多个正确的推理路径或CoT注释，并从中学习（图2，右）。

由于训练数据包含真实答案，因此在训练PPO时可以直接从这些答案中推导出奖励，无需单独训练奖励模型。相比之下，RLHF（Ziegler et al., 2019）必须依赖从人类标注数据中学习的奖励模型。

在预热阶段，ReFT通过监督学习获得了一定的准确性。在RL阶段，ReFT通过采样各种CoT推理路径进一步增强了其能力。通过这种方式，ReFT获得了比SFT更丰富的监督信号，从而在数学问题求解中显著提高了泛化能力（Cobbe et al., ; Zhang et al., 2023）。值得注意的是，ReFT仅通过与SFT相同的训练问题进行学习，无需依赖额外或增强的训练数据。事实上，ReFT与这些数据工程并不冲突，并且可以无缝结合。

我们的贡献如下：

- 我们提出了一种新的微调方法——强化微调（ReFT），利用强化学习解决数学问题。与传统的监督微调相比，ReFT在相同数据集上表现出更强的泛化能力。
- 我们在两个基础模型CodeLLAMA（Roziere et al., 2023）和Galactica（Taylor et al., 2022）上进行了广泛的实验，使用了三个标准数据集：GSM8K（Cobbe et al., ）、MathQA（Amini et al., 2019）和SVAMP（Patel et al., 2021）。实验涵盖了自然语言和基于程序的CoT，展示了ReFT显著提高的性能和泛化能力。
- 我们还展示了ReFT在推理时受益于多数投票（Wang et al., ）和奖励模型重新排序（Lightman et al., 2023），进一步提升了其性能。

## 2 相关工作

### 数学问题求解

最近的研究主要集中在CoT提示设计和数据工程上。大多数研究试图使CoT全面且细致，以呈现逐步推理的解决方案（Wei et al., 2022; Gao et al., 2023; Zhou et al., 2023）。Gao et al.（2023）进一步提出使用Python程序作为CoT提示，展示了更准确的推理步骤，并显著优于自然语言CoT（Wei et al., 2022）。Zhou et al.（2023）引入了一种提示方法，生成代码以验证GPT-4（OpenAI, 2023）的中间推理步骤，从而在GSM8K（Cobbe et al., ）和MATH（Hendrycks et al., 2021）数据集上实现了最先进的性能。另一项工作则集中在提高CoT的质量（Wang et al., ; Lightman et al., 2023）和增加CoT数据的数量（Wang et al., ; Yue et al., 2023），这些数据通常来自OpenAI的ChatGPT（gpt-3.5-turbo）或GPT-4。

### 强化学习

我们的工作与最近将PPO（Schulman et al., 2017）应用于自然语言处理以对齐人类偏好的研究密切相关（Ziegler et al., 2019）。此后，提出了几种训练算法以有效提高对齐效果，包括直接偏好优化（DPO）（Rafailov et al., 2023）、身份偏好优化（IPO）（Azar et al., 2023）和Kahneman-Tversky优化（KTO）（Ethayarajh et al., 2023）。除了对齐目的外，我们旨在采用强化学习作为一种微调范式，以提高传统监督微调的性能。

特别是对于解决数学问题，Uesato et al.（2022）和Lightman et al.（2023）训练了一个基于结果或基于过程的奖励模型，以执行重新排序（Cobbe et al., 2021），从而在SFT和多数投票（Wang et al., ）上实现了更好的性能。虽然我们的方法旨在提高策略本身的性能，但这些奖励模型重新排序方法可以轻松集成到生成的策略模型中。

## 3 方法

在本工作中，我们专注于**自然语言CoT**（N-CoT）（Wei et al., 2022）（图1）和**基于程序的CoT**（P-CoT）（Gao et al., 2023），使用Python。Gao et al.（2023）提出了基于程序的CoT用于数学问题求解。我们可以简单地执行程序以获得答案。为了确保清晰并避免歧义，我们使用术语N-CoT和P-CoT分别表示自然语言和基于程序的CoT。

### 强化微调

提出的强化微调（ReFT）过程包括两个阶段：预热阶段和强化学习阶段。整体算法如算法1所示。

**预热**：在此阶段，策略在由"（问题，CoT）"元组组成的数据集上进行几个epoch的微调：（$(x,e)$）。它使模型具备基本的解决问题能力，以生成适当的响应。形式上，CoT生成过程可以分解为一系列下一个 Token 预测动作。最后一个动作 Token <eos>表示生成过程终止。CoT $e$ 可以写为：
$$
e=[a_1,a_2,...,a_{L-1},a_L=\text{<eos>}]
$$
其中 $L$ 表示最大长度。在时间步 $t$，动作 $a_{t}$ 从策略 $\pi_{\theta}(|s_{t})$ 中采样，其中 $a_{t}$ 可以是词汇表中的任何 Token ，状态 $s_{t}$ 包括问题中的所有 Token 和迄今为止生成的所有 Token 。每个动作后，结果状态 $s_{t+1}$ 是当前状态 $s_{t}$ 和动作 $a_{t}$ 的串联：

$$
s_{t+1}=\begin{cases}
x,&t=0\\
[s_{t},a_{t}],&1\leq t\leq L
\end{cases}
$$
当生成的动作是<eos> Token 时，结果状态 $s_{L+1}$ 是终止状态，生成过程完成。使用此符号，样本的损失函数可以写为：

$$
\mathcal{L}_{SFT}(\theta)=-\mathbb{E}_{e\sim\mathcal{D}}\left[ \sum_{t=1}^{L}\log\left(\pi_{\theta}(a_{t}|s_{t})\right)\right]
$$
**强化学习**：在此阶段，策略通过在线自学习的形式提高其性能，使用由（问题，答案）元组组成的数据集：（$(x,y)$）。具体来说，策略模型通过重复采样响应（图2），评估响应的答案正确性，并以在线方式更新其参数（算法1中的第7-14行）。我们使用PPO（Schulman et al., 2017）进行训练，采用裁剪目标算法。遵循Ziegler et al.（2019），价值模型 $V_{\phi}$ 通过在策略模型 $\pi_{\theta}$ 的最后一个隐藏状态上附加一个线性价值头来构建，该模型是预热阶段后的模型。对于所有导致非终止状态的动作，奖励为0。在终止状态，我们使用一个奖励函数，直接比较从状态的CoT中提取的答案和真实答案 $y$。如果答案被认为是正确的，则奖励函数返回1，否则返回0。在答案均为数值的数据集上，当答案可以被提取并且是数值类型时，可以应用0.1的**部分奖励**（Zhong et al., 2017; Le et al., 2022）。对于 $1\leq t\leq L$，我们写
$$
r(s_{t},a_{t},s_{t+1})=\begin{cases}
1,&\text{extract}(s_{t+1})=y\\
0.1,&\text{extract}(s_{t+1})\neq\text{null},\neq y\\
0,&\text{extract}(s_{t+1})=\text{null}
\end{cases}
$$

这种部分奖励可以帮助减少从稀疏奖励中学习的影响（Riedmiller et al., 2018; Trott et al., 2019）。此外，遵循Zheng et al.（2023），我们的总奖励是奖励函数得分和学习的RL策略与初始策略之间的Kullback-Leibler（KL）散度（Kullback and Leibler, 1951）之和，乘以系数因子 $\beta$。

$$
r_{total}(s_{t},a_{t},s_{t+1}) =r(s_{t},a_{t},s_{t+1}) -\beta KL\left(\pi_{\theta}(\cdot|s_{t}),\pi^{(0)}_{\theta}(\cdot|s_{t})\right)
$$

广义优势估计（Schulman et al., 2018）用于优势计算：

$$
\hat{A}_{t}=\sum_{l=0}^{L-t}(\gamma\lambda)^{l}\delta_{t+l}
$$

其中时间差分（TD）定义为

$$
\delta_{V}=-V_{\phi}(s_{V})+r_{total}(s_{V},a_{V},s_{V+1})+\gamma V_{\phi}(s_{V+1})
$$

终止状态值 $V_{\phi}(s_{L+1}):=0$，$\lambda\in(0,1]$ 是奖励的折扣因子，$\gamma\in[0,1]$ 是TD的折扣因子。对于回报的估计，我们利用$\lambda$-回报 $\hat{R}_{t}$，可以写为广义优势估计和价值估计之和：

$$
\hat{R}_{t}=\hat{A}_{t}+V_{\phi}(s_{t})
$$

最后，策略和价值目标可以写成以下两个方程

$$
\mathcal{L}_{policy}(\theta)=-\mathbb{E}_{e\sim\pi_{\theta\text{old}}}\left|\min\left( \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta\text{old}}(a_{t}|s_{t})}\hat{A}_ {t},\text{clip}\left(\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta\text{old}}(a_{t}|s_{t})},1-\epsilon,1+\epsilon\right)\hat{A}_{t} \right)\right|
$$

$$
\mathcal{L}_{value}(\phi)=\frac{1}{2}\,\mathbb{E}_{e\sim\pi_{\theta\text{old}}}\left[ \max\left(\left\|V_{\phi}(s_{t})-\hat{R}_{t}\right\|^{2},\left\|\text{clip}\left(\hat{R}_{t}-V_{\phi}(s_{t}), \hat{A}_{t}-\epsilon,\hat{A}_{t}+\epsilon\right)\right\|^{2}\right)\right]
$$

其中 $\pi_{\theta\text{old}},V_{\phi\text{old}}$ 用于采样CoT并计算 $\hat{A}_{t}$, $\hat{R}_{t}$。统一损失函数是上述目标的加权和。

$$
\mathcal{L}_{RL}(\theta,\phi)=\mathcal{L}_{policy}+\alpha\mathcal{L}_{value}
$$

其中 $\alpha$ 是价值目标的系数。

## 4 实验

### 数据集

我们在三个数学问题数据集上进行了实验：GSM8K（Cobbe et al., 2021）、SVAMP（Patel et al., 2021）和MathQA（Amini et al., 2019）。对于GSM8K和SVAMP，答案的格式是数值。在MathQA中，答案格式是多项选择列表（即ABCD）。表1展示了所有数据集的统计信息。我们使用GPT-3.5-turbo进行少样本提示（Wei et al., 2022; Gao et al., 2023）以获得N-CoT和P-CoT注释。N-CoT和P-CoT注释的获取遵循Jie et al.（2023）。我们还在MathQA的数值版本（Jie and Lu, 2023）上进行了额外的实验，其中答案格式也是数值。这些实验用于展示我们对MathQA上潜在奖励黑客现象的假设（Skalse et al., 2022）（§4.4）。

### 基线

我们将ReFT与SFT和自训练（Xie et al., 2020; Amini et al., 2022）基线进行比较。SFT简单地在训练数据上微调语言模型。自训练方法的实验确保了相对公平的比较，因为这些方法共享从模型生成的样本用于训练的机制。

我们实现了离线自训练（Offline-ST）（He et al., 2020）和在线自训练（Online-ST）（Hoi et al., 2021）。Offline-ST方法与专家迭代（Anthony et al., 2017; Uesato et al., 2022; Zelikman et al., 2022）类似。我们首先使用早期检查点的SFT检查点采样CoT，并根据真实答案验证它们。我们仅保留那些具有正确答案的专家样本。我们在原始训练数据和专家样本的组合上执行SFT。

Online-ST方法与ReFT非常相似。与ReFT一样，Online-ST具有相同的预热过程。之后，我们使用即时生成的样本进行持续训练。在每个训练步骤中，模型首先为一个批次采样CoT，并仅保留那些具有正确答案的样本。生成的批次包括采样和真实CoT。然后，我们使用监督微调目标 $\mathcal{L}_{SFT}$ 更新模型参数。与ReFT相比，Online-ST既不使用负面响应（具有错误答案），也没有专门的机制来防止模型显著偏离初始模型，这可能表现为任务特定的过拟合和训练不稳定性。

### 实验设置

我们在两个基础模型上进行了实验：Galactica-6.7B（Taylor et al., 2022）和CodeLLAMA-7B（Roziere et al., 2023）。这两个模型在数学求解方面表现出色，并且在最近的推理任务文献中常被采用（Yue et al., 2023; Luo et al., 2023）。

除了与基线的比较外，我们还在GSM8K上应用了常见的技术，多数投票（Wang et al., ）和奖励模型重新排序（Lightman et al., 2023）。

**超参数**：在所有实验中，训练使用8个A100-80GB GPU，使用DeepSpeed（Rajbhandari et al., 2020; Rasley et al., 2020）Zero阶段2和HuggingFace Accelerate（Gugger et al., 2022）。在ReFT的预热阶段，我们使用AdamW（Loshchilov and Hutter, 2017）优化器，预热比例为10%。批量大小为48，学习率为1e-5。最大长度设置为1024。预热阶段的epoch数在所有设置中为2，除了在MathQA\({}_{\text{MtCQ}}\)和MathQA\({}_{\text{numberic}}\)上，我们分别使用最多5和10个epoch。模型训练300个epoch，学习率为3e-7。遵循Ziegler et al.（2019），PPO中的$\lambda$、$\gamma$、$\alpha$、$\epsilon$和$U$分别设置为1、0.95、5、0.2和2。KL系数$\beta$在P-CoT实验中设置为0.01，在N-CoT实验中设置为0.05。关于ReFT的进一步超参数设置可以在附录B中找到。

对于SFT基线，我们训练模型40个epoch，并选择性能最佳的检查点。这个epoch数被选择为足够大，以确保SFT收敛。对于Offline-ST基线，我们使用ReFT预热阶段的检查点采样CoT。使用生成温度为1.0和最大长度为1024，我们为每个问题采样100个CoT，并仅保留那些具有正确答案的样本。遵循Singh et al.（2023），我们然后将CoT子采样为每个问题10个随机唯一的CoT，以平衡问题的难度。微调的epoch数设置为20，这足够大以确保训练收敛。如§4.2所述，Online-ST基线试图模仿与ReFT相同的设置。我们具有相同的预热过程，并且超参数设置大致与ReFT相同。

**奖励模型重新排序**：遵循Cobbe et al.（2021）; Uesato et al.（2022），我们训练一个奖励模型（RM）来确定CoT的正确性。为了构建RM训练数据，我们使用预热阶段的模型并执行采样，以获得训练集中每个问题的100个CoT。CoT被去重，并且可以通过将提取的答案与真实答案进行比较来获得二进制标签。

作为一种常见做法，奖励模型是一个语言模型，从最佳SFT检查点初始化（Cobbe et al., ; Ouyang et al., 2022）。与基于结果的奖励模型（ORM）（Uesato et al., 2022）类似，奖励模型被训练为预测一个二进制标签，指示"正确"或"错误"的解决方案。一旦输入通过奖励模型，分类将在最后一个 Token 的隐藏状态上进行线性分类。最后，选择具有最高"正确"分数的候选解决方案作为最终答案。我们使用批量大小为24、最大长度为700和线性学习率计划训练RM模型3个epoch，预热期为10%，最大学习率为1e-6。

**评估**：我们在所有数据集上报告N-CoT和P-CoT的值准确性。对于多数投票和重新排序（表4），我们采样100个CoT进行评估。在投票中，选择具有多数计数的有效答案作为计算准确性的最终答案。在重新排序中，我们选择具有最高分数的CoT并提取答案。

### 结果

**ReFT优于SFT**：表2比较了基线和提出的ReFT在GSM8K、SVAMP和MathQA数据集上的性能。我们可以观察到，ReFT在除 MathQA  N-CoT外的所有数据集上始终优于SFT。具体来说，我们在GSM8K N-CoT和P-CoT上分别比SFT提高了近10分和12分。平均而言，我们在所有数据集上使用CodeLLAMA在N-CoT和P-CoT上分别提高了6.7分和7.4分。值得注意的是，ReFT中没有使用额外的注释或奖励模型。如此强劲的结果展示了ReFT的强大泛化能力（见分析§5），并为进一步探索训练数据与强化学习的潜力提供了巨大的空间（Lu et al., 2023）。

离线自训练包括从初始策略中采样数据进行微调。我们可以看到，这个简单的基线可以比SFT提高性能（He et al., 2020; Gulcehre et al., 2023），但改进远远落后于ReFT。这种比较表明，"探索"在ReFT中对于获得良好性能至关重要。尽管在线自训练在Galactica上取得了一些改进，但平均而言仍远远落后于ReFT。这一结果表明，错误实例对于指导模型进行更好的探索也非常重要。与自训练的比较还表明，提出的具有在线采样和强化学习的方法优于标准的数据增强方法。

**MathQA的奖励黑客**：我们对MathQA 上的负面结果的调查表明，ReFT在多选题训练期间遭受奖励黑客（Skalse et al., 2022）。图3展示了采样解决方案如何产生"不准确的奖励"，这使得RL训练受到影响。正如我们所看到的，采样的CoT获得了一个错误的答案"172"，这不是"18"和"22"乘积的一半。然而，最终的推理步骤仍然预测选项"C"作为最终答案，因为模型总是从{A, B, C, D, E}中预测一个选项，而不管中间CoT的正确性。因此，这种误导性的CoT将获得正奖励"1"，并误导模型将其视为正确的CoT。潜在的奖励黑客现象严重干扰了模型训练（Everitt et al., 2021）。这也是我们选择具有较长预热步骤的检查点用于MathQA N-CoT以减少奖励黑客效应的原因。

为了进一步展示多选题的负面影响，我们在MathQA变体上进行了实验，由Jie and Lu（2023）提出，MathQA 上的奖励黑客效应。然而，开发一个可靠的基于过程的奖励模型是昂贵的，并且需要广泛的手动注释推理步骤。认识到这些挑战，我们认为控制奖励黑客及其分析是未来工作中需要解决的重要问题。

**多数投票和重新排序使ReFT受益**：遵循Wang et al.（2023b）; Uesato et al.（2022）; Lightman et al.（2023），我们还执行了多数投票和奖励模型重新排序，以展示ReFT可以从这些常见技术中受益。具体来说，我们从SFT和ReFT策略中执行采样。我们为每个问题采样100个CoT解决方案，并使用§4.3中描述的奖励模型执行重新排序。表4中的结果表明，ReFT通过奖励模型重新排序在GSM8K上始终表现最佳。ReFT + Voting在所有设置中平均比SFT + Voting高出8.6分。ReFT与重新排序相比，SFT与重新排序相比，平均高出3分以上。

与现有的开源方法（Luo et al., 2023; Wang et al., ; Yue et al., 2023）（表4底部）相比，我们最好的P-CoT变体在GSM8K上实现了最佳性能，准确率为81.2。此外，这些方法主要包括从ChatGPT生成的额外数据，并在微调期间进行蒸馏。相比之下，我们通过挖掘现有训练数据的潜力并推动策略性能的极限来改进策略本身。我们在表4中报告的最佳结果，即CodeLLAMA + ReFT + Reranking with P-CoT设置，甚至超过了GPT-3.5-turbo。然而，我们使用了一个只有7B大小的模型获得了这一结果。

**小模型实验**：直观上，探索可能会导致小语言模型的不完美演示。我们在Galactica-125M、Codeparrot-small和Codegen-350M上使用P-CoT数据进行了实验。表5展示了SFT和ReFT之间的性能比较。令人惊讶的是，ReFT在三个数据集上仍然优于SFT。这些改进展示了ReFT在探索合理程序时的鲁棒性。

**消融研究**：我们使用CodeLLAMA在GSM8K P-CoT上进行了消融研究（表6）。如果没有部分奖励，ReFT获得的准确率较低，为74.4，但仍然比SFT好得多。如§3.1所述，这种部分奖励可以帮助减少训练期间稀疏奖励的影响（Trott et al., 2019）。此外，如果我们将KL系数$\beta$设置为0，策略分布将很容易崩溃，产生意外结果（即0准确率）。对策略探索的空间施加约束是至关重要的（Ouyang et al., 2022）。初始预热步骤本质上施加了这些约束，并允许策略在由$\beta$控制的范围内进一步探索。我们还实验了一个单独的价值模型（Andrychowicz et al., 2021; Cobbe et al., ），其中躯干参数初始化为与策略模型相同。我们发现，这种设置在早期RL训练中允许策略更快地收敛，但最终达到与原始设置相当的性能。与共享价值模型的原始设置相比，由于一次额外的前向传递，计算开销是两倍，并且由于存储单独的价值网络，内存成本也是两倍。最后，在附录C中，我们提供了一个案例研究，展示了生成的P-CoT如何随着SFT和ReFT的演变而变化。

## 5 分析

**泛化**：图4展示了使用CodeLLAMA作为基础模型在GSM8K P-CoT上训练的ReFT的平均奖励、评估准确性和KL散度。SFT在接近40个epoch时收敛并过拟合。然而，我们可以看到ReFT策略在40个epoch时的平均奖励约为80%到90%，并且值准确性也在增加。此外，我们可以看到KL散度（图4（c））在开始时非常大，然后保持在0到10之间的合理值。稳定的KL散度表明我们的策略在包含适当程序的空间内进行探索。潜在的强化学习机制大大提高了ReFT的泛化能力（Brown et al., 2020）。

**定性评估**：我们进行了人工评估，以定性评估SFT模型、预热检查点和ReFT模型的输出。评估使用50个问题，并采样GSM8K测试集中所有三个模型都能正确解决的解决方案。我们要求四位不同的注释者根据以下标准对推理路径进行评分，每个标准评分从0到1。

- **逻辑**：评估导致答案的逻辑是否正确。
- **命名**：评估变量是否传达了适当且合理的语义。
- **紧凑性**：评估推理路径是否包含冗余信息。

完美得分为3表示在这三个维度上表现良好。为了确保评估的公正性和忠实性，我们严格遵循以下设置：（1）每个推理路径的来源（来自SFT、预热或ReFT）被匿名化，以防止注释者偏见。（2）四位不同的注释者负责不同的样本部分。

如表7所示，尽管总体得分非常接近，但ReFT的表现略优于SFT，并且优于预热变体。请注意，SFT本质上是训练来学习真实答案的，因此很可能获得高分。这种比较分析强调了ReFT在生成准确且语义一致的推理路径方面的鲁棒性。

**ReFT何时超越SFT？**：为了进一步研究ReFT和SFT之间的关系，我们使用不同数量的SFT预热步骤进行ReFT训练。图5展示了不同ReFT变体与SFT的值准确性。具体来说，如果预热步骤为3，则意味着策略从第3个epoch的SFT检查点初始化。我们可以看到，所有ReFT策略的性能在预热后立即下降，直到训练epoch达到8左右。因为共享价值模型中的线性层是随机初始化的，可能需要几个epoch来调整分布。从第30个epoch开始，SFT收敛，所有ReFT变体仍在改进。我们还可以看到，所有变体都显著优于SFT，并且没有明显的特定ReFT变体的优势。

## 6 结论

我们引入了强化微调（ReFT）作为一种新的方法，用于微调模型以解决数学问题。与SFT相比，ReFT通过探索多个CoT注释来优化不可微的目标，而不是依赖于单个注释。

通过在三个数据集上使用两个基础模型进行广泛的实验，我们证明了ReFT在性能和泛化能力方面优于SFT。此外，我们展示了使用ReFT训练的模型与多数投票（Wang et al., ）和奖励模型重新排序（Cobbe et al., ; Uesato et al., 2022）等技术的兼容性。

此外，ReFT在数学问题求解方面表现出优于几个公开可用的开源模型的性能。这证明了ReFT方法的有效性和实用价值。

## 7 未来工作

我们首次尝试将强化学习，特别是PPO算法（Schulman et al., 2017），应用于LLMs的微调以解决数学问题。我们的未来工作包括利用离线强化学习技术（Levine et al., 2020; Gulcehre et al., 2023），开发一种**无预热**方法以提高训练效率和性能，从而缩小与重新排序方法的差距。此外，Lightman et al.（2023）建议，训练有素的基于过程的奖励模型（PRM）可以显著提高性能。因此，值得探索在强化学习训练中实现基于过程的奖励。最后，由于ReFT是一种多功能方法，我们打算将其应用于更一般的推理任务，其中推理可以通过CoT形式化。
