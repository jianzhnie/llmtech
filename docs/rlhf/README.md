## LLM RLHF Framework

- [RLHF 中的 PPO 代码拆解](rlhf/infra/RLHF中的PPO代码拆解.md)
- [RLHF 训练框架 NeMo-Aligner](rlhf/infra/NeMo-Aligner.md)
- [RLHF 训练框架 DeepSpeedChat](rlhf/infra/DeepSpeedChat.md)
- [RLHF 训练框架 OpenR](rlhf/infra/OpenR.md)
- [RLHF 训练框架 OpenRLHF](rlhf/infra/OpenRLHF.md)
- [RLHF 训练框架 OpenRLHF 源码解读](rlhf/infra/OpenRLHF源码解读.md)
- [RLHF 训练框架 VeRL](rlhf/infra/Verl.md)
- [RLHF 训练框架 VeRL 源码解读](rlhf/infra/Verl源码解读.md)
- [RLHF 训练框架 VeRL 参数配置指南](rlhf/infra/Verl参数配置.md)
- [OpenRLHF & &Verl参数转换指南](rlhf/infra/OpenRLHF&Verl参数转换指南.md)
- [从 Ray 角度分析 OpenRLHF 和 Verl 的工程设计](rlhf/infra/Ray_OpenRLHF_Verl.md)


## LLM RLHF Intro

- [理解 RLHF](rlhf/intro/rlhf_advance.md)
- [Chip Huyen 对 RLHF 的分析](rlhf/intro/rlhf_chiphuyen.md)
- [RLHF 相关知识整理](rlhf/intro/rlhf_overview.md)
- [RLHF 中的 Policy Gradient Algorithms](rlhf/intro/rlhf_policy_gradient.md)


## LLM RLHF Algorithm and Paper

- [直接偏好优化 (DPO)](rlhf/paper/rlhf_dpo.md)
- [直接偏好优化 (DPO) 推导](rlhf/paper/rlhf_dpo_notes.md)
- [Kahneman-Tversky-Optimization (KTO)](rlhf/paper/rlhf_kto.md)
- [RLOO](rlhf/paper/RLOO.md)
- [DeepSeek-R1：通过强化学习激励 LLMs 的推理能力](rlhf/paper/DeepSeek-R1.md)
- [Kimi k1.5：使用 LLM 扩展强化学习](rlhf/paper/KimiK1.5.md)
- [DAPO: 一个开源的大规模 LLM 强化学习系统](rlhf/paper/DAPO.md)
- [深入理解 R1-Zero 类训练：一个批判性视角](rlhf/paper/DR.GRPO.md)
- [DeepScaleR：通过扩展强化学习超越 o1](rlhf/paper/deepscaler.md)
- [REINFORCE++：一种简单高效的大型语言模型对齐方法](rlhf/paper/REINFORCE++.md)
- [ChatGPT O1 Reasoning](rlhf/paper/chatgpt_O1.md)
- [KL 散度的近似计算](rlhf/paper/KL-Approximate.md)
- [过程奖励模型（Process Reward Model）](rlhf/paper/PRM.md)
- [数学推理中过程奖励模型的开发经验](rlhf/paper/PRM_Reasoning.md)
- [ReFT: 通过强化微调提升推理能力](rlhf/paper/ReFT.md)
- [拒绝采样（Reject Sampling）在 RLHF 中的应用](rlhf/paper/RejectSampling.md)
- [ReST-MCTS：通过过程奖励引导的树搜索实现 LLM 自训练](rlhf/paper/ReST-MCTS.md)
- [rStar-Math：小型语言模型通过自我进化的深度思考掌握数学推理](rlhf/paper/rStar-Math.md)
