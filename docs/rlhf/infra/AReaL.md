# AREAL：一种面向语言推理的大规模异步强化学习系统

# 摘要

强化学习（Reinforcement Learning, RL）已成为训练大型语言模型（Large Language Models, LLMs）的重要范式，尤其在提升模型推理能力方面表现出色。高效的LLM强化学习依赖于大规模并行化以生成充足的Rollout数据，从而充分探索策略空间，这对训练系统的效率提出了极高要求。当前主流的大型强化学习系统多采用同步设计，通过在固定批次中交替执行生成与训练过程，确保每个训练批次的数据均由相同或最新版本的模型生成。尽管这种设计有助于稳定训练过程，但其系统效率存在显著瓶颈：由于LLM在不同提示下的输出长度差异显著，同步系统在生成阶段必须等待整个批次中最长序列完成解码后才能进入训练阶段，导致大量GPU资源处于空闲状态，利用率低下。

为此，本文提出AReaL（Asynchronous Reinforcement Learning），一个完全异步的强化学习训练系统，实现了生成与训练流程的彻底解耦。在AReaL中，Rollout  Worker持续并行地生成新样本，无需等待训练阶段完成；而训练Worker则在积累足够数据后立即进行模型更新。为提升系统效率，AReaL引入了一系列优化机制，显著提高了GPU利用率。为保障训练稳定性，系统通过平衡Rollout与训练Worker的负载来控制数据陈旧度，并采用一种增强型陈旧度容忍的PPO变体，以更有效地处理过时样本。在数学推理与代码生成任务上的大量实验表明，相比相同GPU资源下的同步系统，AReaL最高可实现2.77倍的训练加速，同时在最终性能上达到相当甚至更优的结果。AReaL的开源代码可通过以下链接获取：[链接]。

# 1 引言

强化学习（Reinforcement Learning, RL）作为扩展大型语言模型（LLMs）推理能力的关键技术，能够引导模型在输出最终答案前生成中间推理步骤（即“思维链”），从而实现测试时扩展（test-time scaling）。这类具备推理能力的模型被称为大型推理模型（Large Reasoning Models, LRMs），在数学推理、代码生成、逻辑推理及代理任务等复杂问题求解中展现出卓越性能。

高效的RL训练通常需要大规模并行化，以生成海量Rollout样本，充分探索策略空间，这对训练系统的吞吐能力提出了严峻挑战。例如，主流RL算法如PPO和GRPO通常要求每轮训练使用数千个样本构成全局批次。此外，LRMs在单个提示下可能生成长达数万个token的推理轨迹，进一步加剧了对高效训练系统的依赖。

然而，构建高效的大型RL系统面临多重挑战。首先，系统需在模型生成（推理）与参数更新（训练）之间频繁切换，若缺乏优化，将引入显著的系统开销。其次，由于不同提示对应的输出长度差异巨大，生成与训练阶段的负载高度动态，易导致硬件资源利用率波动。更重要的是，经典RL算法（如PPO）依赖于“同策略”（on-policy）数据，即训练数据需由当前或最新策略生成，以保证训练稳定性，这进一步增加了系统设计的复杂性。

现有系统普遍采用同步设计，严格交替执行生成与训练阶段，以确保使用最新策略数据进行训练。然而，这种设计在生成阶段必须等待最长序列完成，导致短序列的GPU资源被浪费。尽管已有研究尝试引入异步或并行机制，但大多仍受限于批量同步生成的框架，即一个训练批次中的所有样本来自同一模型版本，未能从根本上解决生成阶段的效率瓶颈。

为突破上述限制，我们提出AReaL（Asynchronous Reinforcement Learning），一个完全异步的RL训练系统，通过解耦生成与训练流程，在不牺牲甚至提升模型性能的前提下大幅提升系统效率。AReaL采用流式生成机制，Rollout  Worker持续生成样本，无需等待训练完成，从而最大化GPU利用率。训练Worker则在收集到足够数据后立即执行更新。模型更新完成后，Rollout  Worker通过权重同步机制加载最新参数。在此异步架构下，每个训练批次可能包含由多个不同版本策略生成的样本。为此，AReaL设计了改进的PPO目标函数，能够有效利用陈旧数据，并引入数据过滤机制控制样本陈旧度。此外，系统还集成了可中断生成、动态批处理可变长度序列及并行奖励计算等优化技术，进一步提升整体效率。

我们在数学推理与代码生成任务上对AReaL进行了评估，使用了最高达32B参数的模型。实验结果表明，相比最先进的同步系统，AReaL实现了最高2.57倍的训练吞吐量提升，并在多达512块GPU上展现出良好的线性扩展性。更重要的是，这种效率提升并未以牺牲性能为代价，反而在部分任务上带来了准确率的提升，验证了AReaL在保持高性能的同时显著提升训练效率的能力。

# 2 相关工作

## 2.1 大型语言模型的强化学习

强化学习已成为提升LLMs推理能力的核心方法。现有研究主要聚焦于具有明确奖励信号的任务，如数学推理、代码生成、科学问题求解和工具调用等。训练过程中，模型通过逐步扩展推理路径学习复杂推理能力。部分开源项目通过知识蒸馏技术，利用小规模模型提升推理性能。与本文工作不同，基于人类反馈的强化学习（RLHF）和零样本推理方法侧重于从预训练模型中激发推理能力，而非针对特定任务进行有监督的强化学习训练。

## 2.2 异步强化学习

异步强化学习架构及其算法创新在游戏AI领域已取得显著成功。近年来，也有研究探索将异步机制应用于LLM训练，但通常局限于短文本场景（如RLHF）或仅实现生成与训练的有限重叠。本文工作扩展了这些研究，提供了一个更灵活的陈旧度-效率权衡机制。与并发研究相比，AReaL采用算法-系统协同设计思路，不仅构建了高效的系统架构，还实现了适应高陈旧度场景的实用算法。我们的可中断生成技术在概念上与同步系统中的部分Rollout相似，但结合异步架构后，算法能容忍更高程度的数据陈旧度，并与可中断机制兼容。

## 2.3 大型语言模型的训练与推理

本文聚焦于密集型Transformer架构。RL训练包含两个核心阶段：生成（推理）与参数更新（训练）。生成阶段依赖高效的自回归解码、键值缓存管理及优化的解码内核；训练阶段则需协调数据并行、张量并行和流水线并行策略。传统同步系统在同一硬件上顺序执行两阶段，但二者最优并行策略不同，导致资源利用不匹配。近期研究提出上下文切换或权重重切分技术以缓解此问题。AReaL通过解耦两阶段，彻底消除了关键训练路径上的重切分开销。

# 3 背景知识

## 3.1 强化学习训练基础

### 强化学习形式化与PPO算法

我们采用马尔可夫决策过程（Markov Decision Process, MDP）框架，定义为五元组 $\langle\mathcal{S},\mathcal{A},r,P,\gamma,H\rangle$，其中 $\mathcal{S}$ 为状态空间，$\mathcal{A}$ 为动作空间，$P$ 为状态转移函数，$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ 为奖励函数，$\gamma$ 为折扣因子，$H$ 为决策步长。推理模型（LRM）实现参数化策略 $\pi_\theta: \mathcal{S} \to \mathcal{A}$，动作 $a_t \in \mathcal{L}$ 对应词汇表中的token。状态 $s_t$ 由初始问题 $q$ 和历史生成序列 $(a_1, \ldots, a_{t-1})$ 构成，转移函数为确定性拼接操作 $s_{t+1} = \operatorname{concat}(s_t, a_t)$。给定问题分布 $\mathcal{D}$，优化目标为：

$$
J(\theta) = \mathbb{E}_{q \sim \mathcal{D}, a_t \sim \pi_\theta(\cdot | q, a_{<t})} \left[ \sum_{t=1}^{H} \gamma^{t-1} r(s_t, a_t) \right].
$$

实践中，我们采用基于规则的稀疏奖励函数，仅在最终动作处提供非零反馈（如答案正确性），并设 $\gamma = 1$。我们使用近端策略优化（Proximal Policy Optimization, PPO）算法优化该目标：

$$
J_{\operatorname{PPO}}(\theta) = \mathbb{E}_{q \sim \mathcal{D}, a_t \sim \pi_{\operatorname{old}}(\cdot | q, a_{<t})} \left[ \sum_{t=1}^{H} \operatorname{min} \left( u_t(\theta) \hat{A}(s_t, a_t), \operatorname{clip}(u_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}(s_t, a_t) \right) \right],
$$

其中 $u_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\operatorname{old}}(a_t | s_t)}$ 为重要性采样比率，$\hat{A}(s_t, a_t)$ 为优势函数估计值。标准实践中，全局批次被划分为小批次以进行参数更新。

### 分布式推理模型训练系统

本文工作聚焦于在监督微调（Supervised Fine-Tuning, SFT）后，通过RL进一步增强LRMs的推理能力。经过SFT的LRMs可生成长推理序列（如32K tokens），且RL训练通常需要较大的全局批次（如128个提示，每个生成16个响应）以保证稳定性。在同步RL系统中，生成（Rollout）与训练阶段在相同GPU上交替执行：生成阶段使用最新模型为每个查询生成多条推理轨迹，训练阶段则基于这些轨迹更新模型参数。

## 3.2 异步强化学习系统的动机

![Refer to caption](https://arxiv.org/html/2505.24298v2/x1.png)

同步RL系统存在两大瓶颈，促使我们设计完全解耦的异步系统。

**推理设备利用率低**：如图1（左）所示，生成阶段需等待批次中最长序列完成，导致短序列GPU长时间空闲，造成计算资源浪费。

**可扩展性差**：同步系统将生成任务分布到所有设备，降低了单GPU的解码批次大小，使解码过程进入内存-IO受限区域，额外设备难以提升性能。

# 4 系统架构

![Refer to caption](https://arxiv.org/html/2505.24298v2/x2.png)

## 4.1 系统概述

图2展示了AReaL的整体架构与数据流。系统包含四个核心组件：

### 4.1.1 可中断的Rollout  Worker

负责处理两类请求：

1.  **生成请求**：根据输入提示生成响应序列。
2.  **更新权重请求**：中断所有正在进行的生成任务，加载最新模型参数。中断时，  Worker会丢弃基于旧权重计算的KV缓存，并使用新权重重新计算。随后，继续解码未完成的序列，直至下一次中断或终止。值得注意的是，这种中断机制可能导致单条轨迹由多个不同策略版本生成的片段拼接而成，这带来了新的算法挑战，将在第5节详述。

### 4.1.2 奖励服务

负责评估模型生成响应的正确性。例如，在代码生成任务中，该服务提取生成的代码并执行单元测试以验证其功能正确性。

### 4.1.3 训练Worker

持续从回放缓冲区采样数据，直至累积达到预设的训练批次大小。随后执行PPO更新，并将更新后的模型参数存入分布式存储。为保证数据新鲜度，回放缓冲区中的样本仅被使用一次。

### 4.1.4 Rollout 控制器

![Refer to caption](https://arxiv.org/html/2505.24298v2/extracted/6512014/figures/interruptible-gen-timeline.png)

作为Rollout  Worker、奖励服务与训练Worker之间的协调中枢。在训练过程中，控制器从数据集中读取样本，向Rollout  Worker发起生成请求。收到响应后，将其发送至奖励服务获取奖励信号。最终，将完整的轨迹及其奖励存入回放缓冲区供训练使用。当训练Worker完成参数更新后，控制器触发 Rollout  Worker的`update_weights`操作。图3展示了生成与训练的管理流程。该异步流水线设计确保了生成与训练资源的持续高效利用。

## 4.2 算法挑战

尽管异步设计显著提升了设备利用率，但也引入了若干关键算法挑战。

### 4.2.1 数据陈旧度

由于AReaL的异步特性，每个训练批次可能包含由多个历史策略版本生成的样本。数据陈旧度会扩大训练数据分布与当前策略之间的差距，可能影响学习效率，尤其在长轨迹场景下更为显著。

### 4.2.2 策略版本不一致

如前所述，单条轨迹可能由不同策略版本生成的片段构成。这种不一致性违背了标准PPO算法的基本假设——即整个轨迹由单一旧策略 $\pi_{\text{old}}$ 生成。

下一节将详细介绍AReaL如何在保留异步系统效率优势的同时，克服上述算法挑战。

# 5 解决AReaL中的算法挑战

## 5.1 陈旧度感知训练

为防止因训练数据过度陈旧而导致性能下降，我们引入超参数 $\eta$，表示每个训练批次允许的最大陈旧度。当 $\eta = 0$ 时，系统退化为同步模式，所有训练样本均由当前策略生成。我们通过动态调节生成请求的吞吐量来控制系统中的陈旧度。设当前策略版本为 $i$，已生成轨迹总数为 $N_r$，每步训练批次大小为 $B$，在提交新生成请求时强制满足以下约束：

$$
\lfloor (N_r - 1) / B \rfloor \leq i + \eta.
$$

同时，优先从回放缓冲区选取较早生成的轨迹构成训练批次。系统实现中，Rollout控制器跟踪样本总数 $N_r$ 和当前策略版本 $i$，拒绝可能违反陈旧度约束的生成请求。

需注意，该速率控制机制是一种简单有效的工程实践。但当 $\eta$ 过小时，极长轨迹的生成可能成为瓶颈，限制整体吞吐量。因此，实践中建议采用较大的 $\eta$ 值以最大化系统吞吐。这也促使我们采用一种能有效利用陈旧数据的增强型算法。

## 5.2 解耦的PPO目标

我们采用解耦的PPO目标函数，明确区分行为策略（behavior policy）与近端策略（proximal policy）。行为策略 $\pi_{\text{behav}}$ 指实际用于采样轨迹的策略，而近端策略 $\pi_{\text{prox}}$ 则作为正则化更新的近期参考策略。通过重要性采样，我们推导出适用于异步训练的解耦PPO目标：

$$
\begin{aligned}
J(\theta) &= \mathbb{E}_{q \sim \mathcal{D}, a_t \sim \pi_{\text{behav}}}\left[\sum_{t=1}^{H} \operatorname{min}\left(\frac{\pi_\theta(a_t | s_t)}{\pi_{\text{behav}}(a_t | s_t)} \hat{A}_t, \operatorname{clip}\left(\frac{\pi_\theta(a_t | s_t)}{\pi_{\text{behav}}(a_t | s_t)}, 1 - \epsilon, 1 + \epsilon\right) \hat{A}_t\right)\right] \\
&= \mathbb{E}_{q \sim \mathcal{D}, a_t \sim \pi_{\text{behav}}}\left[\sum_{t=1}^{H} \frac{\pi_{\text{prox}}(a_t | s_t)}{\pi_{\text{behav}}(a_t | s_t)} \operatorname{min}\left(u_t^{\text{prox}}(\theta) \hat{A}_t, \operatorname{clip}\left(u_t^{\text{prox}}(\theta), 1 - \epsilon, 1 + \epsilon\right) \hat{A}_t\right)\right],
\end{aligned}
$$

其中 $u_t^{\text{prox}}(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{prox}}(a_t | s_t)}$ 为相对于近端策略的重要性比率。为简洁起见，省略了状态-动作依赖项。

解耦PPO目标（公式5）与标准PPO（公式2）的核心区别在于引入了独立的近端策略 $\pi_{\text{prox}}$。若在异步训练中直接使用行为策略作为近端策略，将导致更新过程向质量较低的旧策略偏移，阻碍模型进步。通过选用近期高质量策略作为 $\pi_{\text{prox}}$，模型更新被约束在高质量策略的信任区域内，从而稳定训练过程。

解耦PPO目标的一个关键优势是：它不再要求训练批次中的所有数据必须由单一策略生成。这一特性对于处理可中断生成与策略更新带来的策略不一致性至关重要。我们证明，跨轨迹的不一致策略版本等价于从单一行为策略 $\pi_{\text{behav}}$ 中采样（详见附录C）。

**命题1**：对于任意由策略序列 $\left(\pi_\theta, \ldots, \pi_{\theta+k}\right)$ 生成的轨迹 $(q, a_1, \ldots, a_H)$，其中 $\pi_{\theta+i}$ 生成子序列 $(a_{t_i}, \ldots, a_{t_{i+1}})$，且 $1 = t_0 < \cdots < t_{k+1} = H$，存在一个行为策略 $\pi_{\text{behav}}$，使得中断生成过程等价于完全从 $\pi_{\text{behav}}$ 中采样。

**实践说明**：尽管Hilton等人[10]建议对 $\pi_{\text{prox}}$ 维护参数的指数移动平均，但对LRMs而言计算开销过高。因此，我们简单地将每次更新前的参数作为 $\pi_{\text{prox}}$。公式5在每个训练步骤处理全局批次时，通过重新计算各token的概率实现。

# 6 实现

AReaL基于Python和PyTorch构建，继承并扩展了ReaLHF框架。系统集成SGLang v0.4.6作为生成服务后端，Megatron-Core v0.11.0作为训练后端，整体由SLURM进行资源调度。为最大化生成与训练吞吐量，我们针对系统流水线中的关键瓶颈实施了多项系统级优化。

AReaL将GPU计算与CPU操作解耦，包括基于规则的奖励计算（如数学题的字符串匹配、代码任务的单元测试执行）及基于TCP的数据传输。通过在独立线程中执行这些操作并流水线化处理流程，我们将奖励计算和数据传输与后续生成请求并行化。在Rollout  Worker中，我们使用`asyncio`协程并发处理多个请求，避免相互阻塞。

为高效处理可变长度序列的训练，我们采用无填充的序列打包策略，并结合一种动态批处理算法。该算法在固定内存约束下优化微批次内的token分布（见算法1），在最大化GPU内存利用率的同时，最小化所需的前向-后向传播次数。

```python
# 算法 1：动态批量分配
def dynamic_batching(sequence_lengths, max_capacity, min_batches):
    """
    对序列进行动态批量分配，确保每个微批次的总长度不超过 max_capacity。

    :param sequence_lengths: 序列长度列表
    :param max_capacity: 每个微批次的最大容量
    :param min_batches: 最小微批次数量
    :return: 分配后的微批次列表
    """
    # 按长度降序排序
    sequence_lengths.sort(reverse=True)
    batches = []
    for length in sequence_lengths:
        if len(batches) < min_batches or not any(batch['remaining'] >= length for batch in batches):
            # 若微批次数不足或无合适空间，则创建新微批次
            batches.append({'lengths': [length], 'remaining': max_capacity - length})
        else:
            # 选择剩余空间最小且能容纳当前序列的微批次
            suitable_batches = [batch for batch in batches if batch['remaining'] >= length]
            chosen_batch = min(suitable_batches, key=lambda batch: batch['remaining'])
            chosen_batch['lengths'].append(length)
            chosen_batch['remaining'] -= length
    return batches
```

# 7 实验

本节的评估包含三个部分：(1) 在不同模型规模下与当前最先进开源框架的全面性能对比；(2) 基于不同计算资源的强扩展性分析；(3) 验证关键设计选择有效性的消融研究。

## 7.1 实验设置

我们在具有挑战性的数学推理与代码生成任务上对AReaL进行了评估。实验采用DeepSeek-R1 [6] 中的蒸馏版Qwen2模型系列作为基础模型（即 R1-Distilled-Qwen），涵盖从1.5B到32B参数的多个规模。对于每个任务-模型组合，我们固定PPO更新步数进行训练，并在训练结束后评估最终检查点的性能。数学任务遵循Qwen评估协议 [56,13]，代码生成任务则采用LiveCodeBench (8/1/24–2/1/25) [14] 的官方评测标准。除非另有说明，最大陈旧度 $\eta$ 在代码任务中设为4，数学任务中设为8，其余训练超参数详见附录A。

实验在由64个节点组成的H800 GPU集群上进行，每个节点配备8张GPU。节点内部通过NVLink互联，节点间通过带宽达3.2Tbps的RoCE网络连接。为保证训练快速收敛，完整实验的基线配置至少使用16个节点。根据模型规模成比例增加计算资源，在训练最大规模的32B模型时使用了48个节点。该扩展策略确保了在高效利用资源的同时，并行执行不同规模的实验。

在AReaL系统中，我们保持推理与训练设备的固定比例，将四分之三的GPU资源分配给rollout生成阶段。该比例基于前期实验确定：75%-25%的资源分配相比50%-50%能实现更高的训练吞吐量。尽管我们采用此比例作为启发式配置，但需指出最优分配可能因具体任务和训练阶段而异，未来可通过动态调整进一步优化（见第8节讨论）。

## 7.2 端到端性能对比

我们构建了两个同步强化学习系统的先进基线：针对1.5B模型的数学推理任务采用DeepScaleR [25]，针对14B模型的代码生成任务采用DeepCoder [24]，两者均使用verl [45]框架进行训练。对于7B和32B等更大规模的模型，由于缺乏可比基线，我们通过从头实现AReaL的同步版本作为对照。训练完成后，在数学任务上采用AIME24基准测试，在代码任务上采用LiveCodeBench [14]进行评估。其他基准的详细结果见附录B。

主要实验结果如表1所示。为确保公平比较，我们使用最新的verl代码重新测量了各系统的吞吐量。结果表明，AReaL在保持甚至略微提升模型性能的同时，显著缩短了训练时间，最高可实现2.77倍的端到端加速。

| 模型              | AIME24↑        | # 节点   | PPO 步骤    | 训练小时数↓ |
| :---------------- | :------------- | :------- | :---------- | :---------- |
| 1.5B 基础模型     | 29.3           |          |             |             |
| w/ verl           | 43.1*          | 16       | 250         | 33.6        |
| w/ 同步 AReaL     | 42.0           | 16       | 250         | 41.0        |
| w/ AReaL (我们的) | 42.2           | 16       | 250         | 14.8        |
| 7B 基础模型       | 54.3           |          |             |             |
| w/ verl           | 63.0           | 24       | 250         | 52.1        |
| w/ 同步 AReaL     | 63.0           | 24       | 250         | 57.7        |
| w/ AReaL (我们的) | 63.1           | 24       | 250         | 25.4        |
| 模型              | LiveCodeBench↑ | PPO 步骤 | 训练小时数↓ | # 节点      |
| 14B 基础模型      | 53.4           |          |             |             |
| w/ verl           | 57.9*          | 32       | 80          | 44.4        |
| w/ 同步 AReaL     | 56.7           | 32       | 80          | 48.8        |
| w/ AReaL (我们的) | 58.1           | 32       | 80          | 21.9        |
| 32B 基础模型      | 57.4           |          |             |             |
| w/ verl           | 61.2           | 48       | 60          | 51.1        |
| w/ 同步 AReaL     | 61.2           | 48       | 60          | 51.1        |
| w/ AReaL (我们的) | 61.0           | 48       | 60          | 31.1        |

> *注：表中verl结果来自原论文报告值，其余为本实验测量值。*

## 7.3 可扩展性分析

![Refer to caption](https://arxiv.org/html/2505.24298v2/x3.png)

我们对比了AReaL与当前最先进的同步系统verl [45] 在不同模型规模和上下文长度下的可扩展性。实验选取7B模型在32k上下文长度下verl不发生OOM的最小GPU数量为基准，并按模型规模同比例扩展。我们测量训练的有效吞吐量（即PPO更新阶段每秒消耗的生成token数），在完成适当热身后进行统计。图4展示了上下文长度为16k和32k时的结果。此处上下文长度指提示与生成序列的总和，最大提示长度限制为1k。

结果显示，在所有配置下，AReaL均表现出接近线性的扩展趋势，而同步系统则难以有效扩展。AReaL在多数场景下吞吐量显著超越基线，最高可达2.5倍加速。值得注意的是，在较短上下文场景下，AReaL的优势可能减弱，因为生成吞吐量无法完全匹配训练速度，导致部分生成序列未被及时消耗。此外，AReaL在处理长生成序列时表现出更强的鲁棒性，这得益于其异步机制与可中断生成能力。长序列的生成时间可完全隐藏在训练的关键路径之外，因此延长生成长度对有效训练吞吐量影响甚微。

## 7.4 算法消融研究

<img src="https://arxiv.org/html/2505.24298v2/x4.png" alt="Refer to caption" style="zoom:33%;" />

<img src="https://arxiv.org/html/2505.24298v2/x5.png" alt="Refer to caption" style="zoom:33%;" />

<img src="https://arxiv.org/html/2505.24298v2/x6.png" alt="Refer to caption" style="zoom:33%;" />

为验证第5节提出的算法创新，我们在1.5B LRM的数学任务上进行了消融实验。沿用DeepScaleR的基本设置，逐步调整最大陈旧度 $\eta$ 进行测试。具体对比了不同 $\eta$ 值下，是否采用解耦PPO目标函数的性能差异。图5a和5b展示了250个训练步后的学习曲线，表2列出了在多个数学推理基准上的最终性能。我们遵循PPO标准实践，在每个训练步骤内执行多个小批次更新。需强调，$\eta$ 参数用于约束训练批次的数据陈旧度上限。

图5a表明，标准PPO在存在数据陈旧时性能显著下降（$\eta > 0$），甚至无法达到同步系统（$\eta = 0$）的水平。轻微的陈旧度即导致性能劣化，这源于不恰当的裁剪中心及可中断生成带来的策略不一致性。且随着陈旧度增加，性能持续下降，这与以往研究结论一致 [2,30]。相比之下，图5b显示，采用解耦PPO目标函数后，系统在处理陈旧数据时的训练稳定性显著提升，与游戏领域观察到的现象相符 [10]。此外，即使使用解耦目标，无限制的陈旧度（$\eta \to \infty$）仍会导致性能劣于零陈旧度配置。但当陈旧度被适当限制（如 $\eta \leq 8$）时，对最终性能影响极小，同时通过异步流水线大幅加速训练（见图5c与表2）。这些结果验证了我们结合可控陈旧度与解耦PPO目标以实现高效异步RL训练的有效性。

## 7.5 系统消融研究

### 可中断生成

我们对可中断生成机制进行了消融研究，结果如图6所示。若不支持中断，控制器必须等待最长响应完成。实验表明，可中断生成使1.5B和7B模型在4个节点上的生成吞吐量分别提升了12%和17%，验证了该架构设计的有效性。

### 动态微批次分配

我们对比了动态微批次分配与标准策略的PPO训练吞吐量。标准策略可能将多个长序列分配至同一微批次，常需配置较多微批次以防内存溢出。实验中，标准设置采用32个微批次，动态策略则设定每微批次最多32,768个token。如图7所示，动态微批次分配在不同模型规模下平均提升吞吐量约30%，显著优化了内存利用率与计算效率。
