# 共享专家特性

## 方案介绍：

随着混合专家模型MoE的演进，产生了路由专家和共享专家的概念。针对路由专家，输入数据会经过路由模块选择概率较高的专家进行计算；而对于共享专家，输入数据无需经过路由模块计算，所有数据都会经过共享专家计算。路由专家和共享专家的计算结果相加后作为MoE模块最终的计算结果。

通过将共享专家和路由专家结合，MOE模型能够在不同的输入情况下既关注到输入数据的共性也能关注到输入数据的差异性，从而提高模型的泛化能力。

共享专家如下图c所示（参考论文：https://arxiv.org/pdf/2401.06066 ）：
![img](../../sources/images/shared-experts.png)

## 使用场景

MoE场景下使用：`--moe-model-type megatron_moe`

## 使用方法

共享专家相关命令和参数说明：

| 命令参数                     | 参数说明                   |
|--------------------------|------------------------|
| `--n-shared-experts [int]` | 共享专家数量                 |

## 注意事项

1. 开启共享专家需要在mcore模式下，即没有设置`--use-legacy-models`

2. 共享专家中间隐藏层大小的配置命令与路由专家相同：`--ffn-hidden-size [int]`

3. 当前版本megatron原生支持，默认调用原生共享专家
