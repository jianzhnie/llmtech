# 脑海 2B 模型 GRPO 强化微调

## 模型和数据

- Model：Naohai-2B
- Train Dataset：GSM8k  Train
- Evaluation Dataset:   GSM8k  Test

## 训练超参数


| 超参数                 | 值        | 说明                   |
| ---------------------- | --------- | ---------------------- |
| advantage_estimator    | grpo_norm | GRPO优势估计方法       |
| n_samples_per_prompt   | 8         | 每个prompt生成的样本数 |
| prompt_max_len         | 1024      | 最大prompt长度         |
| generate_max_len       | 1024      | 最大生成长度           |
| micro_train_batch_size | 4         | 每个GPU的训练微批大小  |
| train_batch_size       | 256       | 全局训练批大小         |
| data_type              | bf16      | 模型参数类型           |
| actor_learning_rate    | 5e-7      | Actor模型学习率        |
| use_kl_loss            | False     | 是否使用KL损失         |
| init_kl_coef           | 0         | KL损失系数初始值       |

## 训练曲线

### Reward 曲线

<img src="/Users/jianzhengnie/Library/Application%20Support/typora-user-images/image-20250425155917816.png" alt="image-20250425155917816" style="zoom:100%;" />

训练过程中的奖励曲线显示了模型性能的稳定提升：

- 初始阶段奖励值较低
- 中期阶段有明显上升趋势
- 后期趋于稳定，说明模型收敛

### Response Length 曲线

![image-20250425160202955](../../../Library/Application%20Support/typora-user-images/image-20250425160202955.png)

![image-20250425160202955](ascend/images/Cosine-Scaled-Reward-Function.png)

<img src='ascend/images/Cosine-Scaled-Reward-Function.png' title='title' data-no-caption>

响应长度曲线反映了模型输出的变化：

- 随着训练进行，响应长度先上升后下降。
- 最终阶段长度趋于稳定，表明模型学会了更详细的推理过程。


## 评估结果


| 模型                 | GSM8K Test 准确率 |
| -------------------- | ----------------- |
| NaoHai2B Base        | 59.0%             |
| NaoHai2B Base + GRPO | 74.5%             |

性能提升分析：
基础模型在GSM8K测试集上达到59.0%的准确率，
经过GRPO强化微调后，准确率提升至74.5%
总体提升15.5个百分点，显示GRPO训练的有效性。
