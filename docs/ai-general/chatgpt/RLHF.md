# Implementing RLHF: Learning to Summarize with trlX

## ä»‹ç»

éšç€æœ€è¿‘ ChatGPT çš„ æ¨å‡ºï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF) å·²æˆä¸ºè¯­è¨€å»ºæ¨¡ç•Œçš„çƒ­é—¨è¯é¢˜â€”â€”åŒ…æ‹¬å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œã€‚

æˆ‘ä»¬å¯ä»¥è¿½æº¯ RLHF åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ï¼Œ  OpenAI 2019 å¹´å‘å¸ƒçš„[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)ã€‚å¿«è¿›ä¸€å¹´äº†ï¼ŒOpenAI å‘å¸ƒäº†ç¬¬ä¸€ç¯‡å…³äºä»äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ åº”ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆçš„é‡è¦è®ºæ–‡ä¹‹ä¸€ã€‚åœ¨é‚£ç¯‡è®ºæ–‡â€”â€”[å­¦ä¹ ä»äººç±»åé¦ˆä¸­æ€»ç»“](https://arxiv.org/abs/2009.01325)â€”â€”OpenAI è¡¨æ˜ï¼Œåœ¨æ ¹æ®äººç±»åå¥½è¿›è¡Œè¯„ä¼°æ—¶ï¼Œç®€å•åœ°å¯¹æ€»ç»“æ•°æ®è¿›è¡Œå¾®è°ƒä¼šå¯¼è‡´è¡¨ç°ä¸ä½³ã€‚ä½œè€…å»ºè®®ç›´æ¥é€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•é’ˆå¯¹äººç±»åå¥½è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç¼“è§£è¿™äº›æ€§èƒ½é—®é¢˜ã€‚

## ä½¿ç”¨ trlX]

ï»¿[CarperAI çš„trlX](https://github.com/CarperAI/trlx)æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œå…¶çµæ„Ÿæ¥è‡ª Transformer å¼ºåŒ–å­¦ä¹ åº“ï¼ˆå¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š[lvwerra/trlï¼‰](https://github.com/lvwerra/trl)ã€‚trlX ä»å¤´å¼€å§‹è®¾è®¡ï¼Œä»¥å¤§è§„æ¨¡å…³æ³¨ RLHFï¼Œè¿™æ˜¯é‡ç°æœ€è¿‘ RLHF æ–‡çŒ®ä¸­è§‚å¯Ÿåˆ°çš„è®¸å¤šç»“æœçš„å¿…è¦å› ç´  [ [Steinnon ç­‰äººï¼Œ2020 å¹´](https://arxiv.org/abs/2009.01325)ï¼›[Askell et al., 2021](https://arxiv.org/abs/2112.00861) , [Ouyang et al., 2022](https://arxiv.org/abs/2203.02155) ].

ç‰¹åˆ«æ˜¯ï¼ŒtrlX[ä»äººç±»åå¥½è¿‡ç¨‹ä¸­æŠ½è±¡å‡ºå¾®è°ƒè¯­è¨€æ¨¡å‹](https://arxiv.org/abs/1909.08593)çš„ RL éƒ¨åˆ†ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿä¸“æ³¨äºç®¡ç†å¼ºåŒ–å­¦ä¹ çš„æŒ‘å‰”åŠ¨æ€çš„é«˜çº§é€‰æ‹©ï¼Œè€Œä¸æ˜¯è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ‰€éœ€çš„æ ·æ¿ä»£ç ã€‚å®ƒçš„è®¾è®¡è¶³å¤Ÿçµæ´»ä»¥æ”¯æŒå¹¿æ³›çš„ç®—æ³•ï¼Œç›®å‰æ”¯æŒ[è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–](https://openai.com/blog/openai-baselines-ppo/)(PPO) å’Œ[éšå¼è¯­è¨€ Q å­¦ä¹ ](https://arxiv.org/abs/2206.11871)(ILQL) ã€‚

> åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå¥–åŠ±å‡½æ•°æ˜¯æ‰‹å·¥åˆ¶ä½œçš„ã€‚å¦‚ä¸Šæ‰€è¿°ï¼ŒtrlX æŠ½è±¡äº† RLHF çš„ RL ç»„ä»¶ï¼Œç”¨äºå¾®è°ƒ LLMã€‚æ‚¨å¯ä»¥å¸¦ä¸Šè®­ç»ƒæœ‰ç´ çš„å¥–åŠ±æ¨¡å‹æˆ–æ‰‹å·¥åˆ¶ä½œã€‚

```python
sentiment_fn = pipeline(
	"sentiment-analysis",
	"sentiment-analysis",
	"gpt2",
	top_k=2,
	truncation=True,
	batch_size=256,
	device=device,
)


def get_positive_score(scores):
	"Extract value associated with a positive sentiment from pipeline's output"
	return dict(map(lambda x: tuple(x.values()), scores))["POSITIVE"]


def reward_fn(samples: List[str]) -> List[float]:
	sentiments = list(map(get_positive_score, sentiment_fn(samples)))
	return sentiments


trainer = trlx.train("gpt2", reward_fn=reward_fn)

```

æˆ–è€…ï¼Œè¦ä½¿ç”¨ç¦»çº¿ ILQLï¼Œè¯·æä¾›æ‚¨çš„å¥–åŠ±æ ‡è®°æ•°æ®é›†ï¼š

```
trainer = trlx.train(
	"EleutherAI/gpt-j-6B",
	dataset=[("dolphins", "geese"), (1.0, 100.0)],
)
```

æˆªè‡³å‘ç¨¿æ—¶ï¼ŒtrlX å¯ä»¥å€ŸåŠ© HuggingFace [Accelerate](https://huggingface.co/docs/accelerate/index)å¯¹æ¨¡å‹è¿›è¡Œ 30B è§„æ¨¡çš„å¾®è°ƒã€‚æˆ‘ä»¬æ­£åœ¨ç»§ç»­åŠªåŠ›ï¼Œä»¥å°½å¿«æ”¯æŒå…·æœ‰æ›¿ä»£åç«¯çš„æ›´å¤§æ¨¡å‹ã€‚æ¬¢è¿æŠ•ç¨¿ï¼

[æ‚¨å¯ä»¥ä»ä»–ä»¬çš„ç¤ºä¾‹](https://github.com/CarperAI/trlx/tree/main/examples)ä¸­äº†è§£æœ‰å…³ä½¿ç”¨ trlX çš„æ›´å¤šä¿¡æ¯ã€‚ğŸ’¡

## ä»æ‘˜è¦ä¸­å­¦ä¹ 

åœ¨æœ¬èŠ‚ä½¿ç”¨çš„ trlX ä¸­ï¼Œæˆ‘ä»¬å°†ä¸ºæ‘˜è¦ä»»åŠ¡å®æ–½ RLHFã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼š

- æˆ‘ä»¬å°†é¦–å…ˆåœ¨æˆ‘ä»¬çš„æ‘˜è¦æ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„Transformeræ¨¡å‹ï¼ˆä¸‹ä¸€èŠ‚å°†è¯¦ç»†ä»‹ç»æ•°æ®é›†ï¼‰ã€‚è¿™æ˜¯æˆ‘ä»¬çš„ç›‘ç£å¾®è°ƒæ¨¡å‹ (SFT)ã€‚
- ç„¶åæˆ‘ä»¬å°†è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ã€‚è¯¥æ¨¡å‹ä» SFT æ¨¡å‹åˆå§‹åŒ–å¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ã€‚è¿™ä¸ªæ ‡é‡å€¼æ˜¯è¡¨ç¤ºæ‘˜è¦åå¥½çš„å¥–åŠ±ã€‚
- æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ RM é€šè¿‡ PPO å¾®è°ƒ SFT æ¨¡å‹ã€‚æ­¤æ­¥éª¤ä½¿æˆ‘ä»¬çš„ SFT æ¨¡å‹ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚

## æ•°æ®é›†

å¯¹äºæˆ‘ä»¬ä»Šå¤©çš„å®éªŒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æœ€åˆåœ¨å­¦ä¹ ä¸­ä½¿ç”¨çš„ TL;DR æ‘˜è¦æ•°æ®é›†[æ¥ä»äººç±»åé¦ˆä¸­è¿›è¡Œæ€»ç»“](https://arxiv.org/abs/2009.01325)ã€‚

åŸºäºä¸Šè¿°è®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ç§ç±»å‹çš„æ•°æ®é›†ï¼š

- ä¸€ä¸ªç”¨äºå¾®è°ƒé¢„è®­ç»ƒçš„ç›‘ç£æ¨¡å‹ï¼Œç„¶åç”¨ PPO å’Œå¥–åŠ±æ¨¡å‹å†æ¬¡å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥åŠ
- ä¸€ä¸ªç”¨äºè®­ç»ƒæˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹ã€‚

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç”¨äºå¾®è°ƒçš„æ•°æ®é›†æ˜¯è¿‡æ»¤è¿‡çš„ TL;DR æ•°æ®é›†ã€‚ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ•°æ®é›†æ˜¯æ¯”è¾ƒæˆ–åå¥½æ•°æ®é›†ã€‚

> ä½œè€…è¿‡æ»¤äº†åŸå§‹çš„ TL;DR æ•°æ®é›†ï¼Œä»¥åŒ…å«ä¸€ä¸ªå®‰å…¨çš„ subreddits åˆ—è¡¨ï¼Œè¿™äº›åˆ—è¡¨å¾ˆå®¹æ˜“è¢«æ™®é€šå¤§ä¼—ç†è§£ã€‚æ­¤å¤–ï¼Œä»–ä»¬åªæœ‰æ ·æœ¬ï¼Œå…¶ä¸­äººå·¥ç¼–å†™çš„æ‘˜è¦åœ¨ 24 åˆ° 48 ä¸ªæ ‡è®°ä¹‹é—´ã€‚

### å¦‚ä½•ä¸‹è½½æ•°æ®é›†

æˆ‘ä»¬å°†é¦–å…ˆä¸‹è½½[AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)ï¼Œè¿™æ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œå®ç”¨ç¨‹åºï¼Œå¯ç”¨äºå°† blob æˆ–æ–‡ä»¶å¤åˆ¶åˆ°å­˜å‚¨å¸æˆ·æˆ–ä»ä¸­å¤åˆ¶ã€‚ç›¸å…³ä»£ç ï¼š

[å¯ä»¥åœ¨å®˜æ–¹å­˜å‚¨åº“](https://github.com/openai/summarize-from-feedback)ä¸­æ‰¾åˆ°æŒ‡å‘ TL;DR æ•°æ®é›†å’Œæ¯”è¾ƒæ•°æ®é›†çš„ä¸åŒæ‹†åˆ†çš„é“¾æ¥ã€‚

ä»¥ä¸‹æ˜¯ä¸‹è½½ TL;DR æ•°æ®é›†çš„è®­ç»ƒæ‹†åˆ†çš„æ–¹æ³•ï¼š

```
!azcopy copy "https://openaipublic.blob.core.windows.net/summarize-from-feedback/datasets/tldr_3_filtered/train.jsonl"
```

### TL;DR æ•°æ®é›†

TL;DR æ‘˜è¦æ•°æ®é›†åŒ…å« 129,722 ä¸ª Reddit å¸–å­ï¼Œå…¶ä¸­çº¦ 5% ç”¨äºæ‹†åˆ†éªŒè¯å’Œæµ‹è¯•ã€‚è®­ç»ƒé›†ä¸­æ€»å…±æœ‰ 116,722 ä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†ä¸­æœ‰ 6,447 ä¸ªæ ·æœ¬ï¼Œæµ‹è¯•é›†ä¸­æœ‰ 6,553 ä¸ªæ ·æœ¬ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ­¤æ•°æ®é›†æ¥å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚

è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```json
{
  'id': 't3_1hxu8s',
  'subreddit': 'relationships',
  'title': 'I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting',
  'post': "Not sure if this belongs here but it's worth a try. \n\nBackstory:\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \n\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \n\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \n\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.",
  'summary': "I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?"
}
```

è¯¥æ•°æ®é›†ç»è¿‡ç²¾å¿ƒæ•´ç†ä»¥ç”¨äºå¾®è°ƒï¼Œå¹¶ä½œä¸º Hugging Face æ•°æ®é›†æ‰˜ç®¡ã€‚[ä½ å¯ä»¥åœ¨è¿™é‡Œ](https://huggingface.co/datasets/CarperAI/openai_summarize_tldr)æ‰¾åˆ°ã€‚æ•°æ®é›†æ ¼å¼ï¼ˆéªŒè¯é›†ï¼‰å¦‚ä¸‹æ‰€ç¤ºã€‚æç¤ºæ˜¯ä¸ Subreddit åç§°å’Œæ ‡é¢˜ç›¸è¿çš„ Reddit å¸–å­ã€‚labelæ˜¯çœŸäººå†™çš„æ€»ç»“ï¼š

### æ¯”è¾ƒæ•°æ®é›†

æ¯”è¾ƒæ•°æ®é›†ç”±è®­ç»ƒæ•°æ®é›†ä¸­çš„ 92,858 ä¸ªæ ·æœ¬å’ŒéªŒè¯é›†ä¸­çš„ 83,797 ä¸ªæ ·æœ¬ç»„æˆã€‚ä»åŠŸèƒ½ä¸Šè®²ï¼Œè¿™äº›åªæ˜¯ Reddit å¸–å­å’Œæ¯ä¸ªå¸–å­çš„ä¸¤ä¸ªæ‘˜è¦ã€‚å®ƒè¿˜å…·æœ‰ä¸€ä¸ªé€‰æ‹©å€¼ï¼ŒæŒ‡ç¤ºäººå·¥æ ‡è®°è€…æ›´å–œæ¬¢ä¸¤ä¸ªæ‘˜è¦ä¸­çš„å“ªä¸€ä¸ªï¼ˆåœ¨ä¸‹é¢æ ‡è®°ä¸ºâ€œé€‰æ‹©â€ï¼š0ï¼‰ã€‚

è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```json
{
    "info": {
        "id": "t3_3pb8rl",
        "post": "Hi reddit.\n\nI recently started dating a woman that I really like, after talking to her a lot for around a month. We go to university together and have a bunch of classes together, eat together, study together, etc. I asked her out, we went to the movies, had a lot of fun, kissed, yada yada.  \n\nMy biggest problem is that I've never been in a relationship. I'm relatively inexperienced romantically(kissed like 2 girls and had sex once before), and this is the first time I met someone that I thought 'Damn I really want to spend a lot of time with you'.\n\nI really like her, and so I don't want to rush things, but then I don't know what I can or can't do. How often can we hold hands? Do we just kiss whenever one of us feels like it? How do I know she wants to be kissed at a particular moment? How do I know HOW she wants to be kissed? How do I know if I'm doing something 'wrong'?\n\nThese are a bunch of things that, if it were some random girl, I wouldn't even care about(or at least not care as much). I really just don't want to fuck this up. Are there any basic relationship rules or something other than 'do what your heart wants'? I appreciate anything you guys can tell me (criticisms or advice)\n\nThanks in advance.\n\nP.S I'm guessing that some people will wonder about the age gap. We've talked about it. It's weird but we both like each other and don't care for it. The fact that she's older than me only stresses me out more because she's had more experience with relationships than me, and I really, REALLY don't want to fuck up.\n\nP.S.S This is my first post here, so I'm not sure how things work. If you guys need any additional information that I didn't mention to help out just ask :P",
        "title": "I [19/M] just started dating a girl [25/F] I really like, but I've never been in an actual relationship. I don't really know what to do.",
        "subreddit": "relationships"
    },
    "split": "train",
    "summaries": [
        {
            "text": " I've never been in a relationship, but I like this woman. How do I know if I'm doing things wrong? How do I know if I like her?",
            "policy": "sup2",
            "note": "ok"
        },
        {
            "text": " I'm dating a girl, I don't know how things work. I want to make it work, but I don't know what the hell I can/should do.",
            "policy": "sup2",
            "note": "OP doesn't have relationship experience"
        }
    ],
    "choice": 0,
    "worker": "HNzkrs9geGu1YMMfZ5Qvdt0ZaCthfB",
    "batch": "batch5",
    "extra": {}
}
```

### è¿™äº›æ‘˜è¦æ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Ÿ

å¯¹äºæ¯ä¸ª Reddit å¸–å­ï¼ˆåœ¨æ•°æ®é›†ä¸­ï¼‰ï¼Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹ç”Ÿæˆ N ä¸ªæ‘˜è¦ã€‚é¢„è®­ç»ƒæ¨¡å‹ç”¨ä½œé›¶æ ·æœ¬æ‘˜è¦ç”Ÿæˆå™¨ï¼Œå¹¶ä¸”è¿˜ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆåœ¨ Reddit TL;DR ä¸Šï¼‰æ¨¡å‹ï¼ˆ12Bã€6B å’Œ 1.3Bï¼‰ç”Ÿæˆæ‘˜è¦ã€‚äººå·¥ç¼–å†™çš„ TL;DRï¼ˆå‚è€ƒï¼‰ä¹Ÿè¢«è§†ä¸ºæ ·æœ¬ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œè¿™äº›æ¨¡å‹è¢«è§†ä¸ºç­–ç•¥ã€‚

æ¯ä¸ªå¸–å­çš„è¿™ N ä¸ªæ‘˜è¦è¢«æˆå¯¹æ‰¹å¤„ç†å¹¶å‘é€ç»™é›‡ä½£çš„æ ‡ç­¾å‘˜ã€‚è´´æ ‡ç­¾è€…é€‰æ‹©/åçˆ±ä¸€ä¸ªæ‘˜è¦è€Œä¸æ˜¯å¦ä¸€ä¸ªã€‚

![å›¾ç‰‡](https://api.wandb.ai/files/carperai/images/projects/37218153/8323a45b.png)

è¯¥æ•°æ®é›†ä¸“ä¸ºè®­ç»ƒå¥–åŠ±æ¨¡å‹è€Œè®¾è®¡ï¼Œå¹¶ä½œä¸º HuggingFace æ•°æ®é›†æ‰˜ç®¡ã€‚ä½ å¯ä»¥[åœ¨è¿™é‡Œ](https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons)æ‰¾åˆ°å®ƒã€‚æ•°æ®é›†æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºã€‚æç¤ºæ˜¯ä¸ Subreddit åç§°å’Œæ ‡é¢˜è¿æ¥çš„ Reddit å¸–å­ï¼Œè€Œâ€œé€‰æ‹©â€åˆ—æ˜¾ç¤ºè¯„è®ºè€…é¦–é€‰çš„æ ‡ç­¾ã€‚å½“ç„¶ï¼Œé‰´äºäººç±»åé¦ˆä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é¢†åŸŸï¼Œä½¿ç”¨æ•°æ®é›†çš„æ–¹å¼æ²¡æœ‰å¯¹é”™ä¹‹åˆ†ã€‚

## æºä»£ç 

æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„è„šæœ¬å¯ä»¥åœ¨[trlXå­˜å‚¨åº“çš„](https://github.com/CarperAI/trlx)[trlx/examples/summarize_rlhf/](https://github.com/CarperAI/trlx/tree/main/examples/summarize_rlhf) * ç›®å½•ä¸­æ‰¾åˆ°ã€‚

è¦å¼€å§‹ï¼Œè¯·é¦–å…ˆæŒ‰ç…§ä¸‹é¢æ¦‚è¿°çš„ trlX å®‰è£…æŒ‡å—è¿›è¡Œæ“ä½œï¼š

```
git clone https://github.com/CarperAI/trlx.git
cd trlx
pip install torch --extra-index-url https://download.pytorch.org/whl/cu116 # for cuda
pip install -e .
```

## ç›‘ç£å¾®è°ƒ (SFT)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åœ¨ TL;DR æ•°æ®é›†ä¸Šå¾®è°ƒ GPT-J æ¨¡å‹ä»¥è¿›è¡Œæ–‡æœ¬æ‘˜è¦ã€‚

è¿™æ˜¯ç›¸å¯¹ç®€å•çš„ã€‚åŠ è½½æ•°æ®é›†ï¼Œå¯¹å…¶è¿›è¡Œ tokenize ï¼Œç„¶åè®­ç»ƒæ¨¡å‹ã€‚æ•´ä¸ª pipeline æ˜¯ä½¿ç”¨ HuggingFace æ„å»ºçš„ã€‚å¾®è°ƒï¼š

```
!deepspeed examples/summarize_rlhf/sft/train_gptj_summarize.py
```

æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ ROUGE åˆ†æ•°è¿›è¡Œè¯„ä¼°ã€‚éªŒè¯é›†ä¸Šçš„å¹³å‡ ROUGE åˆ†æ•°é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†ç”¨äºåˆå§‹åŒ–å¥–åŠ±æ¨¡å‹ï¼Œç¨åå°†ä½¿ç”¨ PPO è¿›è¡Œå¾®è°ƒã€‚

ä¸‹é¢æ˜¾ç¤ºçš„å›¾è¡¨æ€»ç»“äº† TL;DR æ•°æ®é›†æµ‹è¯•é›†ä¸Šçš„ä¸åŒ ROUGE åˆ†æ•°ã€‚

## è®­ç»ƒå¥–åŠ±æ¨¡å‹

æˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹æ˜¯ç”¨æ”¶é›†åˆ°çš„äººç±»è´¨é‡åˆ¤æ–­æ•°æ®é›†è®­ç»ƒçš„ã€‚è¯¥æ¨¡å‹å°†ç»™å®šçš„å¸–å­å’Œå€™é€‰æ‘˜è¦æ˜ å°„åˆ°å¥–åŠ±*r* ã€‚

æˆ‘ä»¬å°†ä» SFT æ¨¡å‹åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹ï¼Œå¹¶é™„åŠ ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„çº¿æ€§å¤´ï¼Œåœ¨é¡¶éƒ¨è¾“å‡ºæ ‡é‡å€¼ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°ç ”ç©¶æ•°æ®å¦‚ä½•è¾“å…¥åˆ°æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œå¥–åŠ±æ¨¡å‹çš„å…¶ä»–é—®é¢˜ã€‚

### åŸå§‹è¾“å…¥

[æ•°æ®åŠ è½½å™¨å°†ä½¿ç”¨æ­¤å¤„](https://huggingface.co/datasets/pvduy/openai_summarize_comparisions)æ‰˜ç®¡çš„æ¯”è¾ƒæ•°æ®é›†ã€‚ä¸è¿‡åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ create_comparison_dataset å‡½æ•°ï¼ˆå¦‚ä¸‹æ‰€ç¤ºï¼‰åˆ›å»ºä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå­—å…¸éƒ½æœ‰ä¸¤ä¸ªé”® - é€‰æ‹©å’Œæ‹’ç»ã€‚æ¯ä¸ªé”®çš„å€¼æ˜¯ä¸æ‘˜è¦è¿æ¥çš„æç¤ºï¼ˆæˆ– Reddit å¸–å­ï¼‰ã€‚

```python
def create_comparison_dataset(
     path="CarperAI/openai_summarize_comparisons", split="train"
 ):
     dataset = load_dataset(path, split=split)
     if split == "test":
         dataset = dataset.select(range(10000))
ï»¿

     pairs = []
     for sample in tqdm(dataset):
         pair = {}
         prompt = sample["prompt"]
         chosen_summary = sample["chosen"]
         rejected_summary = sample["rejected"]
         if chosen_summary == rejected_summary:
             continue
         if  len(chosen_summary.split()) < 5 or len(rejected_summary.split()) < 5:
             continue
         pair["chosen"] = prompt + "\n" + chosen_summary
         pair["rejected"] = prompt + "\n" + rejected_summary
         pairs.append(pair)
     return pairs
```

### æˆå¯¹æ•°æ®åŠ è½½

ä¸‹é¢æ˜¾ç¤ºçš„ PairwiseDataset ç±»æ ‡è®°äº†é€‰æ‹©å’Œæ‹’ç»çš„â€œæ‘˜è¦â€ã€‚æ•°æ®é›†ç±»è¿”å›é€‰æ‹©å’Œæ‹’ç»æ‘˜è¦çš„ input_ids å’Œ attention_masksï¼š

```python
class PairwiseDataset(Dataset):
     def __init__(self, pairs, tokenizer, max_length):
         self.chosen_input_ids = []
         self.chosen_attn_masks = []
         self.rejected_input_ids = []
         self.rejected_attn_masks = []
         for pair in tqdm(pairs):
             chosen, rejected = pair["chosen"], pair["rejected"]
             chosen_encodings_dict = tokenizer(
                 "<|startoftext|>" + chosen + "<|endoftext|>",
                 truncation=True,
                 max_length=max_length,
                 padding="max_length",
                 return_tensors="pt",
             )
             rejected_encodings_dict = tokenizer(
                 "<|startoftext|>" + rejected + "<|endoftext|>",
                 truncation=True,
                 max_length=max_length,
                 padding="max_length",
                 return_tensors="pt",
             )
             self.chosen_input_ids.append(chosen_encodings_dict["input_ids"])
             self.chosen_attn_masks.append(chosen_encodings_dict["attention_mask"])
             self.rejected_input_ids.append(rejected_encodings_dict["input_ids"])
             self.rejected_attn_masks.append(rejected_encodings_dict["attention_mask"])
ï»¿

     def __len__(self):
         return len(self.chosen_input_ids)
ï»¿

     def __getitem__(self, idx):
         return (
             self.chosen_input_ids[idx],
             self.chosen_attn_masks[idx],
             self.rejected_input_ids[idx],
             self.rejected_attn_masks[idx],
         )
```

### Data Collator

DataCollatorReward ç±»ä¸ºæˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹åˆ›å»ºæ•°æ®æ‰¹æ¬¡ï¼ˆdictï¼‰ã€‚æ•´ç†å™¨è¿”å›ï¼š

- input_ids: collator åœ¨ dim=0 ä¸Šè¿æ¥é€‰æ‹©å’Œæ‹’ç»çš„æ‘˜è¦çš„ input_idsã€‚
- attention_mask: collator åœ¨ dim=0 ä¸Šè¿æ¥é€‰æ‹©å’Œæ‹’ç»çš„æ‘˜è¦çš„ attention_maskã€‚
- labels: collator ä¸ºé€‰æ‹©çš„æ‘˜è¦åˆ›å»ºä¸€ä¸ªé›¶å¼ é‡ï¼Œä¸ºåœ¨ dim=0 ä¸Šè¿æ¥çš„æ‹’ç»æ‘˜è¦åˆ›å»ºä¸€ä¸ªå¼ é‡ã€‚

è¯·æ³¨æ„ï¼Œç”±äºè¿™ç§è¿æ¥ï¼Œæä¾›ç»™æ¨¡å‹çš„æ‰¹å¤„ç†æ˜¯å…¨å±€æ‰¹å¤„ç†å¤§å°çš„ä¸¤å€ã€‚

```python
class DataCollatorReward:
     def __call__(self, data):
         batch = {}
         batch["input_ids"] = torch.cat([f[0] for f in data] + [f[2] for f in data])
         batch["attention_mask"] = torch.cat([f[1] for f in data] + [f[3] for f in data])
         batch["labels"] = torch.tensor([0] * len(data) + [1] * len(data))
         return batch
```

### å¥–åŠ±æ¨¡å‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª Reddit å¸–å­å’Œä¸¤ä¸ªæ‘˜è¦ï¼ˆé€‰æ‹©å’Œæ‹’ç»ï¼‰ä½œä¸ºè¾“å…¥ã€‚çœŸå®æ ‡ç­¾ï¼ˆlabelsï¼‰æ˜¯äººç±»çš„åé¦ˆï¼ˆ0 ä»£è¡¨é€‰æ‹©ï¼Œ1 ä»£è¡¨æ‹’ç»ï¼‰ã€‚æŸå¤±å‡½æ•°ä¸ºï¼š

åœ¨ä¸Šè¿°å…¬å¼ä¸­ï¼Œ *ï¿½ï¿½ y* **i ï»¿ï¼Œå…¶ä¸­ ï¿½ï¿½âˆˆ{0,1} *i* âˆˆ{0,1} ï¼Œæ˜¯äººç±»é¦–é€‰æˆ–é€‰æ‹©çš„æ‘˜è¦ã€‚å¥–åŠ±æ¨¡å‹ *ï¿½ï¿½r* **Î¸ï»¿ é‡‡ç”¨å¸–å­ *ï¿½ï¿½x*ï»¿ å’Œæ‘˜è¦ *ï¿½ï¿½y*ï»¿ å¹¶è¿”å›æ ‡é‡å€¼ã€‚ä¸ºä¸¤ä¸ªæ‘˜è¦è®¡ç®—è¯¥å€¼ï¼Œå¹¶å°† sigmoid æ¿€æ´»åº”ç”¨äºå·®å¼‚ã€‚æœ€åï¼Œè®¡ç®—è´Ÿå¯¹æ•°ã€‚

![å›¾ç‰‡](https://api.wandb.ai/files/carperai/images/projects/37218153/8b589edc.png)

ï¼ˆ[æ¥æº](https://arxiv.org/pdf/2009.01325.pdf)ï¼‰

GPTRewardModel ç±»ä½¿ç”¨ SFT æ¨¡å‹å’Œå…¶ä¸Šçš„çº¿æ€§å±‚åˆå§‹åŒ– GPT-J æ¨¡å‹ã€‚å®ƒè¿˜è®¡ç®—ä¸Šé¢æ˜¾ç¤ºçš„æŸå¤±ã€‚

```python
class GPTRewardModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        model = AutoModelForCausalLM.from_pretrained(config)
        self.config = model.config
        # gpt-neo models have hidden_size instead of n_embd
        self.config.n_embd = (
            self.config.hidden_size
            if hasattr(self.config, "hidden_size")
            else self.config.n_embd
        )
        self.transformer = model.transformer
        self.v_head = nn.Linear(self.config.n_embd, 1, bias=False)
        self.tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.PAD_ID = self.tokenizer(self.tokenizer.pad_token)["input_ids"][0]
ï»¿

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
    ):
        transformer_outputs = self.transformer(
            input_ids,
            attention_mask=attention_mask,
        )
        hidden_states = transformer_outputs[0]
        rewards = self.v_head(hidden_states).squeeze(-1)
        reward_scores = []
        bs = input_ids.shape[0] // 2
    # Note half is chosen and another half is rejected.
        chosen = input_ids[:bs]
        rejected = input_ids[bs:]
        chosen_rewards = rewards[:bs]
        rejected_rewards = rewards[bs:]
        # compute pairwise loss. Only backprop on last value before padding
        loss = 0
        for i in range(bs):
            # Find the index of the first occurrence where chosen summary input_ids
        # and rejected summary input_ids are different.
            divergence_ind = (chosen[i] != rejected[i]).nonzero()[0]
ï»¿

        # Find the index of the first occurrence of the padding token the chosen summary.
            c_inds = (chosen[i] == self.PAD_ID).nonzero()
            c_ind = c_inds[0].item() if len(c_inds) > 0 else chosen.shape[1]
ï»¿

        # Find the index of the first occurrence of the padding token the rejected summary.
            r_inds = (rejected[i] == self.PAD_ID).nonzero()
            r_ind = r_inds[0].item() if len(r_inds) > 0 else rejected.shape[1]
            end_ind = max(c_ind, r_ind)
        
        # Find the slice of reward which belongs to diverging input_ids
            c_truncated_reward = chosen_rewards[i][divergence_ind:end_ind]
            r_truncated_reward = rejected_rewards[i][divergence_ind:end_ind]
            reward_scores.append(c_truncated_reward[-1])  # reward at last token
            
            # Compute loss
            loss += -torch.log(
                torch.sigmoid(c_truncated_reward - r_truncated_reward)
            ).mean()
            loss = loss / bs
        return {"loss": loss, "reward_scores": torch.stack(reward_scores)}
```

æˆ‘ä»¬çš„æ¨¡å‹æ¥æ”¶æ•°æ®æ•´ç†å™¨å‡†å¤‡çš„è¾“å…¥ã€‚æ­¤è¾“å…¥é€šè¿‡ GPT-J æ¨¡å‹ä¼ é€’ä»¥è·å¾—æœ€ç»ˆçš„éšè—çŠ¶æ€ã€‚ç„¶åéšè—çŠ¶æ€é€šè¿‡çº¿æ€§å±‚è·å¾—å¥–åŠ±åˆ†æ•°ã€‚å¯¹äºè¾“å…¥æ¨¡å‹çš„æ¯ä¸ªæ‰¹æ¬¡ï¼Œå‰åŠéƒ¨åˆ†æ˜¯é€‰æ‹©çš„æ‘˜è¦ï¼ŒååŠéƒ¨åˆ†æ˜¯æ‹’ç»çš„æ‘˜è¦ã€‚æ¨¡å‹çš„å‰å‘æ–¹æ³•éå†æ¯ä¸ªè¾“å…¥æ ·æœ¬ä»¥è®¡ç®—æˆå¯¹æŸå¤±ã€‚è®¡ç®—æ­¤æŸå¤±æ‰€éœ€çš„æ­¥éª¤è®°å½•åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ã€‚

è¦è®­ç»ƒå¥–åŠ±æ¨¡å‹è¿è¡Œï¼š

```python
!deepspeed examples/summarize_rlhf/reward_model/train_reward_model_gptj.py
```

ä¸‹é¢ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ•´ä¸ªå¥–åŠ±æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±ä»¥åŠå‡†ç¡®æ€§ã€‚

## ä½¿ç”¨ PPO è¿›è¡Œå¾®è°ƒ

[æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ trlX ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–](https://openai.com/blog/openai-baselines-ppo/)(PPO) ç®—æ³•å¾®è°ƒ SFT æ¨¡å‹ã€‚

PPOç®—æ³•ä½¿ç”¨ä»·å€¼å‡½æ•°ï¼Œå¯ä»¥æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™ä¸ªå€¼å‡½æ•°æ˜¯ç”¨ SFT æ¨¡å‹åˆå§‹åŒ–çš„ GPT-J æ¨¡å‹ã€‚ç­–ç•¥ ( *ï¿½ Ï€* ) ä¹Ÿä½¿ç”¨ Reddit TL;DR æ•°æ®é›†ä¸Šçš„å¾®è°ƒ GPT-J Transformer (SFT) è¿›è¡Œåˆå§‹åŒ–ã€‚ç„¶ååƒä»»ä½• RL ç­–ç•¥ä¸€æ ·ä½¿ç”¨å¥–åŠ±æ¨¡å‹çš„è¾“å‡ºä½œä¸ºè¯¥ç­–ç•¥çš„å¥–åŠ±å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚[ï»¿](https://openai.com/blog/openai-baselines-ppo/)ï»¿

![å›¾ç‰‡](https://api.wandb.ai/files/carperai/images/projects/37218153/1c367d95.png)

ï¼ˆ[æ¥æº](https://arxiv.org/pdf/2009.01325.pdf)ï¼‰

ä½†æ˜¯ï¼Œè¿™é‡Œæœ‰å‡ ç‚¹å€¼å¾—ç‰¢è®°ï¼š

### é™·é˜± 1ï¼šè§„èŒƒåŒ–

ç”±äºåŸå§‹å¥–åŠ±åˆ†æ•°å…·æœ‰é«˜æ–¹å·®ï¼Œå› æ­¤ä½¿ç”¨ä»äººç±»ç¼–å†™çš„æ‘˜è¦è®¡ç®—çš„å¥–åŠ±åˆ†æ•°å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ã€‚åœ¨æŒ‰ä»¥ä¸‹æ–¹å¼è®­ç»ƒå¥–åŠ±æ¨¡å‹åè¿›è¡Œå½’ä¸€åŒ–ï¼š

å…¶ä¸­ ï¿½ï¿½(ï¿½ï¿½) *r**m* ( *x* ) å’Œ ï¿½ï¿½(ï¿½ï¿½ï¿½ï¿½) *r**m* ( *x **r** e**f* ) æ˜¯ç»è¿‡è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹åœ¨â€œpost+model generated summaryâ€å’Œâ€œpost+human-written summaryâ€ã€‚â€œpost+<....>â€çš„æ„æ€æ˜¯ï¼Œâ€œ<...>â€è¿æ¥åˆ° Redditâ€œpostâ€ï¼Œå¦‚ä¸Šä¸€èŠ‚æ‰€ç¤ºã€‚

trlX æ¡†æ¶éœ€è¦ä¸€ä¸ªåœ¨ä¸‹é¢å®ç°çš„ reward_fnã€‚è§„èŒƒåŒ–æ­¥éª¤æ˜¯åœ¨æ­¤å‡½æ•°æœ¬èº«ä¸­å®Œæˆçš„ã€‚

```python
def reward_fn(samples: List[str]):
    # get humans summarizes
    posts = [sample.split('TL;DR')] for sample in samples]
    ref_samples = [post + 'TL;DR' + post_summ_dict[post] for post in post]
    samples_encodings = reward_tokenizer(samples)
    samples_scores = reward_model(**samples_encodings) # get scores from reward model for samples
    ref_samples_encodings = reward_tokenizer(ref_samples) # get scores from reward model corresponding references samples
    ref_samples_scores = reward_model(**ref_samples_encodings)
    norms_rewards = samples_scores - ref_samples_scores
    return norms_rewards
```

### é™·é˜± 2ï¼šKL æ•£åº¦

åœ¨ä½¿ç”¨ PPO ç®¡é“è¿›è¡Œå¾®è°ƒæ—¶ï¼Œä¼šä½¿ç”¨æˆ‘ä»¬çš„ç­–ç•¥ (LLM) ä¸º Reddit å¸–å­ç”Ÿæˆæ‘˜è¦ã€‚è¿™ç¯‡æ–‡ç« å’Œæ‘˜è¦è¢«ä¼ é€’ç»™å¥–åŠ±æ¨¡å‹ä»¥è·å¾—å¥–åŠ±åˆ†æ•°ã€‚æ­¤å¥–åŠ±åˆ†æ•°ç”¨äºæ›´æ–°ç­–ç•¥ã€‚è¯·æ³¨æ„ï¼Œæ“ä½œæ˜¯åˆ†æ‰¹å®Œæˆçš„ã€‚ç„¶è€Œï¼ŒRL è®­ç»ƒæœ‰å™ªéŸ³ï¼Œå°¤å…¶æ˜¯åœ¨å¼€å§‹æ—¶ï¼Œè¿™å¯èƒ½ä¼šä½¿æˆ‘ä»¬çš„æ”¿ç­–åç¦»å¥–åŠ±æœ‰æ•ˆçš„èŒƒå›´å¤ªè¿œã€‚

ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µå‘ç”Ÿï¼Œåœ¨å¥–åŠ±å‡½æ•°ä¸­æ·»åŠ äº†ä¸€ä¸ª KL é¡¹ä½œä¸ºæƒ©ç½šï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

è¿™ä¸ª KL æœ¯è¯­æ˜¯åœ¨ trlX æ¡†æ¶ä¸­[å®ç°çš„](https://github.com/CarperAI/trlx/blob/0c5246f64e5e0ecb5fb2de65d440b122c792caf8/trlx/orchestrator/ppo_orchestrator.py#L224)ï¼Œå› æ­¤æ‚¨ä¸éœ€è¦è‡ªå·±å®ç°å®ƒã€‚

è¦ä½¿ç”¨ PPO å’Œè®­ç»ƒæœ‰ç´ çš„å¥–åŠ±æ¨¡å‹å¾®è°ƒ SFT æ¨¡å‹ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```
!deepspeed examples/summarize_rlhf/trlx_gptj_text_summarization.py
```

è®©æˆ‘ä»¬çœ‹çœ‹ä½¿ç”¨ trlX å¾®è°ƒæˆ‘ä»¬çš„ SFT æ¨¡å‹æ—¶çš„æŸå¤±ã€‚

åœ¨ä½¿ç”¨ RL è®­ç»ƒä»£ç†æ—¶ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–å¥–åŠ±åˆ†æ•°ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†å¹³å‡å¥–åŠ±éšç€è®­ç»ƒçš„è¿›è¡Œè€Œå¢åŠ ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä½¿ç”¨ PPO å¾®è°ƒçš„ SFT æ¨¡å‹çš„ ROUGE åˆ†æ•°ï¼Œå¹¶å°†å…¶ä¸ SFT æ¨¡å‹çš„ ROUGE åˆ†æ•°è¿›è¡Œæ¯”è¾ƒã€‚è¯·æ³¨æ„ï¼ŒROUGE åˆ†æ•°è¶Šé«˜è¶Šå¥½ã€‚

æ˜¾ç„¶ï¼Œä½¿ç”¨ PPO å¾®è°ƒçš„ SFT æ¨¡å‹çš„ ROUGE åˆ†æ•°æ¯”ä»… SFT æ¨¡å‹å·®ã€‚é‚£ä¹ˆæœ‰ç›‘ç£çš„å¾®è°ƒå°±è¶³å¤Ÿäº†å—ï¼Ÿå¹¶ä¸çœŸåœ°ã€‚ROUGE ä¸æ•æ‰äººç±»çš„åå¥½ã€‚å¦‚æœæ¨¡å‹ç®€å•åœ°ç”Ÿæˆç±»ä¼¼äºäººç±»ç¼–å†™çš„æ‘˜è¦ï¼Œè¿™æ ·çš„åˆ†æ•°ä¼šæ›´é«˜ã€‚ä½†æ˜¯ç»™å®šçš„äººå·¥ç¼–å†™çš„æ‘˜è¦å¯èƒ½ä¸æ˜¯é¦–é€‰ã€‚æˆ‘ä»¬æƒ³è¦ä¸€ä¸ªæ•´ä½“ä¸Šç¬¦åˆäººç±»åå¥½çš„æ¨¡å‹ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå®˜æ–¹æŠ¥å‘Šçš„ ROUGE åˆ†æ•°ä¸æˆ‘ä»¬çš„ç»“æœï¼ˆPPO å¾®è°ƒæ¨¡å‹å…·æœ‰è¾ƒä½çš„ ROUGE åˆ†æ•°ï¼‰è¶‹åŠ¿ä¸€è‡´ã€‚

![å›¾ç‰‡](https://api.wandb.ai/files/carperai/images/projects/37218153/0ebd093e.png)

ï¼ˆ[æ¥æºï¼›ç¬¬ 34 é¡µ](https://arxiv.org/pdf/2009.01325.pdf)ï¼‰

ä¸‹é¢è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„ SFT æ¨¡å‹å’Œ PPO å¾®è°ƒæ¨¡å‹ç”Ÿæˆçš„ä¸€äº›æ‘˜è¦ã€‚ä½œä¸ºäººç±»è¯»è€…ï¼Œæ‚¨å¯ä»¥å†³å®š RL_PPO æ‘˜è¦æ˜¯å¦ä¼˜äºç®€å•çš„ç›‘ç£å¾®è°ƒ (SFT) æ‘˜è¦ã€‚

> è­¦å‘Šï¼šæŸäº›æ ·æœ¬å¯èƒ½åŒ…å«å…·æœ‰æ”»å‡»æ€§çš„è¾“å‡ºã€‚

## ç»“è®º

ï»¿[InstructGPT](https://openai.com/blog/instruction-following/)è¡¨æ˜ï¼Œé€šè¿‡ç»“åˆäººç±»åé¦ˆï¼ˆé€šè¿‡å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼‰å’Œä½¿ç”¨ RLï¼ŒLLM æ›´ç¬¦åˆäººç±»åå¥½ã€‚ç¬¦åˆäººç±»åå¥½çš„æ¨¡å‹å¯ä»¥[æé«˜æ¨¡å‹çš„å®‰å…¨æ€§å’Œæƒ…ç»ª](https://arxiv.org/pdf/2204.05862.pdf)ï¼Œä½†æ˜¯ï¼Œå®ƒä¸ä¼šæ¶ˆé™¤ LLM ä¸­çš„æ½œåœ¨åè§ã€‚[ChatGPT](https://openai.com/blog/chatgpt/)ï¼Œå®ƒçš„å…„å¼Ÿï¼Œä½¿ç”¨äº†ä¸€ç§å¯¹è¯æ ¼å¼ï¼Œå¯ä»¥å›ç­”åç»­é—®é¢˜ã€æ‰¿è®¤é”™è¯¯ã€æŒ‘æˆ˜ä¸æ­£ç¡®çš„å‰æå’Œæ‹’ç»ä¸é€‚å½“çš„è¯·æ±‚ã€‚ChatGPT æŠ“ä½äº†å¤§ä¼—çš„æƒ³è±¡åŠ›ã€‚å®ƒé¦–æ¬¡ä½¿ RL å®ç”¨åŒ–ã€‚

ä¸ºäº†è®© RLHF çš„ç ”ç©¶æ›´å®¹æ˜“è·å¾—ï¼ŒCarperAI çš„äººä»¬æ„å»ºäº† trlX - ä¸€ä¸ªå­˜å‚¨åº“ï¼Œå…è®¸æ‚¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒ Hugging Face æ”¯æŒçš„è¯­è¨€æ¨¡å‹ï¼ˆåŸºäº gpt2ã€gpt-jã€gpt-neo å’Œ gpt-neoxï¼‰å¹¶æä¾›å¥–åŠ±æ¨¡å‹ã€‚ä»–ä»¬è¿˜æ„å»ºäº†[CHEESE](https://github.com/CarperAI/cheese)ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ„å»ºæ»¡è¶³ RLHF éœ€æ±‚çš„æ•°æ®æ ‡æ³¨å¹³å°ã€‚

æœ€åï¼Œæœ¬æ•™ç¨‹æ—¨åœ¨ä½¿ RLHF æ›´æ˜“äºç†è§£ã€‚æˆ‘ä»¬å·²ç»å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ trlX ä¸ºæ‘˜è¦ä»»åŠ¡å®ç° RLHFã€‚

æˆ‘ä»¬å¸Œæœ›å®ƒèƒ½æ¿€å‘å¤§å®¶æ›´å¤šåœ°äº†è§£è¿™ä¸ªæ¦‚å¿µã€‚å¦‚æœä½ æƒ³ä¸º trlX è´¡çŒ®ä¸€ä¸ªæœ‰ä»·å€¼çš„ä¾‹å­ï¼Œæ‰“å¼€ä¸€ä¸ª PRã€‚æ‚¨ä¹Ÿå¯ä»¥åŠ å…¥ CarperAI çš„[Discord é¢‘é“](https://discord.com/invite/KgfkCVYHdu)ï¼Œå°±æœ¬æ•™ç¨‹æå‡ºé—®é¢˜ï¼Œæ›´ç§¯æåœ°å‚ä¸ã€‚

## å‚è€ƒ

1. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, "[Learning to summarize from human feedback](https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)", Neural Information Processing Systems, 2020.
2. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving, "[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)", arXiv, 2019.
3. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Jared Kaplan, "[A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/abs/2112.00861)", arXiv, 2021.
4. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, "[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)", arXiv, 2017.
5. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine, "[Offline RL for Natural Language Generation with Implicit Language Q Learning](https://arxiv.org/abs/2206.11871)", arXiv, 2022.
6. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, "[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)", arXiv, 2022.
7. Ayush Thakur, "[Understanding Reinforcement Learning from Human Feedback (RLHF): Part 1](https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx)", 2023.
8. Nathan Lambert, Louis Castricato, Leandro von Werra, Alex Havrilla, "[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)", 2022.

