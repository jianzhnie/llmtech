# 预填充-解码解耦

大型语言模型（LLM）日益复杂，需要更高效的推理策略。预填充解码解耦是一项重要的人工智能优化技术，它将推理的两个关键阶段解耦。预填充阶段负责输入上下文的批量处理，而 LLM 解码阶段则专注于逐个生成Token。通过分离这两个阶段，我们可以更好地利用硬件资源、降低延迟并支持更长上下文——使大规模推理优化更实用且可扩展。

## 1. 什么是预填充与解码？

在大型语言模型（LLM）的推理过程中，通常包含两个阶段：

### 预填充阶段（输入处理）

在此阶段，模型并行处理整个输入文本，将注意力层中的键值向量存储在 KV 缓存中，以支持后续生成。该过程以高度并行的矩阵运算为核心，计算密集但内存使用相对较低。非常适合在髙吞吐量 GPU 或多 GPU 系统上运行，能高效处理输入数据。

### 解码阶段（输出生成）

在 LLM 解码阶段，模型进入自回归生成模式：基于预填充阶段存储的 KV 缓存和先前生成的内容，逐个预测并输出新Token。新Token的生成都依赖于前一个结果，使得整个过程严格串行。

该阶段的计算模式与预填充阶段完全不同。相较于预填充阶段高度并行的大规模矩阵计算，解码逐个生成Token的特性使其对延迟高度敏感。随着上下文扩展，KV 缓存持续增长，导致 GPU 显存使用量快速上升。

因此，LLM 解码阶段通常比预填充慢得多，且往往无法充分利用 GPU。在传统部署中，预填充和解码阶段常共享相同 GPU。但由于预填充阶段优先考虑高吞吐量计算，而解码阶段强调低延迟和大显存，将两者混合往往会限制各自的性能表现。

###  预填充和解码合并执行的问题

在传统架构中，LLM 预填充和 LLM 解码阶段合并运行，共享同一设备，表面看来这似乎很直接。

实践中，经常会同时收到多个请求。每个请求都有各自的预填充和解码需求，但每次只能运行一个阶段。当 GPU 被计算密集的预填充任务占用时，解码任务必须等待，这会增加 ITL（推理延迟），反之亦然。

由于预填充主要决定 TTFT（首Token生成时间），而解码影响 ITL（推理延迟），将二者部署在同一位置会难以同时优化这两个指标。

此外，两个阶段适合不同的并行化策略：预填充适合通过张量并行降低延迟，解码则适合通过流水线并行提升吞吐量。这种混合运行模式效率低下。

## 2. 什么是预填充-解码解耦？

顾名思义，预填充解码解耦将模型的两个阶段部署在不同硬件资源池中，以更好匹配各自的计算特性。

预填充阶段侧重高吞吐量计算，因此采用算力强劲且适合大规模并行计算的 GPU。解码阶段则对延迟敏感，需要更大显存来存储不断增长的上下文信息，故而采用显存更大、响应更快的 GPU，从而提升整体 LLM 推理优化效率。

## 3. 预填充-解码解耦的优势

预填充解码分离通过物理隔离两个阶段，可实现更高效的资源利用和更流畅的响应。

- **专用资源分配：** 预填充和解码可在不同硬件上独立调度与扩展。对于高重叠工作负载（如多轮对话或智能体工作流），可重复利用大部分 KV 缓存，降低预填充计算需求，为解码阶段释放资源。
- **并行执行：** 预填充和解码不再相互阻塞，可以更高效地并行运行，从而实现更好的并发性和吞吐量。
- **独立优化：** 可对预填充和解码阶段分别采用不同的 LLM 优化策略（如张量并行或流水线并行），以更好地实现降低首Token生成时间（TTFT）和 LLM 推理延迟（ITL）的目标。

多个开源框架和项目正在积极探索 PD 解耦，包括 [SGLang](https://github.com/sgl-project/sglang/issues/4655)、[vLLM](https://docs.vllm.ai/en/latest/features/disagg_prefill.html)、[Dynamo](https://docs.nvidia.com/dynamo/latest/architecture/disagg_serving.html) 和 [llm-d](https://docs.google.com/document/d/1FNN5snmipaTxEA1FGEeSH7Z_kEqskouKD1XYhVyTHr8/edit?pli=1&tab=t.0)。

## 4. 预填充-解码分离的挑战

### 额外通信开销

若 LLM 预填充与 LLM 解码在不同 GPU 上执行，需传输 KV 缓存，此时网络延迟与带宽将成为瓶颈。

**解决方案：**

- **高性能网络：** 采用超低延迟、高带宽网络降低预填充节点与解码节点间的 KV 缓存传输延迟。
- **KV 缓存分区：** 按Token粒度分片存储数据，最大化数据与解码节点的邻近性以减少跨节点传输。

### 调度复杂度提升

系统需实时协调双列任务分配，否则 LLM 解码阶段可能空等预填充结果，或预填充资源被解码阶段阻塞。

**解决方案：**

- **优先级调度：** 优先执行 LLM 预填充任务以确保低首Token延迟（TTFT），公平调度解码任务以维持稳定吞吐量（TPOT）。
- **流水线并行：** 并行执行预填充与解码任务，确保解码不空闲且预填充资源不被阻塞。

### 实现复杂度高

框架与任务调度器需支持流水线操作、KV 缓存输出及异步执行等功能，其维护难度高于单机单 GPU 的 LLM 推理方案。

**解决方案：**

- **流水线与异步执行支持：** 任务调度器可管理预填充与解码的重叠执行，避免空闲等待与资源阻塞。
- **KV 缓存输出管理：** 确保缓存数据的正确传输与使用，提升跨 GPU 及跨节点 LLM 推理优化的稳定性。

### 高内存占用

预填充阶段可能同时处理多请求，解码阶段需保留完整 KV 缓存，二者均会带来显著内存压力。

**解决方案：**

- **分层存储：** 将热数据保留在 GPU 内存中，暖/冷数据存储在主机内存或 NVMe 中，从而降低内存使用量。
- **内存池管理：** 通过内存复用减少动态分配开销。
- **RDMA 加速：** 在分布式场景中实现快速 KV 缓存传输，缓解 LLM 预填充和 LLM 解码阶段的内存压力

## 结论

预填充解码分离技术有助于提升 LLM 推理的速度与效率。通过在独立资源上运行预填充和解码过程，该技术能够降低延迟、提高吞吐量并优化内存使用。随着模型规模的增长，这一方法已成为保持 AI 系统响应能力与可扩展性的关键，为更可靠的大规模 LLM 推理优化铺平道路。
