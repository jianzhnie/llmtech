# Transformerçš„KV Cachingæœºåˆ¶è¯¦è§£

## KV Cachingæ¦‚è¿°

ç”Ÿæˆå¼Transformeræ¨¡å‹ä¸­çš„é”®(Key)å’Œå€¼(Value)çŠ¶æ€ç¼“å­˜æŠ€æœ¯å·²å­˜åœ¨ä¸€æ®µæ—¶é—´ï¼Œä½†æ‚¨å¯èƒ½éœ€è¦ç¡®åˆ‡ç†è§£å®ƒçš„åŸç†åŠå…¶å¸¦æ¥çš„æ˜¾è‘—æ¨ç†åŠ é€Ÿæ•ˆæœã€‚

é”®å€¼çŠ¶æ€ç”¨äºè®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›(scaled dot-product attention)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›åŠå…¶åœ¨Transformeræ¶æ„ä¸­çš„åº”ç”¨ä½ç½®](https://miro.medium.com/v2/resize:fit:690/0*6D_17aytq215gMcF.png)

> ***KV Cachingå‘ç”Ÿåœ¨å¤štokenç”Ÿæˆæ­¥éª¤ä¸­ï¼Œä»…å­˜åœ¨äºè§£ç å™¨éƒ¨åˆ†*** *(ä¾‹å¦‚GPTç­‰çº¯è§£ç å™¨æ¨¡å‹ï¼Œæˆ–T5ç­‰ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„è§£ç å™¨éƒ¨åˆ†)ã€‚åƒBERTè¿™æ ·çš„éç”Ÿæˆå¼æ¨¡å‹ä¸ä½¿ç”¨KV Cachingã€‚*

### è‡ªå›å½’è§£ç æœºåˆ¶

è§£ç å™¨ä»¥è‡ªå›å½’æ–¹å¼å·¥ä½œï¼Œå¦‚ä¸‹é¢GPT-2æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹æ‰€ç¤ºï¼š

<img src="https://miro.medium.com/v2/resize:fit:700/0*sexO6adGhaKr7aH0.gif" alt="GPT-2è§£ç å™¨çš„è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹" style="zoom:50%;" />

åœ¨è§£ç å™¨çš„è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç„¶åå°†ç»„åˆè¾“å…¥ç”¨äºä¸‹ä¸€æ­¥é¢„æµ‹ã€‚

è¿™ç§è‡ªå›å½’è¡Œä¸ºä¼šé‡å¤æŸäº›è®¡ç®—æ“ä½œã€‚é€šè¿‡æ”¾å¤§è§‚å¯Ÿè§£ç å™¨ä¸­çš„æ©ç ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ¸…æ¥šåœ°ç†è§£è¿™ä¸€ç‚¹ï¼š

<img src="https://miro.medium.com/v2/resize:fit:700/1*8xqD4AYTwn6mQXNw0uhDCg.gif" alt="è§£ç å™¨ä¸­ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„é€æ­¥å¯è§†åŒ–" style="zoom:50%;" />

ç”±äºè§£ç å™¨æ˜¯å› æœçš„(å³tokençš„æ³¨æ„åŠ›ä»…å–å†³äºå…¶å‰é¢çš„token)ï¼Œåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­æˆ‘ä»¬éƒ½åœ¨é‡å¤è®¡ç®—ç›¸åŒçš„å‰ç½®tokenæ³¨æ„åŠ›ï¼Œè€Œå®é™…ä¸Šæˆ‘ä»¬åªéœ€è¦è®¡ç®—æ–°tokençš„æ³¨æ„åŠ›ã€‚

### è‡ªå›å½’è§£ç ä»£ç ç¤ºä¾‹

```python
import torch

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
# torch.manual_seed(0)

class Sampler:
    def __init__(self , model_name : str ='gpt2-medium') -> None:

        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to("cpu").to(self.device)

    def encode(self, text):
        return self.tokenizer.encode(text, return_tensors='pt').to(self.device)

    def decode(self, ids):
        return self.tokenizer.decode(ids)

    def get_next_token_prob(self, input_ids: torch.Tensor):
        with torch.no_grad():
            logits = self.model(input_ids=input_ids).logits
        logits = logits[0, -1, :]
        return logits

class GreedySampler(Sampler):
    def __call__(self, prompt, max_new_tokens=10):
        predictions = []
        result = prompt
        # generate until max_len
        for i in range(max_new_tokens):

            print(f"step {i} input: {result}")
            input_ids = self.encode(result)
            next_token_probs = self.get_next_token_prob(input_ids=input_ids)

            # choose the token with the highest probability
            id = torch.argmax(next_token_probs, dim=-1).item()
            # convert to token and add new token to text
            result += self.decode(id)

            predictions.append(next_token_probs[id].item())

        return result
```



```shell
gs = GreedySampler()
gs(prompt="Large language models are recent advances in deep learning", max_new_tokens=10)

step 0 input: Large language models are recent advances in deep learning
step 1 input: Large language models are recent advances in deep learning,
step 2 input: Large language models are recent advances in deep learning, which
step 3 input: Large language models are recent advances in deep learning, which uses
step 4 input: Large language models are recent advances in deep learning, which uses deep
step 5 input: Large language models are recent advances in deep learning, which uses deep neural
step 6 input: Large language models are recent advances in deep learning, which uses deep neural networks
step 7 input: Large language models are recent advances in deep learning, which uses deep neural networks to
step 8 input: Large language models are recent advances in deep learning, which uses deep neural networks to learn
step 9 input: Large language models are recent advances in deep learning, which uses deep neural networks to learn to

```

å¯ä»¥çœ‹åˆ°ï¼Œéšç€æ¯æ¬¡æ¨ç†çš„è¾“å…¥tokenå˜é•¿ï¼Œæ¨ç†FLOPs(æµ®ç‚¹è¿ç®—)ä¼šå¢åŠ ã€‚KV Cachingé€šè¿‡å­˜å‚¨å…ˆå‰è®¡ç®—çš„é”®å€¼å¯¹çš„éšè—è¡¨ç¤ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

ä¾‹å¦‚åœ¨ç¬¬4æ­¥ç”Ÿæˆ"deep"æ—¶ï¼Œæˆ‘ä»¬åªéœ€å°†"uses"è¾“å…¥æ¨¡å‹ï¼Œå¹¶ä»ç¼“å­˜ä¸­è·å–"Large language models are recent advances in deep learning, which"çš„è¡¨ç¤ºã€‚

### KV Caching åŸºç¡€åŸç†

åœ¨Transformeræ¶æ„ä¸­ï¼Œé”®å€¼ï¼ˆKey-Valueï¼ŒKVï¼‰å‘é‡æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒè®¡ç®—å•å…ƒï¼Œç”¨äºç”ŸæˆQuery-Keyç‚¹ç§¯æ³¨æ„åŠ›åˆ†æ•°ã€‚ä»¥GPTä¸ºä»£è¡¨çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹é‡‡ç”¨é€tokenç”Ÿæˆç­–ç•¥ï¼Œå…¶è®¡ç®—è¿‡ç¨‹å…·æœ‰ä¸¥æ ¼çš„å‰å‘ä¾èµ–æ€§â€”â€”æ¯ä¸ªæ–°tokençš„é¢„æµ‹éƒ½éœ€è¦åŸºäºå®Œæ•´çš„å…ˆå‰ä¸Šä¸‹æ–‡é‡æ–°è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚è¿™ç§æœºåˆ¶å¯¼è‡´å†å²tokençš„KVå‘é‡åœ¨æ¯æ¬¡æ¨ç†è¿­ä»£æ—¶éƒ½è¢«é‡å¤è®¡ç®—ï¼Œäº§ç”Ÿæ˜¾è‘—çš„ç®—åŠ›å†—ä½™ã€‚

KVç¼“å­˜æŠ€æœ¯é€šè¿‡æŒä¹…åŒ–å­˜å‚¨å†å²tokençš„KVå‘é‡çŠ¶æ€ï¼Œæœ‰æ•ˆè§£å†³äº†è¿™ä¸€æ€§èƒ½ç“¶é¢ˆã€‚å…¶æŠ€æœ¯ä¼˜åŠ¿ä¸»è¦ä½“ç°åœ¨ï¼š

1. **è®¡ç®—å¤ç”¨**ï¼šé¿å…é‡å¤è®¡ç®—å·²ç”Ÿæˆtokençš„KVå‘é‡
2. **å»¶è¿Ÿä¼˜åŒ–**ï¼šå°†ç«¯åˆ°ç«¯æ¨ç†å»¶è¿Ÿé™ä½30-70%ï¼ˆå®æµ‹æ•°æ®ï¼‰
3. **ç²¾åº¦æ— æŸ**ï¼šä¿æŒåŸå§‹æ¨¡å‹è¾“å‡ºçš„æ•°å­¦ç­‰ä»·æ€§
4. **å†…å­˜-è®¡ç®—æƒè¡¡**ï¼šé€šè¿‡ç‰ºç‰²éƒ¨åˆ†å†…å­˜å¼€é”€æ¢å–è®¡ç®—æ•ˆç‡æå‡

## KV Cachingçš„å·¥ä½œåŸç†

åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒTransformeræ¨¡å‹[ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªtoken](https://neptune.ai/blog/customizing-llm-output-post-processing-techniques)ã€‚å½“æˆ‘ä»¬æç¤ºæ¨¡å‹å¼€å§‹ç”Ÿæˆæ—¶(ä¾‹å¦‚è¾“å…¥"She")ï¼Œå®ƒå°†äº§ç”Ÿä¸€ä¸ªè¯(å¦‚"poured")ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†"She poured"ä¼ é€’ç»™æ¨¡å‹ï¼Œå®ƒä¼šç”Ÿæˆ"coffee"ã€‚æ¥ç€æˆ‘ä»¬ä¼ å…¥"She poured coffee"å¹¶è·å¾—åºåˆ—ç»“æŸtokenï¼Œè¡¨ç¤ºç”Ÿæˆå®Œæˆã€‚

è¿™æ„å‘³ç€æˆ‘ä»¬è¿è¡Œäº†ä¸‰æ¬¡å‰å‘ä¼ æ’­ï¼Œæ¯æ¬¡éƒ½å°†æŸ¥è¯¢(queries)ä¸é”®(keys)ç›¸ä¹˜ä»¥è·å¾—æ³¨æ„åŠ›åˆ†æ•°(åŒæ ·é€‚ç”¨äºåç»­ä¸å€¼(values)çš„ä¹˜æ³•)ã€‚

### è®¡ç®—å†—ä½™åˆ†æ

1. ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­åªæœ‰å•ä¸ªè¾“å…¥token("She")ï¼Œäº§ç”Ÿå•ä¸ªé”®å‘é‡å’ŒæŸ¥è¯¢å‘é‡ï¼Œç›¸ä¹˜å¾—åˆ°q1k1æ³¨æ„åŠ›åˆ†æ•°ã€‚

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-2.png" alt="ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­çš„è®¡ç®—" style="zoom:33%;" />

2. ä¼ å…¥"She poured"åï¼Œæ¨¡å‹çœ‹åˆ°ä¸¤ä¸ªè¾“å…¥tokenï¼Œæ³¨æ„åŠ›æ¨¡å—è®¡ç®—å¦‚ä¸‹ï¼š

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-3.png" alt="ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­çš„è®¡ç®—" style="zoom:33%;" />

æˆ‘ä»¬è®¡ç®—äº†ä¸‰ä¸ªé¡¹ï¼Œä½†q1k1æ˜¯ä¸å¿…è¦çš„é‡å¤è®¡ç®—â€”â€”å› ä¸ºï¼š

- q1æ˜¯è¾“å…¥("She")çš„åµŒå…¥ä¹˜ä»¥WqçŸ©é˜µ
- k1æ˜¯è¾“å…¥("She")çš„åµŒå…¥ä¹˜ä»¥WkçŸ©é˜µ
- åµŒå…¥å’Œæƒé‡çŸ©é˜µåœ¨æ¨ç†æ—¶éƒ½æ˜¯æ’å®šçš„

3. ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­çš„æŸ¥è¯¢-é”®è®¡ç®—ï¼š

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-4.png" alt="ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­çš„è®¡ç®—" style="zoom:33%;" />

æˆ‘ä»¬è®¡ç®—äº†å…­ä¸ªå€¼ï¼Œå…¶ä¸­ä¸€åŠæ˜¯å·²çŸ¥ä¸”ä¸éœ€è¦é‡æ–°è®¡ç®—çš„ï¼

### KV Cachingæœºåˆ¶

KV Cachingçš„åŸç†æ˜¯ï¼šåœ¨æ¨ç†æ—¶ï¼Œå½“æˆ‘ä»¬è®¡ç®—é”®(K)å’Œå€¼(V)çŸ©é˜µæ—¶ï¼Œå°†å…¶å…ƒç´ å­˜å‚¨åœ¨ç¼“å­˜ä¸­ã€‚ç¼“å­˜æ˜¯ä¸€ä¸ªè¾…åŠ©å†…å­˜ï¼Œæ”¯æŒé«˜é€Ÿæ£€ç´¢ã€‚åœ¨ç”Ÿæˆåç»­tokenæ—¶ï¼Œæˆ‘ä»¬åªè®¡ç®—æ–°tokençš„é”®å’Œå€¼ã€‚

ä¾‹å¦‚ï¼Œä½¿ç”¨ç¼“å­˜æ—¶ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­å¦‚ä¸‹ï¼š

<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/Transformers-Key-Value-Caching-Explained-5.png" alt="ä½¿ç”¨ç¼“å­˜åçš„ç¬¬ä¸‰æ¬¡å‰å‘ä¼ æ’­" style="zoom:33%;" />

å¤„ç†ç¬¬ä¸‰ä¸ªtokenæ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦é‡æ–°è®¡ç®—å‰ä¸¤ä¸ªtokençš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¯ä»¥ä»ç¼“å­˜ä¸­æ£€ç´¢å®ƒä»¬çš„é”®å’Œå€¼ï¼Œä»è€ŒèŠ‚çœè®¡ç®—æ—¶é—´ã€‚

### KV Caching åŸç†

è¿™æ­£æ˜¯KV Cachingå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚é€šè¿‡ç¼“å­˜å…ˆå‰çš„Keyså’ŒValuesï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºä»…è®¡ç®—æ–°tokençš„æ³¨æ„åŠ›ï¼š

<img src="https://miro.medium.com/v2/resize:fit:700/1*uyuyOW1VBqmF5Gtv225XHQ.gif" alt="ä½¿ç”¨ä¸ä¸ä½¿ç”¨KV Cachingçš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å¯¹æ¯”" style="zoom:50%;" />

ä¸ºä»€ä¹ˆè¿™ç§ä¼˜åŒ–å¾ˆé‡è¦ï¼Ÿå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œ**ä½¿ç”¨KV Cachingè·å¾—çš„çŸ©é˜µè¦å°å¾—å¤šï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„çŸ©é˜µä¹˜æ³•è¿ç®—**ã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ›´å¤šçš„GPUæ˜¾å­˜(å¦‚æœä¸ä½¿ç”¨GPUåˆ™éœ€è¦æ›´å¤šCPUå†…å­˜)æ¥ç¼“å­˜Keyå’ŒValueçŠ¶æ€ã€‚

## KV Cachingçš„æ•°å­¦è¡¨è¾¾

ç»™å®šç”Ÿæˆçš„ç¬¬ $t $ ä¸ª token åœ¨ Transformer å±‚ä¸­çš„è¡¨ç¤ºï¼Œè®°ä½œ $t^i \in \mathbb{R}^{b \times 1 \times h} $ï¼Œå…¶ä¸­ï¼š

- $b $ è¡¨ç¤º batch size
- $h $ è¡¨ç¤º hidden dimension

åœ¨ Transformer ç¬¬ $t $ å±‚çš„è®¡ç®—åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

1. KV Cachingçš„æ›´æ–°
2. ä¸‹ä¸€å±‚è¾“å…¥ $t^{i+1} $ çš„è®¡ç®—

### KV Cachingæ›´æ–°å…¬å¼

$$
\begin{aligned}
x_{K}^i &\leftarrow \text{Concat} \left( x_{K}^i, t^i \cdot W_{K}^i \right) \\
x_{V}^i &\leftarrow \text{Concat} \left( x_{V}^i, t^i \cdot W_{V}^i \right)
\end{aligned}
$$

### å‰©ä½™è®¡ç®—æ­¥éª¤

1. Query å‘é‡è®¡ç®—ï¼š
   $$
   t_Q^i = t^i \cdot W_Q^i
   $$

2. Attention è¾“å‡ºï¼š
   $$
   t_{\text{out}}^i = \text{softmax} \left( \frac{t_Q^i x_{K}^{i\top}}{\sqrt{h}} \right) \cdot x_V^i \cdot W_O^i + t^i
   $$

3. Feed-Forward è®¡ç®—ï¼š
   $$
   t^{i+1} = f_{\text{activation}} \left( t_{\text{out}}^i \cdot W_1 \right) \cdot W_2 + t_{\text{out}}^i
   $$



## KV Cachingå®ç°

### ç¼“å­˜ç±»å®ç°

å½“ä½¿ç”¨Transformersåº“çš„`Cache`ç±»æ—¶ï¼Œè‡ªæ³¨æ„åŠ›æ¨¡å—ä¼šæ‰§è¡Œå‡ ä¸ªå…³é”®æ­¥éª¤æ¥æ•´åˆè¿‡å»å’Œå½“å‰çš„ä¿¡æ¯ï¼š

1. æ³¨æ„åŠ›æ¨¡å—å°†å½“å‰çš„é”®å€¼å¯¹ä¸ç¼“å­˜ä¸­å­˜å‚¨çš„å†å²é”®å€¼å¯¹è¿›è¡Œæ‹¼æ¥ã€‚è¿™ä¼šåˆ›å»ºå½¢çŠ¶ä¸º`(new_tokens_length, past_kv_length + new_tokens_length)`çš„æ³¨æ„åŠ›æƒé‡ã€‚ å½“å‰å’Œå†å²é”®å€¼å¯¹å®è´¨ä¸Šè¢«ç»„åˆèµ·æ¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç¡®ä¿æ¨¡å‹åŒæ—¶æ„ŸçŸ¥å†å²ä¸Šä¸‹æ–‡å’Œå½“å‰è¾“å…¥
2. å½“è¿­ä»£è°ƒç”¨`forward`æ–¹æ³•æ—¶ï¼Œå¿…é¡»ç¡®ä¿æ³¨æ„åŠ›æ©ç (attention mask)çš„å½¢çŠ¶ä¸å†å²å’Œå½“å‰é”®å€¼å¯¹çš„ç»„åˆé•¿åº¦åŒ¹é…ã€‚æ³¨æ„åŠ›æ©ç åº”å…·æœ‰`(batch_size, past_kv_length + new_tokens_length)`çš„å½¢çŠ¶ã€‚è¿™é€šå¸¸åœ¨`generate()`æ–¹æ³•å†…éƒ¨å¤„ç†ï¼Œä½†å¦‚æœä½ æƒ³ç”¨`Cache`å®ç°è‡ªå·±çš„ç”Ÿæˆå¾ªç¯ï¼Œè¯·ç‰¢è®°è¿™ä¸€ç‚¹ï¼æ³¨æ„åŠ›æ©ç åº”åŒ…å«å†å²å’Œå½“å‰tokençš„å€¼ã€‚
3. è¿˜éœ€è¦ç‰¹åˆ«æ³¨æ„`cache_position`å‚æ•°ã€‚å¦‚æœä½ æƒ³ç”¨`forward`æ–¹æ³•é‡ç”¨é¢„å¡«å……çš„ç¼“å­˜ï¼Œå¿…é¡»ä¼ é€’æœ‰æ•ˆçš„`cache_position`å€¼ã€‚å®ƒè¡¨ç¤ºåºåˆ—ä¸­çš„è¾“å…¥ä½ç½®ã€‚`cache_position`ä¸å—å¡«å……(padding)å½±å“ï¼Œæ€»æ˜¯ä¸ºæ¯ä¸ªtokenå¢åŠ ä¸€ä¸ªä½ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœKV CachingåŒ…å«10ä¸ªtoken(ä¸è€ƒè™‘å¡«å……token)ï¼Œä¸‹ä¸€ä¸ªtokençš„ç¼“å­˜ä½ç½®åº”è¯¥æ˜¯`torch.tensor([10])`ã€‚



### Naive Implemention

å‡è®¾æ¶æ„ä¸­æœ‰nä¸ªTransformerå±‚ï¼Œé‚£ä¹ˆæ¯ä¸ªæ³¨æ„åŠ›å¤´å°†ç»´æŠ¤è‡ªå·±ç‹¬ç«‹çš„KV Cachingï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# æ¨¡æ‹Ÿæ¨¡å‹å‚æ•°ç»“æ„
class ModelArgs:
    def __init__(self, dim=16, n_heads=2, max_seq_len=8, max_batch_size=1):
        self.dim = dim
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.max_seq_len = max_seq_len
        self.max_batch_size = max_batch_size


class SelfAttention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.n_heads = args.n_heads
        self.head_dim = args.head_dim
        self.dim = args.dim

        self.w_q = nn.Linear(self.dim, self.dim, bias=False)
        self.w_k = nn.Linear(self.dim, self.dim, bias=False)
        self.w_v = nn.Linear(self.dim, self.dim, bias=False)
        self.w_o = nn.Linear(self.dim, self.dim, bias=False)

        # ç¼“å­˜åˆå§‹åŒ–
        self.register_buffer("cache_k", torch.zeros(
            args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim
        ))
        self.register_buffer("cache_v", torch.zeros(
            args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim
        ))

    def forward(self, x: torch.Tensor, start_pos: int):
        # x shape: (B, 1, D)
        B, S, D = x.size()
        H = self.n_heads
        Hd = self.head_dim

        # Linear projections
        q = self.w_q(x).view(B, S, H, Hd)
        k = self.w_k(x).view(B, S, H, Hd)
        v = self.w_v(x).view(B, S, H, Hd)

        # æ›´æ–°ç¼“å­˜
        self.cache_k[:B, start_pos:start_pos+S] = k
        self.cache_v[:B, start_pos:start_pos+S] = v

        # è·å–å½“å‰å…¨éƒ¨ key/valueï¼ˆä»0åˆ°å½“å‰ä½ç½®ï¼‰
        keys = self.cache_k[:B, :start_pos+S]   # (B, Seq_KV, H, Hd)
        values = self.cache_v[:B, :start_pos+S]

        # Attentionè®¡ç®—
        q = q.transpose(1, 2)           # (B, H, 1, Hd)
        k = keys.transpose(1, 2)        # (B, H, Seq_KV, Hd)
        v = values.transpose(1, 2)      # (B, H, Seq_KV, Hd)

        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (Hd ** 0.5)  # (B, H, 1, Seq_KV)
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)  # (B, H, 1, Hd)

        attn_output = attn_output.transpose(1, 2).contiguous().view(B, S, D)  # (B, 1, D)
        out = self.w_o(attn_output)
        return out

# ====== æµ‹è¯•ç¤ºä¾‹ ======

torch.manual_seed(42)
args = ModelArgs()
attn = SelfAttention(args)

# æ¨¡æ‹Ÿä¸€ä¸ªåºåˆ—åˆ†æ­¥ç”Ÿæˆï¼Œæ¯æ­¥è¾“å…¥ä¸€ä¸ª token
sequence = torch.randn(1, args.max_seq_len, args.dim)
outputs = []
for i in range(args.max_seq_len):
    x = sequence[:, i:i+1, :]  # å½“å‰æ—¶é—´æ­¥çš„ token
    y = attn(x, start_pos=i)   # KV Caching è‡ªåŠ¨ç”Ÿæ•ˆ
    outputs.append(y)

# æ‹¼æ¥ç»“æœ
final_out = torch.cat(outputs, dim=1)
print("ğŸ§¾ æœ€ç»ˆè¾“å‡º shape:", final_out.shape)
print(final_out)
```

### Transformers ä¸­çš„å®ç°

ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨`DynamicCache`åˆ›å»ºç”Ÿæˆå¾ªç¯ã€‚å¦‚å‰æ‰€è¿°ï¼Œæ³¨æ„åŠ›æ©ç æ˜¯å†å²å’Œå½“å‰tokenå€¼çš„æ‹¼æ¥ï¼Œå¹¶ä¸”ä¸ºä¸‹ä¸€ä¸ªtokenå°†ç¼“å­˜ä½ç½®åŠ 1ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache

model_id = "meta-llama/Llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="cuda:0")
tokenizer = AutoTokenizer.from_pretrained(model_id)

past_key_values = DynamicCache()
messages = [{"role": "user", "content": "Hello, what's your name."}]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt", return_dict=True).to("cuda:0")

generated_ids = inputs.input_ids
cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device="cuda:0")
max_new_tokens = 10

for _ in range(max_new_tokens):
    outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)
    # è´ªå©ªé‡‡æ ·ä¸‹ä¸€ä¸ªtoken
    next_token_ids = outputs.logits[:, -1:].argmax(-1)
    generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)
    # é€šè¿‡ä¿ç•™æœªå¤„ç†çš„token(æœ¬ä¾‹ä¸­åªæœ‰ä¸€ä¸ªæ–°token)æ¥å‡†å¤‡ä¸‹ä¸€ä¸ªç”Ÿæˆæ­¥éª¤çš„è¾“å…¥
    # å¹¶æŒ‰ç…§ä¸Šè¿°è¯´æ˜æ‰©å±•æ³¨æ„åŠ›æ©ç 
    attention_mask = inputs["attention_mask"]
    attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)
    inputs = {"input_ids": next_token_ids, "attention_mask": attention_mask}
    cache_position = cache_position[-1:] + 1 # ä¸ºä¸‹ä¸€ä¸ªtokenå¢åŠ ä¸€ä¸ªä½ç½®

print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])
# è¾“å‡º: "[INST] Hello, what's your name. [/INST]  Hello! My name is LLaMA,"
```

### ä¼ ç»Ÿç¼“å­˜æ ¼å¼

åœ¨`Cache`ç±»å‡ºç°ä¹‹å‰ï¼Œç¼“å­˜æ˜¯ä»¥å¼ é‡å…ƒç»„çš„å…ƒç»„å½¢å¼å­˜å‚¨çš„ã€‚è¿™ç§æ ¼å¼æ˜¯åŠ¨æ€çš„ï¼Œä¼šéšç€æ–‡æœ¬ç”Ÿæˆè€Œå¢é•¿ï¼Œç±»ä¼¼äº`DynamicCache`ã€‚

å¦‚æœä½ çš„é¡¹ç›®ä¾èµ–è¿™ç§ä¼ ç»Ÿæ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨`from_legacy_cache()`å’Œ`DynamicCache.to_legacy_cache()`å‡½æ•°åœ¨`DynamicCache`å’Œå…ƒç»„å…ƒç»„ä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚è¿™å¯¹äºéœ€è¦ä»¥ç‰¹å®šæ ¼å¼æ“ä½œç¼“å­˜çš„è‡ªå®šä¹‰é€»è¾‘å¾ˆæœ‰å¸®åŠ©ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16, device_map="auto")
inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

# `return_dict_in_generate=True`æ˜¯è¿”å›ç¼“å­˜æ‰€å¿…éœ€çš„ï¼Œè€Œ`return_legacy_cache`å¼ºåˆ¶è¿”å›ä¼ ç»Ÿæ ¼å¼çš„ç¼“å­˜
generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)

cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)
legacy_format_cache = cache.to_legacy_cache()

```

## KV Cachingæ€§èƒ½å½±å“è¯„ä¼°

KV Cachingå¯èƒ½å¯¹æ¨ç†æ—¶é—´äº§ç”Ÿé‡å¤§å½±å“ã€‚å½±å“ç¨‹åº¦å–å†³äºæ¨¡å‹æ¶æ„ã€‚å¯ç¼“å­˜çš„è®¡ç®—è¶Šå¤šï¼Œå‡å°‘æ¨ç†æ—¶é—´çš„æ½œåŠ›å°±è¶Šå¤§ã€‚

æˆ‘ä»¬ä½¿ç”¨[transformersğŸ¤—](https://github.com/huggingface/transformers)åº“æ¯”è¾ƒGPT-2åœ¨ä½¿ç”¨å’Œä¸ä½¿ç”¨KV Cachingæ—¶çš„ç”Ÿæˆé€Ÿåº¦ï¼š

```python
import numpy as np
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2").to(device)

for use_cache in (True, False):
  times = []
  for _ in range(10):  # æµ‹é‡10æ¬¡ç”Ÿæˆ
    start = time.time()
    model.generate(**tokenizer("What is KV caching?", return_tensors="pt").to(device), use_cache=use_cache, max_new_tokens=1000)
    times.append(time.time() - start)
  print(f"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds")
```

åœ¨Google Colabç¬”è®°æœ¬ä¸Šä½¿ç”¨Tesla T4 GPUï¼Œç”Ÿæˆ1000ä¸ªæ–°tokençš„å¹³å‡æ—¶é—´å’Œæ ‡å‡†å·®å¦‚ä¸‹ï¼š

> ä½¿ç”¨KV Caching: 11.885 Â± 0.272ç§’
> ä¸ä½¿ç”¨KV Caching: 56.197 Â± 1.855ç§’

æ¨ç†é€Ÿåº¦å·®å¼‚å·¨å¤§ï¼Œè€ŒGPUæ˜¾å­˜ä½¿ç”¨é‡å˜åŒ–å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚å› æ­¤è¯·ç¡®ä¿åœ¨æ‚¨çš„Transformeræ¨¡å‹ä¸­ä½¿ç”¨KV Cachingï¼
