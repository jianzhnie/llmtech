# 从零开始：深入理解大模型高效服务核心技术——连续批处理 (Continuous Batching)

> 本文深入探讨了连续批处理（Continuous Batching）技术，它是现代大型语言模型（LLM）高效推理服务的核心。我们将从注意力机制和键值缓存（KV-Cache）的基础出发，逐步推导出连续批处理的原理，并展示它如何通过优化批处理策略来最大化模型推理的吞吐量。

## 摘要 (TL;DR)

所有LLM本质上都是复杂的“下一词元（next token）预测器”。为了在服务高并发用户时实现实用性，提高吞吐量至关重要。

**连续批处理**是一种革命性的优化技术，它通过**动态调度**、结合**不规则批处理**（Ragged Batching）、**键值缓存**（KV-Cache）和**分块预填充**（Chunked Prefill）来消除传统批处理中的计算和内存浪费（即填充/Padding），从而最大化GPU的利用率和整体吞吐量。

本文将从基础机制出发，逐步推导并解释连续批处理的工作原理。

## 一、 LLM推理的基础：词元生成

当你使用AI聊天机器人时，你会观察到回复的第一个词元（Token）出现需要一定时间，随后后续词元会以稳定、快速的频率逐个出现在屏幕上。

这是因为LLM的工作流程分为两个阶段：

1. **预填充（Prefill）阶段：** 模型处理完整的输入提示（Prompt），并生成**第一个**新词元。
2. **解码（Decoding）阶段：** 模型重复读取**之前生成的所有词元**，并在此基础上一次生成**一个**新词元，直到生成结束标志 `<eos>`。

每次词元生成都需要将输入通过包含数十亿参数的模型进行一次前向传播（Forward Pass），计算成本极高。为了实现高效服务，我们需要深入理解模型的核心计算机制。

### 1.1 注意力机制

注意力机制是LLM的核心组件，它允许序列中的词元相互交互。

在一个输入序列 $x$ 中，首先通过三个投影矩阵 $W_q, W_k, W_v$ 得到查询（Query）$Q$、键（Key）$K$ 和值（Value）$V$ 状态。假设序列长度为 $n$，隐藏维度为 $d$，注意力头维度为 $A$，则 $Q, K, V$ 的形状均为 $[1, n, A]$（其中 $1$ 是批量大小）。

注意力计算的核心在于：

$$
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{Q K^T}{\sqrt{d_k}} + \text{Mask}) V
$$

- **二次复杂度：** 计算相似度矩阵 $Q K^T$ 的操作复杂度为 $\mathcal{O}(n^2 d)$，即与序列长度 $n$ 的平方成正比。这使得预填充长序列的计算成本非常高。
- **因果掩码（Causal Mask）：** 在文本生成中，我们通常使用因果掩码，确保当前词元只能关注它之前出现的词元。

### 1.2 键值缓存（KV-Cache）

在解码阶段，模型每次生成一个新词元时，都需要重新计算其注意力分数。

**朴素生成（Naive Generation）** 的问题在于：为了计算新词元的注意力分数，我们必须重新计算整个序列（包括所有旧词元和新词元）的 $K$ 和 $V$ 状态。这造成了大量的重复计算。

**KV-Cache** 的核心思想是：将预填充和之前解码步骤中已经计算过的 $K$ 和 $V$ 状态缓存起来。

- **加速效果：** 通过复用缓存，生成第 $n+1$ 个词元的计算复杂度从 $O(n^2)$ 降低到 $O(n)$。
- **内存成本：** 虽然降低了计算量，但缓存也引入了内存成本。对于一个具有 $\mathcal{L}$ 层、 $H$ 个头、头维度为 $A$ 的模型，存储一个词元所需的缓存空间为 $2 \times \mathcal{L} \times AH$（K 和 V 各占一半）。
  - *例如，Llama-2-7B（$\mathcal{L}=32, H=32, A=128$）每个词元需要约 $16$ KB 的内存（FP16精度）。*

### 1.3 分块预填充（Chunked Prefill）

在处理超长提示（如大段文档作为上下文）时，即使是预填充阶段，一次性计算整个 $n$ 词元序列的激活值也可能超出GPU内存限制。

**分块预填充**通过利用 KV-Cache 来解决这个问题：

1. 将长提示分割成多个小块（Chunks）。
2. 依次处理每个块，并将计算出的 $K$ 和 $V$ 状态存储到 KV-Cache 中。
3. 后续块在前向传播时，可以读取并拼接之前缓存的 $K$ 和 $V$ 状态，并相应调整注意力掩码。

这个机制的关键在于：**缓存的 $K$ 和 $V$ 状态允许模型以增量方式处理提示，而不丢失任何上下文信息。**



## 二、 批处理的演进：从静态到连续

为了最大化吞吐量（每秒生成的词元数），最有效的方法是**并行处理**多个请求，即进行批处理（Batching）。

### 2.1 朴素批处理（Naive Batched Generation）

最简单的方法是通过在输入张量中增加一个批量维度（Batch Axis）来实现并行。然而，这引入了一个严格约束：**批处理中的所有序列必须具有相同的长度**。

为满足这一要求，我们必须使用 **填充（Padding）** 词元（`<pad>`）将所有提示补齐到批中最长序列的长度。

**缺点：**

1. **静态浪费：** 填充词元本身是无用的计算，占用了宝贵的计算资源。
2. **动态浪费：** 如果批处理中有一个请求提早生成了 `<eos>` 词元，它后续在批次中生成的任何词元都是无用的计算，直到批次中最长的请求完成为止。

### 2.2 动态调度（Dynamic Scheduling）

为了解决动态浪费，我们可以实施**动态调度**（或称动态批处理）。当批次中的一个请求生成 `<eos>` 结束时，我们立即将其从批次中移除，并用一个**等待中的新请求**替换它的位置。

**主要缺点：巨大的填充浪费**

动态调度虽然提升了有效性，但当新请求（需要经历完整的预填充过程）被插入到正在解码的批次中时，会引入更严重的填充问题：

- 新请求通常有较长的输入序列 $n$。
- 批次中其他请求正处于解码阶段（每步只生成 $1$ 个词元）。
- 为了让新请求进入批次，它几乎需要被填充到其自身长度 $n$。
- 填充成本：在一个 $B$ 大小的批次中引入一个长度为 $n$ 的提示，需要大约 $(n-1)(B-1)$ 个填充词元。

> **示例：** $B=8$，$n=100$，需要 $99 \times 7 = 693$ 个填充词元！

此外，像 CUDA graphs 或 `torch.compile` 等高级优化通常要求**张量形状是静态固定**的，这迫使我们必须将所有提示填充到预设的**最大长度**，极大地加剧了填充浪费。

### 2.3 连续批处理：消除填充浪费

问题的根源在于我们为了批处理而添加的那个“批量维度”。要根本解决填充问题，我们必须**消除这个批量维度**，转而使用**序列拼接**的方式进行批处理。

#### 核心技术一：不规则批处理（Ragged Batching）

不规则批处理，即将批次中的所有词元（来自不同请求）**简单地串联**成一个大序列。

- **零填充：** 不规则批处理完全消除了对填充词元的需要。
- **交互控制：** 关键在于，如何防止不同请求的词元（例如提示 0 的词元与提示 1 的词元）之间发生交互？

解决方案正是我们熟悉的工具：**注意力掩码（Attention Mask）**。我们创建一个精心构造的布尔掩码，确保：

1. 在单个请求内部，词元间的交互遵循**因果关系**。
2. 在请求之间，**任何词元都不允许与来自其他请求的词元进行交互**（即所有交叉位置都设置为 `False`）。

通过这种方式，我们可以将尽可能多的词元（受限于GPU内存总量 $m$）无缝地拼接在一起进行一次前向传播，从而最大化计算效率。

#### 核心技术二：连续批处理的调度算法

连续批处理将**不规则批处理**和**动态调度**结合起来，形成一种高效的调度策略：

1. **确定预算：** 设定一个批次中最大可容纳的词元数预算 $m$（由GPU内存决定）。
2. **优先解码：** 首先将所有处于**解码阶段**（每个只贡献 1 个词元）的请求加入批次。
3. **填充分块预填充：** 使用**分块预填充**的灵活性，将等待中的新请求（处于预填充阶段）**按需分割**成块，并填充剩余的 $m$ 词元预算。
4. **动态交换：** 一旦批次中的某个请求完成生成（生成 `<eos>`），它立即被移除，其腾出的空间会立即被用于容纳新的解码词元或新的分块预填充。

**连续批处理（Continuous Batching）** 的工作机制确保了GPU始终以接近满负荷的状态运行，并且批次中的每一个计算周期都产生有效的新词元，从而实现极高的吞吐量。



## 结论

连续批处理是现代LLM服务系统不可或缺的关键技术，它通过以下三种技术的协同作用，实现了吞吐量的最大化：

1. **键值缓存 (KV-Cache)：** 避免重复计算过去的词元表示。
2. **分块预填充 (Chunked Prefill)：** 灵活处理超出内存限制的可变长度长提示。
3. **不规则批处理 + 动态调度：** 彻底消除填充浪费，允许在同一批次中混合处理预填充和解码阶段的请求，并时刻保持GPU的高效利用。

正是这种对批处理策略的根本性革新，使得像 ChatGPT 这样的服务能够以极高的效率，同时服务数以千计的并发用户请求。

*后续文章预告：我们将继续探讨如何通过**分页注意力（Paged Attention）**来进一步优化KV-Cache的内存管理。*
