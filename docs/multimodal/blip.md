# BLIP：引导语言图像预训练，实现统一的视觉语言理解和生成

BLIP 是一种用于统一视觉语言理解和生成的新预训练框架，它在各种视觉语言任务上取得了最先进的结果。

------

## 背景

视觉和语言是人类感知世界的两种最基本的方式，也是人工智能的两大关键基石。人工智能的一个长期目标是构建能够通过视觉和语言输入理解世界并通过自然语言与人类交流的智能代理。

为了实现这一目标，视觉语言预训练 已成为一种有效的方法，其中深度神经网络模型在大规模图像文本数据集上进行预训练，以提高下游视觉语言任务（例如图像文本）的性能。文本检索、图像字幕和视觉问答。

简而言之，视觉语言预训练旨在利用图像文本数据来教会模型共同理解视觉和文本信息的能力。通过预训练，模型在微调之前就已经过训练（微调涉及使用下游任务的数据对预训练模型进行额外的训练）。如果没有预训练，模型需要在每个下游任务上从头开始训练，这会导致性能下降。

## 局限：大多数模型缺乏灵活性，Web 数据嘈杂

尽管视觉语言预训练取得了巨大成功，但现有方法有两个主要局限性：

- 从模型角度来看，大多数现有的预训练模型不够灵活，无法适应广泛的视觉语言任务。基于编码器的模型不太容易直接转移到文本生成任务，而编码器-解码器模型尚未成功应用于图像文本检索任务。

- 从数据角度来看，大多数模型都会对从网络自动收集的图像和替代文本对进行预训练。然而，网络文本通常不能准确描述图像的视觉内容，使它们成为监督的嘈杂来源。

## 我们的解决方案：用 BLIP 翻转剧本

为了解决这些限制，我们提出了BLIP：Bootstrapping Language-Image Pre-training（语言-图像预训练），以实现视觉-语言的统一理解和生成。BLIP介绍：

- 一种新的模型架构，能够比现有方法实现更广泛的下游任务；
- 一种新的数据集引导方法，用于从嘈杂的网络数据中学习。

BLIP在七项视觉语言任务上实现了最先进的性能，包括：

- image-text retrieval
- image captioning
- visual question answering
- visual reasoning
- visual dialog
- zero-shot text-video retrieval
- zero-shot video question answering.

##  深入探讨：BLIP 的工作原理

视觉语言理解和生成的统一模型

为了预训练具有理解和生成能力的统一视觉语言模型， BLIP 引入了编码器-解码器的多模态混合模型，这是一个多任务模型，可在三种功能之一中运行：

1. 单模态编码器，分别对图像和文本进行编码。图像编码器是一个视觉变换器。文本编码器与 BERT 相同。文本输入的开头会添加一个 [CLS] 标记，用于总结句子。
2. 基于图像的文本编码器，通过在文本编码器的每个变换器块的自注意力层和前馈网络之间插入交叉注意力层来注入视觉信息。特定于任务的 [Encode] 标记被附加到文本中，并且 [Encode] 的输出嵌入用作图像-文本对的多模态表示。
3. 基于图像的文本解码器，用因果自注意力层替换文本编码器中的双向自注意力层。特殊的 [Decode] 标记用于表示序列的开始。

BLIP在预训练过程中联合优化了三个目标，其中两个基于理解的目标（ITC、ITM）和一个基于生成的目标（LM）：

- 图像文本对比损失（ITC）激活单模态编码器。它的目的是对齐视觉变换器和文本变换器的特征空间，通过鼓励正图像-文本对与负图像-文本对相比具有更为相似的表示。
- 图像文本匹配损失（ITM）激活基于图像的文本编码器。ITM 是一项二元分类任务，要求模型在给定多模态特征的情况下预测图像-文本对是正（匹配）还是负（不匹配）。
- 语言建模损失（LM）激活基于图像的文本解码器，其目的是生成以图像为条件的文本描述。

在不同的下游任务中，我们微调预训练模型的不同路径以实现不同的目标，如下面的动画所示。

## 从嘈杂的图像文本对中引导字幕

视觉语言预训练依赖于从网络自动收集的大规模图像文本对。然而，文本通常不能准确地描述图像的视觉内容，这使得它们成为一种嘈杂的监督。

为了解决这个问题，我们通过引入两个模块来引导字幕：字幕生成器和过滤器。

- 字幕生成器是一个基于图像的文本解码器。给定网络图像，我们使用字幕生成器生成合成字幕作为额外的训练样本。
- 过滤器是一个基于图像的文本编码器。它会删除与相应图像不匹配的嘈杂字幕。

##  结果

通过引导数据集，我们的预训练模型在下游任务上实现了显着的性能改进，如下表所示。

我们还发现，由于合成字幕的多样性更高，因此使用随机解码方法（核采样）比使用波束搜索来生成字幕更好。


下面我们展示了BLIP 在图像文本检索上的性能，在使用相同数量的图像的情况下，它的平均召回率 @1 比现有最先进的[ALBEF](https://arxiv.org/abs/2107.07651?ref=blog.salesforceairesearch.com)高出2.7% 。（有关其他任务的更多结果，请参阅我们的[论文](https://arxiv.org/abs/2201.12086?ref=blog.salesforceairesearch.com)）

------

## 影响

BLIP 研究对人工智能及其他领域有好处：

- AI 优势

  ：BLIP 对人工智能的贡献包括：

  - 为统一的基于图像的文本理解和生成任务生成最先进的视觉语言预训练模型
  - 引入了从嘈杂的网络数据中学习的新框架
    - 通过以下方式处理噪音：
      - 生成合成字幕作为额外的训练样本
      - 删除嘈杂的字幕

- 更广泛（一般）影响：BLIP 可以通过更好的视觉语言智能来实现广泛的下游应用，例如电子商务平台中的产品推荐和分类。

## 总结

- 视觉语言研究是：
  - 人工智能的核心问题，因为视觉和语言是世界上两种基本的信息模态
  - 这是一个重要的应用领域，因为许多工业人工智能应用都是由视觉语言智能提供支持的。
- 我们的框架称为 BLIP，它介绍了：
  - 一种新的模型架构，能够比现有方法实现更广泛的下游任务
  - 一种新的数据集引导方法，用于从嘈杂的网络数据中学习。
- BLIP 框架为深度学习和人工智能做出了宝贵的贡献：
  - 为统一的基于图像的文本理解和生成任务生成最先进的视觉语言预训练模型
  - BLIP 用于从嘈杂的网络数据中学习的新框架很有价值，因为网络收集的图像描述通常不准确，即充满噪音。
  - 提供简单、灵活且强大的视觉语言模型，可以进行端到端微调
    - 我们微调预训练模型的不同路径，以在不同的下游任务上实现不同的目标
  - 在图像文本检索、图像字幕、视觉问答、视觉推理和视觉对话方面实现最先进的性能。
  - 以零样本方式直接转移到视频语言任务时表现出很强的泛化能力。
- BLIP 还提供更广泛的好处：
  - 为广泛的下游应用提供更好的视觉语言智能，例如电子商务平台中的产品推荐和分类。
- 演示示例显示 BLIP 可以：
  - 生成准确详细的图像标题
  - 为一系列不同的问题生成准确的答案。
- 我们发布了代码、模型和引导数据集，以促进视觉语言研究和工业应用。



## 相关资源

- ALBEF (ALign BEfore Fuse):
  - Blog post:
    - [Align before Fuse (ALBEF): Advancing Vision-language Understanding with Contrastive Learning](https://blog.salesforceairesearch.com/align-before-fuse/)
  - Research paper:
    - [Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/abs/2107.07651?ref=blog.salesforceairesearch.com), Advances in Neural Information Processing Systems (Spotlight), 2021.

## 附录：术语和定义

对我们讨论中使用的一些人工智能/深度学习术语的回顾：

- 图像文本数据集：数据的集合，其中每一项都是图像文本对，即一个或多个图像加上一个或多个文本描述的组合
- 预训练：模型在适应（或微调）下游任务数据之前已经过训练
- 微调：使用目标任务的数据进一步训练预训练模型
- 端到端：模型的所有参数都可以联合训练
- 编码器视觉语言模型：一种将图像文本数据编码为特征表示的模型，通常用于执行基于理解的任务
- Encoder-Decoder视觉语言模型：一种首先将图像文本编码为多模态特征，然后将特征解码为文本的模型
- 数据集引导：一种生成额外（合成）数据供系统用于构建模型的方法。
