# StreamRL：面向分离式架构的大规模语言模型强化学习训练框架

## 研究背景

在大语言模型（Large Language Models, LLMs）的发展进程中，强化学习（Reinforcement Learning, RL）已成为关键的训练后对齐技术。通过引入奖励信号优化模型行为，RL显著提升了LLMs在复杂推理、对话系统和代码生成等任务中的表现。当前，众多前沿模型如 OpenAI 的 o1、o3 系列，Anthropic 的 Claude 3.7 Sonnet，以及 DeepSeek-R1 等，均采用强化学习技术实现了性能突破。

早期的RL训练系统多采用**共置架构**（colocated architecture），即将生成（rollout）与训练（training）模块部署在同一组GPU设备上。该架构可复用现有推理与训练基础设施，实现简单，但存在严重的资源闲置问题：当一个阶段运行时，另一阶段的计算资源处于空闲状态，造成利用率低下。

为提升效率，**共置时间复用架构**（time-multiplexed colocated）应运而生，通过在时间上交替执行生成与训练任务，提高了GPU利用率，一度成为主流方案。然而，在大规模部署场景中，此类架构暴露出**资源强耦合**的缺陷——生成与训练共享同一资源池，无法根据各自负载特性独立扩展，限制了系统的可伸缩性与成本效益。

与此同时，**分离式架构**（disaggregated architecture）因其天然支持资源解耦、灵活扩展和异构硬件适配等优势，重新受到关注。然而，现有框架在实现分离式RL训练时面临两大挑战：

1. **流水线气泡**（Pipeline Bubbles）：由于生成与训练为串行流水线结构，若两阶段处理时间不匹配，将导致后续阶段等待，形成计算空洞；
2. **偏态气泡**（Skewness Bubbles）：LLM推理任务中输出长度呈现显著的长尾分布（long-tail distribution），少数长输出样本显著拖慢整体批次处理速度，造成GPU资源浪费。

针对上述问题，本文介绍 **StreamRL** —— 一种专为分离式架构设计的高效强化学习训练框架，旨在彻底释放该架构的潜力，解决传统方法中的效率瓶颈。

## 研究问题

### 1. 共置架构的局限性

共置架构将生成与训练任务绑定于同一物理资源，导致以下问题：

- **资源分配僵化**：无法根据生成与训练的实际算力需求独立扩展；
- **硬件同构性约束**：必须使用相同类型GPU，难以利用专用硬件（如高内存GPU用于训练，高吞吐GPU用于生成）；
- **跨数据中心训练受限**：难以实现地理分布式的协同训练。

### 2. 分离式架构的挑战

尽管分离式架构理论上具备更高灵活性，但在实践中面临以下关键挑战：

- **流水线气泡**：由于生成与训练串行执行，若生成完成前训练器空闲，或训练未完成时生成器需等待，均会造成GPU利用率下降。
- **偏态气泡**：受长尾输出长度影响，即使批次中仅含少量“长文本”样本，也会显著延长整个批次的生成时间，导致批量处理效率降低。

现有系统未能有效建模和调度长尾工作负载，导致训练延迟增加、吞吐下降，严重制约了大规模RL训练的可行性与经济性。

---

## 主要贡献

1. **重新评估架构范式**
   系统分析了共置架构在大规模部署中的资源耦合问题，论证了分离式架构在资源弹性、异构支持与跨域协同方面的根本优势，提出应重新回归分离式设计，并为其提供系统级支持。

2. **提出 StreamRL 框架**
   设计并实现了 StreamRL，一套专为分离式RL训练优化的高效系统框架，核心创新包括：
   - **动态批处理流水线** 与 **完全异步流水线**，消除流水线气泡；
   - **输出长度排序器模型**（Output Length Ranker）与 **偏态感知调度策略**，缓解长尾效应引发的偏态气泡。

3. **实验验证卓越性能**
   在多种LLM（如 LLaMA-2、Qwen 等）与真实数据集上的实验表明：
   - 相比当前最先进的共置系统，StreamRL 的吞吐量最高提升 **2.66 倍**；
   - 在异构与跨数据中心场景下，成本效益最高提升 **1.33 倍**；
   - 展现出优异的可扩展性与稳定性。

---

## 方法论精要

### 1. 核心架构设计：流生成服务与训练器解耦

StreamRL 将强化学习训练流程解耦为两个独立组件：

- **流生成服务**（Stream Generation Service, SGS）：负责环境交互、动作采样与轨迹生成；
- **训练器**（Trainer）：负责策略梯度计算、模型参数更新。

二者部署在物理分离的资源池上，可通过高速网络（如RDMA）连接，甚至可跨数据中心部署。通信通过专用的 **RL-RPC** 框架完成，实现低延迟、高吞吐的数据传输。

$$
\text{SGS} \xrightarrow{\text{Generated Trajectories}} \text{Trainer}
\quad \text{and} \quad
\text{Trainer} \xrightarrow{\text{Updated Weights}} \text{SGS}
$$

此架构支持：

- 独立扩展生成与训练资源；
- 使用异构GPU类型（如A100用于训练，H100用于生成）；
- 跨地域协同训练，提升容灾与资源利用率。

---

### 2. 消除流水线气泡：动态与异步流水线设计

#### （1）同步RL场景：动态批处理流水线（Dynamic-batch Pipelining）

传统方法采用固定批量生成（batched rollout），生成完成后才启动训练，造成训练器等待（即流水线气泡）。

StreamRL 改为**流式生成**：每当一个样本生成完成，立即通过RL-RPC发送至训练器。训练器端采用**动态批处理机制**，根据到达速度自适应聚合样本形成训练批次。

设生成单个样本时间为 $t_g$，训练批次时间为 $t_t$，则传统方法中空闲时间为：

$$
T_{\text{idle}} = \max(0, t_t - N \cdot t_g)
$$

而StreamRL通过流式传输与动态批处理，使得 $t_t \approx N \cdot t_g$，显著减少 $T_{\text{idle}}$，几乎消除流水线气泡。

#### （2）异步RL场景：完全异步流水线（Fully Asynchronous Pipelining）

在异步RL中，权重更新无需等待所有生成完成。StreamRL 将权重同步从关键路径中移除：

- SGS持续生成新轨迹；
- Trainer完成训练后立即广播新权重；
- 权重传输与生成/训练过程并行进行。

只要生成与训练的平均速率匹配，且波动有限，系统即可稳定运行，避免因迭代周期不一致引入的新气泡。

---

### 3. 缓解偏态气泡：基于预测的偏态感知调度

#### （1）输出长度排序器模型（Output Length Ranker）

为应对长尾输出问题，StreamRL 训练一个轻量级排序模型，用于预测输入提示（prompt）对应的输出长度。

该模型基于监督学习训练，输入为 $(p_i, l_i)$ 对（提示及其实际输出长度），输出为长度估计或排序分数。模型结构可为小型Transformer或MLP，训练数据来自历史轨迹日志。

令 $f_\theta(p)$ 表示排序模型对提示 $p$ 的输出长度预测值，则：

$$
\hat{l}_p = f_\theta(p)
$$

SGS 利用该模型对输入批次进行预排序，并识别潜在的“长尾样本”。

#### （2）偏态感知调度（Skewness-aware Scheduling）

基于预测结果，SGS 实施差异化调度策略：

- **长尾样本**：单独或小批量处理，分配专用GPU实例，避免拖累整体批次；
- **常规样本**：聚合成大批次，最大化GPU利用率。

调度策略采用**最长处理时间优先**（Longest Processing Time first, LPT）贪心算法：

$$
\text{Schedule order: } \arg\max_{p \in P} \hat{l}_p
$$

优先处理预计耗时最长的样本，有助于平滑整体生成延迟，减少最大完成时间（makespan）。

---

### 4. 系统实现优化

#### （1）高性能生成引擎

SGS 采用内部优化的C++推理引擎，支持：

- **连续批处理**（Continuous Batching）：动态添加/移除序列；
- **前缀缓存**（Prefix Sharing）：共享相同上下文的Key-Value缓存，降低显存占用与计算开销。

#### （2）高效训练系统

Trainer 实现 **3D并行**（数据并行、张量并行、流水线并行），并引入**动态CPU卸载**技术，将不活跃的模型参数临时卸载至CPU内存，突破GPU显存限制，支持更大模型训练。

#### （3）低延迟通信框架：RL-RPC

专为RL场景设计的RPC协议，具备以下特性：

- 基于 **GPU-Direct RDMA** 实现零拷贝张量传输，减少CPU介入；
- 支持 **TCP fallback** 机制，确保在无RDMA环境下的兼容性；
- 序列化开销极低，适用于高频小批量数据交换。

#### （4）权重分发优化

根据不同部署模式优化权重广播：

- **单数据中心**：采用多树（multi-tree）负载均衡广播，减少网络拥塞；
- **跨数据中心**：由中心节点发送至各区域网关，再在本地广播，降低跨域带宽消耗。

---

## 总结

StreamRL 通过对分离式架构的系统性重构，解决了传统共置方案在可扩展性与资源利用率方面的根本瓶颈。其核心思想在于：

- **解耦**：将生成与训练分离，释放资源调度灵活性；
- **预测**：利用长度排序模型预判长尾行为；
- **调度**：基于预测实施偏态感知资源分配；
- **异步**：通过动态与完全异步流水线消除等待气泡。

实验结果表明，StreamRL 不仅在吞吐量上实现显著提升，更在异构与分布式场景中展现出卓越的成本效益，为未来大规模语言模型的强化学习训练提供了可扩展、高效率的系统范式。



# StreamRL：面向解耦架构的大语言模型强化学习训练框架

## 1 引言

强化学习（Reinforcement Learning, RL）已成为提升大语言模型（Large Language Model, LLM）推理能力的重要范式，显著推动了其在代码生成、数学推理等复杂任务上的性能突破，并揭示了“推理阶段扩展定律”（inference-time scaling law）[7,13,53]。当前主流先进模型，如 OpenAI o1[2] 与 o3[4]、Claude 3.7 Sonnet[6] 以及 DeepSeek-R1[13]，均通过强化学习后训练（post-training）实现了业界领先的性能。

与传统预训练中采用的“下一 token 预测”目标不同，RL 利用奖励信号引导模型通过试错机制进行优化。尽管存在 PPO[36]、GRPO[37] 等多种算法实现，典型的 LLM 强化学习流程仍遵循两个串行阶段：

- **生成阶段（Generation Phase）**：模型根据输入提示（prompt）生成完整的响应样本（trajectory）；
- **训练阶段（Training Phase）**：基于生成样本计算奖励信号，并更新模型参数。

早期的 RL 训练框架（如 OpenRLHF[19]、NeMo[18]）普遍采用**解耦架构**（图 1a），即为生成与训练阶段分别配置独立的计算资源。生成任务通常借助成熟的推理框架（如 vLLM[23]）完成，生成结果通过网络传输至训练模块（基于 DeepSpeed[8] 或 Megatron-LM[41]），训练完成后更新的权重再回传至推理节点，用于下一轮生成。该架构便于复用现有基础设施，支持快速迭代与算法部署。然而，其核心缺陷在于资源利用率低下：由于两阶段串行执行，训练阶段运行时生成 GPU 空闲，反之亦然，导致严重的计算资源浪费。

为解决这一问题，近期研究（如 verl[38]、ReaL[28]、RLHFuse[59]）转向**共置架构**（colocated architecture，图 1b），将生成与训练共置于同一组 GPU 上。当某一阶段运行时，另一阶段的状态（如模型权重、优化器状态）暂存于 CPU 内存，通过上下文切换实现 GPU 时间片共享。此架构有效消除了空闲等待，提升了整体训练效率，迅速成为主流方案。

然而，在实际大规模商用模型训练中，我们发现共置架构存在**资源耦合**（resource coupling）的根本性瓶颈。其根源在于生成与训练阶段的计算特性存在本质差异：

- **生成阶段**：主要为自回归解码过程，每个新 token 仅需计算一次，但需频繁访问完整模型参数 → **内存带宽受限**；
- **训练阶段**：需对整个批次执行前向与反向传播，计算密集 → **计算单元受限**。

共置架构强制两阶段共享相同数量与类型的 GPU，导致以下冲突：

| 维度               | 问题表现                                                     |
| ------------------ | ------------------------------------------------------------ |
| **资源数量**       | 生成阶段随 GPU 增加迅速饱和（受内存带宽限制），而训练阶段可线性扩展；共置下整体资源利用率下降。 |
| **硬件类型**       | 不同 GPU 在计算能力、内存带宽与成本之间存在权衡（如 H20 带宽高、成本低 35%）；共置无法为各阶段独立选型。 |
| **跨数据中心部署** | 共置依赖全互联通信（如 3D 并行），跨数据中心通信开销巨大，难以利用异构资源池。 |

在此背景下，**解耦架构**重新展现出其独特优势：

- **资源分配灵活**：可为两阶段独立配置 GPU 数量；
- **硬件异构支持**：生成阶段可选用高性价比推理卡（如 H20），训练阶段使用高性能训练卡（如 H800）；
- **跨数据中心兼容**：阶段间仅需点对点数据传输，通信开销低，易于扩展至多地异构集群。

然而，现有解耦框架仍面临两大性能瓶颈：

1. **流水线气泡**（Pipeline Bubbles）：串行执行导致 GPU 空闲；动态负载变化加剧此问题。
2. **偏斜气泡**（Skewness Bubbles）：LLM 生成长度呈长尾分布，少量长样本拖慢整体进度，后期 GPU 利用率骤降。

为此，我们提出 **StreamRL**，一种专为解耦架构设计的高效 RL 训练框架。其核心思想是将生成与训练抽象为**流式生成服务**（Stream Generation Service, SGS）与**Trainer**，通过流式样本传输实现两阶段的高效重叠。SGS 以流方式逐样本返回结果，Trainer 可立即处理，显著减少等待时间。

在此基础上，StreamRL 引入：

- **动态资源分配算法**：基于性能建模，静态与动态结合，平衡两阶段执行时间；
- **偏斜感知调度机制**：利用轻量级输出长度预测模型识别长尾样本，优先调度以缩短整体生成时间。

实验表明，StreamRL 在多种 LLM 与真实数据集上，相比现有最优系统最高提升 **2.66 倍吞吐**，在异构与跨数据中心场景下进一步提升 **1.33 倍成本效率**。

### 本文主要贡献：

- 深入分析共置架构在可扩展性与效率方面的根本局限，重新审视并论证解耦架构的潜力；
- 提出 StreamRL 框架，通过流式生成、动态资源调整与偏斜感知调度，系统性解决解耦架构的关键挑战；
- 在真实场景下验证 StreamRL 在异构、跨数据中心环境中的显著优势。

---

## 2 背景与动机

### 2.1 大语言模型中的强化学习流程

在 LLM 强化学习中，标准 RL 概念映射如下：

| RL 概念             | LLM 中的对应实体                                   |
| ------------------- | -------------------------------------------------- |
| 智能体（Agent）     | 待训练的 LLM                                       |
| 环境（Environment） | 提示 + 奖励函数构成的任务环境                      |
| 状态（State）       | 当前已生成的完整 token 序列（prompt + 已生成内容） |
| 动作（Action）      | 下一个生成的 token                                 |
| 轨迹（Trajectory）  | 完整的生成样本（rollout）                          |

每轮 RL 迭代包含两个串行阶段：

1. **生成阶段**
   - **预填充（Prefill）**：一次性处理提示所有 token，构建 KV 缓存；
   - **解码（Decoding）**：基于 KV 缓存自回归生成后续 token。

2. **训练阶段**
   - 计算样本级奖励（来自奖励模型或规则函数）；
   - 结合 KL 正则项（来自参考模型）更新策略网络；
   - 常用算法：PPO[36]、GRPO[37] 等。

### 2.2 并行化技术

为支持大规模 LLM 训练，通常采用以下三种并行策略组合：

| 并行方式         | 核心机制                                         | 主要约束                      |
| ---------------- | ------------------------------------------------ | ----------------------------- |
| 数据并行（DP）   | 每卡复制完整模型，处理不同数据子集；梯度全局同步 | 通信量随模型规模增长          |
| 张量并行（TP）   | 单个算子拆分至多卡并行计算                       | 需节点内高速互联（如 NVLink） |
| 流水线并行（PP） | 按层切分模型，微批次流水执行                     | 层间通信开销大                |

### 2.3 共置架构的瓶颈

尽管共置架构看似资源利用率更高，但在大规模训练中暴露出**资源耦合**问题。

#### 工作负载差异

- **生成阶段**：解码计算量小，但频繁读取模型权重 → **内存带宽受限**；
- **训练阶段**：需处理整批 token 的前向与反向传播 → **计算单元受限**。

共置架构强制两阶段共享相同资源，导致以下冲突：

| 维度           | 问题表现                                                     |
| -------------- | ------------------------------------------------------------ |
| **资源数量**   | 生成阶段随 GPU 增加迅速饱和，训练阶段持续扩展 → 整体利用率下降 |
| **硬件类型**   | 无法为生成阶段单独选用高带宽低成本 GPU（如 H20）             |
| **跨数据中心** | 依赖全互联通信，难以利用多地异构资源池                       |

### 2.4 解耦架构的优势与挑战

#### 优势

| 维度           | 优势说明                                 |
| -------------- | ---------------------------------------- |
| **灵活性**     | 两阶段可独立配置 GPU 数量与类型          |
| **异构支持**   | 生成用推理卡，训练用计算卡，优化成本效率 |
| **跨数据中心** | 仅需点对点通信，天然支持异地部署         |

#### 挑战

尽管优势明显，解耦架构仍面临两大性能瓶颈（见图 3）：

1. **流水线气泡**（Pipeline Bubbles）
   - 传统“整批生成 → 整批训练”模式导致 GPU 空闲；
   - RL 训练中生成长度动态增长，两阶段耗时比例变化，加剧气泡。

2. **偏斜气泡**（Skewness Bubbles）
   - LLM 生成长度呈长尾分布，少量长样本拖慢整体；
   - 批量解码需维持大 batch 以保证吞吐，末期仅剩长样本时利用率骤降；
   - 现有方案局限：
     - 回放缓冲会改变数据分布；
     - 共置架构的提前计算 KL 策略不适用于解耦场景。

---

## 3 StreamRL 概述

为充分发挥解耦架构潜力，我们提出 **StreamRL**，一种专为解耦架构设计的高效 RL 训练框架。StreamRL 将生成与训练抽象为两个独立组件：

- **流式生成服务**（Stream Generation Service, SGS）
- **Trainer**

二者可部署于物理分离的资源池，甚至跨数据中心，通过高效通信链路连接，充分释放解耦架构在资源灵活性、异构支持与地理扩展性方面的优势。

### 系统架构

StreamRL 架构如图 4 所示。SGS 与 Trainer 通过自研通信框架 RL-RPC 连接，支持样本流式传输与权重高效更新。

### 工作流程

1. **资源分配**
   基于集群、模型与算法配置，StreamRL 通过性能建模枚举最优资源分配方案，最小化整体迭代时间。

2. **生成与训练**
   - **SGS**：提供 `update(weights)` 与 `generate(prompts)` API。接收提示后开始生成，并以流式方式逐样本返回结果。
   - **Trainer**：接收流式样本，立即启动参考模型推理、KL 计算、奖励评估等操作，实现训练阶段提前启动。

3. **偏斜感知调度**
   SGS 内置轻量级**输出长度排序模型**，预测样本长度，优先调度长尾样本，减少整体生成延迟。

4. **动态资源调整**
   监控两阶段执行时间差 $\delta$。当 $\delta$ 超过阈值时，动态增加 SGS 的数据并行（DP）规模，维持阶段平衡，无需重启训练。

### 通信机制

StreamRL 采用自研 **RL-RPC** 框架，专为高效张量传输优化：

- 支持 GPU-Direct RDMA，实现零拷贝传输，绕过 CPU 序列化开销；
- 提供 TCP 回退机制，保障跨数据中心兼容性。

#### 权重传输优化

- **单数据中心**：构建多个以不同 DP 排名为根的广播树，负载均衡，最大化带宽利用率。
- **跨数据中心**：仅由 Trainer 的 DP 0 节点将权重发送至远程 SGS 的 DP 0 节点，再在远程侧本地广播，最小化跨域通信开销。

---

## 4 解决流水线气泡问题

### 4.1 流式重叠设计

关键目标：实现生成与训练阶段的高效重叠，消除空闲等待。

#### 同步 RL：动态批次流水线

同步 RL 中，权重更新在整批样本处理完毕后进行。传统方案采用**微批次流水线**（图 5a）：将样本划分为固定大小的微批次，生成一批即传入训练。但该方法存在以下问题：

- 微批次大小需手动设置，难以兼顾重叠效率与训练开销；
- 生成长度动态增长导致后期微批次处理时间不均，产生显著气泡。

**StreamRL 方案：动态批次流水线**（图 5b）
采用**流式生成**，样本一旦完成即刻返回。Trainer 可在接收到足够样本后立即启动训练，批次大小由生成速度动态决定。该机制：

- 消除训练阶段除首个微批次外的所有空闲时间；
- 自适应应对长尾效应，减少后期气泡。

#### 异步 RL：完全异步流水线

异步 RL 允许使用非最新权重生成的样本进行训练，容忍一定权重陈旧性。研究表明，单步异步对 LLM 性能与收敛性无显著影响[31,48]。

传统方案（图 5c）：在训练第 $i$ 轮样本时，提前生成第 $i+1$ 轮样本，实现跨迭代重叠。但每轮仍需全局同步传输权重，造成空闲。

**StreamRL 方案：完全异步流水线**（图 5d）
结合流式传输与异步机制：

- 权重更新与下一轮训练重叠：前一轮样本已流式缓冲，训练可继续；
- 当前轮生成不依赖最新权重，可并行执行；
- 权重传输被移出关键路径，实现完全重叠。

> **注意**：本方案仅引入单步异步，保持与同步 RL 相同的训练语义。

### 4.2 阶段平衡机制

为实现高效重叠，需平衡 SGS 与 Trainer 的执行时间。

#### 并行配置建模

- **Trainer**：基于性能分析模型预测训练时间，考虑 3D 并行（DP/TP/PP）开销。
- **SGS**：在偏斜感知调度下，生成时间可建模为确定性函数。

#### 资源分配策略

1. **单数据中心部署**
   设总 GPU 预算为 $n$，分配 $x$ 给 SGS，$y$ 给 Trainer，满足 $x + y \leq n$。
   枚举所有 $(x, y)$ 组合，计算生成时间 $T_g(x)$ 与训练时间 $T_t(y)$，选择使
   $$
   \max(T_g(x), T_t(y))
   $$
   最小的配置。

2. **跨数据中心部署**
   设两中心 GPU 预算分别为 $m$ 和 $n$，约束为 $x \leq m$, $y \leq n$。
   初始可设 $(x, y) = (m, n)$，但可能导致资源浪费。
   StreamRL 采用**渐进缩减法**：识别较快阶段，逐步减少其 GPU 数量，直至两阶段时间接近，释放资源供其他任务使用。

#### 动态资源调整

随着训练进行，生成长度增长，$T_g$ 增长速度通常快于 $T_t$（图 2），导致 $\delta = T_g - T_t$ 增大。

**调整策略**：

- 估算增加一个 SGS 数据并行单元可减少的生成时间 $\delta'$；
- 当 $\delta \geq \delta'$ 时，触发扩容；
- 仅需初始化新增 DP 实例，无需重启训练，开销极低。

该机制有效应对 RL 训练的动态负载变化，维持长期高效运行。

## 5 解决偏斜气泡问题

### 5.1 问题分析与设计机遇

#### 问题 1：现有调度策略的局限性

当前系统在生成任务调度中普遍采用随机分配策略，未对长尾样本与常规样本加以区分。如图 6(a) 所示，在一个数据并行（DP）规模为 2 的场景中，若存在 2 个输出长度为普通样本两倍的长尾样本，随机分配将导致每个生成实例均需处理一个长尾样本和部分常规样本。

该策略存在双重性能缺陷：

- **早期阶段**：长尾样本因与其他样本组成大批次，受批处理干扰，解码速度降低；
- **后期阶段**：当大多数常规样本已完成，仅剩少数长尾样本时，批大小急剧下降，GPU 利用率骤降。

图 6 右侧展示了在 NVIDIA A100 GPU 上，13B 模型每 token 解码延迟随批量大小（BS）的变化趋势。可见，在达到计算瓶颈前，延迟增长缓慢；此后则近似线性上升。为提升吞吐，系统通常采用大批次策略，但在长尾样本存在时，此策略反而加剧了整体延迟。

#### 机遇 1：基于负载特性的优化调度

生成单个样本的延迟可建模为：

$$
\text{Sample Latency} = \text{PTL}(BS) \times L
$$

其中 $\text{PTL}(BS)$ 为每 token 延迟（是批量大小 $BS$ 的函数），$L$ 为输出长度。随机分配仅依据 $L$ 均衡负载，忽略了 $\text{PTL}(BS)$ 随 $BS$ 单调递增的特性。

由此可得优化思路：**将长尾样本与常规样本分离调度**：

- 为长尾样本分配专用实例，使用小批量以降低 $\text{PTL}$，提升解码速度；
- 将常规样本集中处理，使用大批量以最大化 GPU 利用率。

如图 6(b) 所示，该二维调度策略可显著缩短整体生成时间。

#### 问题 2：长尾样本的识别挑战

上述策略依赖于生成前对样本长度的预判。然而，LLM 的输出长度通常被认为难以准确预测。

#### 机遇 2：基于排序的预测可行性

尽管精确预测输出长度困难，但通过轻量级模型估计样本的**相对长度排名**是可行的[14]。其理论依据在于：

- 输出长度与提示复杂度正相关，而复杂度是提示的固有属性；
- 排名问题本质为分类任务（按难度分级），比回归任务（预测绝对长度）更易实现；
- 不同 LLM 对同一提示的推理难度感知具有一致性，支持跨模型泛化。

实验表明（见第 7.2 节），排序模型可对前 20% 长尾样本实现近 90% 的召回率。由于整体延迟主要由长尾样本决定，高精度识别长尾样本即可获得接近“先验知识”（oracle）的性能增益。

基于上述分析，我们设计了**输出长度排序模型**（5.2 节）与**偏斜感知调度机制**（5.3 节）。

---

### 5.2 输出长度排序模型

#### 模型构建方法

1. **数据收集**：从在线推理服务或离线生成中获取目标 LLM 的（提示，输出长度）样本对。
2. **模型训练**：将提示与对应长度拼接，构建训练集，对小型 LLM 进行监督微调（SFT），使其具备长度预测能力。
3. **推理与排序**：输入一批提示，模型输出各提示的长度估计值，据此生成排序结果。

> **注意**：SFT 训练目标为预测绝对长度。随着 RL 训练推进，目标模型输出长度可能变化。因此，需定期使用最新生成数据对排序模型进行在线微调。

得益于提示难度的稳定性，即使绝对预测值存在漂移，相对排名仍保持较高准确性，降低了频繁微调的需求。

#### 开销分析

排序模型的引入几乎不增加在线开销：

- 模型训练快速（数分钟内收敛）；
- 训练完成后，对 RL 数据集进行**一次性离线预处理**，生成长度估计；
- 在线训练阶段仅使用预处理结果，无额外计算负担。

该设计确保了调度优化与训练效率的解耦。

---

### 5.3 偏斜感知调度机制

在排序模型支持下，SGS 可获取一批提示及其估计长度，进而执行以下调度决策：

#### 1. 分配策略

**步骤 1：样本分类**
将提示按估计长度降序排列，取最长的 $\alpha\%$ 作为长尾样本。实验表明（第 7.4 节），$\alpha = 20$ 可取得良好效果。

**步骤 2：资源分配**
设共有 $N$ 个生成实例，需确定 $N_l$（处理长尾样本）与 $N_r = N - N_l$（处理常规样本）的最优值。

为实现负载均衡，需估计两类样本的工作负载。假设已知目标模型输出长度分布 $D$（可通过近期生成样本统计获得），使用 $D$ 的 P50 与 P90 分别估计常规与长尾样本的平均长度 $L_{\text{avg}}$。

单个实例的生成延迟建模为：

$$
\text{Latency} = \text{PTL}(BS) \times L_{\text{avg}} \times \left\lceil \frac{M}{BS} \right\rceil
$$

其中 $M$ 为分配给该实例的提示数，$BS$ 受限于 GPU 内存（键值缓存占用）。

通过遍历所有 $(N_l, N_r)$ 组合，选择使整体生成时间最小的配置。算法 2 给出了偏斜感知分配的伪代码。确定 $N_l$ 和 $N_r$ 后，长尾与常规样本分别在对应实例间均匀分配。

#### 2. 调度顺序

当单个实例的 $M > BS$ 时，需进行多轮生成。此时调度顺序影响最大完成时间（makespan）。

该问题可视为并行机调度问题 $P||C_{\max}$ 的变体。我们采用经典启发式算法——**最长处理时间优先**（Longest Processing Time first, LPT）[15]：

- 将样本按估计长度降序排列；
- 每轮选择剩余长度最长的 $BS$ 个样本进行解码；
- 任一样本完成后，立即填入下一个最长的未完成样本。

LPT 算法是 $4/3$-近似算法，即其完成时间不超过最优调度的 $4/3$ 倍，能有效控制长尾样本的延迟。
