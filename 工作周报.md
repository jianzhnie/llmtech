## 调研

1. ReST-MCTS：通过过程奖励引导的树搜索实现LLM自训练

2.  rStar-Math：小型语言模型通过自我进化的深度思考掌握数学推理

3. 过程奖励模型（Process Reward Model）
4. Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement
5. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

## 强化微调

1. 在超节点跑通了强化微调算法；（repo: https://github.com/lqtrung1998/mwp_ReFT）
2. 强化微调（ReFT），以增强LLMs在推理中的泛化能力，并以数学问题求解为例进行验证。ReFT首先通过SFT进行预热，然后利用在线强化学习（RL），特别是PPO算法，进一步微调模型.

关键步骤说明：

1. 仓库原生基于GPU采用pytorch开发，在NPU上运行需要配置好所在机器的ascend pytorch开发环境，并熟悉如何将pytorch代码转换为torch-npu代码；
2. 运行需要安装 accelerate、deepspeed 等加速库；
3. accelerate 配置文件需要设置为 NPU 环境，切换为 bf16 为 fp16；
4. 下载大模型预训练权重，为了快速验证，这里使用opt-125m 模型；
5. 根据运行调试，修改部分代码，修改后的代码上传至启智社区https://openi.pcl.ac.cn/cloudbrain-llm/ReFT

难点：1. 定位 accelerate 的启动脚本问题。2. 如何配置 deepspeed zero1, zero2, zero3 三个不同的stage

## 模型权重和数据集：

1. 下载 Qwen 2.5系列模型权重

- https://huggingface.co/collections/Qwen/qwen25-math

2. 下载 open-web-math

- https://huggingface.co/datasets/open-web-math/open-web-math

```
Qwen2.5-1.5B
Qwen2.5-72B
Qwen2.5-7B
Qwen2.5-Math-1.5B
Qwen2.5-Math-7B
Qwen2.5-Math-PRM-72B
Qwen2.5-Math-PRM-7B
Qwen2.5-Math-RM-72B
```

3. 在 ReFT 框架中跑通 Qwen2.5 1.5 监督微调和 强化微调

## CPT

1. 开发open-web-math 数据读取和预处理代码

2. 开发 continued pretraining 训练代码
3. 在 open-web-math 数据集上微调 Qwen 2.5 模型
